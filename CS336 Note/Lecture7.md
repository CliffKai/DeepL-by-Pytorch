# Part 1 recap

1. 新的计算单元 —— 数据中心 (New unit of compute – the datacenter)
2. 多机扩展的理想目标 (What we want from multi-machine scaling) :
   - 线性内存扩展 (Linear memory scaling)
   - 线性算力扩展 (Linear compute scaling)
3. 简单的集合通信原语 (Simple collective comms primitives)

这三条便是构建现代 AI 基础设施的三大基石：
1. 宏观架构：把数据中心当成一台电脑用。
2. 性能目标：加多少卡，就得涨多少显存和速度（拒绝边际效应递减）。
3. 通信基础：需要标准化的通信方式来连接这些卡。

> 课程中，在评估一个分布式系统的性能时，通过计算**集合通信原语（Collective Communications Primitives）**的数量来进行推理，不会在这里深入探讨网速、带宽、延迟毫秒数等，也不去探讨通信算法的底层实现细节

## 1.1 basics about collective communication

首先来认识一下并行计算中五种最核心的 “集体通信” (Collective Communication) 操作。
![Figure_1](../images/CS336/CS336_Lecture_7_Figure_2.png)

这张图详细解释了并行计算中五种最核心的 **“集体通信” (Collective Communication)** 操作。

"集体通信"是指在一个并行计算集群中（例如，由多个 GPU 或多个服务器节点组成），所有参与的进程（图中的 "rank" 可以理解为一个个独立的计算单元，比如一个 GPU）**同时参与**的数据交换操作。

这些操作是实现大规模分布式计算的基石。

以下是图中五种操作的详细讲解：

### 1） All reduce (全归约)

* **操作前:** 每个 "rank"（rank 0, 1, 2, 3）都持有一份自己的输入数据（`in0`, `in1`, `in2`, `in3`）。
* **操作过程:**
    1.  **Reduce (归约):** 系统对所有 "rank" 的输入数据执行一个归约操作（例如 `sum` 求和）。`result = in0 + in1 + in2 + in3`。
    2.  **All (全部):** 将这个最终的计算结果（`out`）分发给**所有**的 "rank"。
* **操作后:** **所有 "rank"** 都拥有了相同的、归约后的最终结果 `out`。
* **常见用途:** 在数据并行训练中，每个GPU计算出自己的梯度（`in0`...`in3`），`All reduce` 用来计算所有梯度的总和，并将这个总和（`out`）分发回给每个GPU，以便它们能同步更新模型。

* **通信量：** 一次 all reduce 的通信量大概是本次 all reducing 的数据的两倍。

### 2） Broadcast (广播)

* **操作前:** 只有一个 "rank"（称为 "root"，例如图的 `rank 2`）持有数据（`in`）。
* **操作过程:** "root" 进程将其数据（`in`）发送给组内所有其他的进程。
* **操作后:** **所有 "rank"** (包括 "root" 自己) 都拥有了 `rank 2` 原始数据的副本（`out`）。
* **常见用途:** 在训练开始时，由 "root" 进程（rank 0 或 2）加载模型权重，然后 `Broadcast` 给所有其他GPU，确保大家从同一个起点开始。

### 3） Reduce (归约)

* **操作前:** 和 `All reduce` 一样，每个 "rank" 都持有一份自己的输入数据（`in0`, `in1`, `in2`, `in3`）。
* **操作过程:** 系统对所有 "rank" 的输入数据执行一个归约操作（例如 `sum` 求和）。
* **操作后:** **只有 "root" 进程**（图中指定了 `rank 2`）接收并存储了这个最终的归约结果（`out`）。其他 "rank" 不会收到这个结果。
* **区别 (Reduce vs All reduce):** `Reduce` 只把结果给 "root"；`All reduce` 把结果给所有人。

### 4） All Gather (全收集)

* **操作前:** 每个 "rank" 都持有**一部分**数据（`in0`, `in1`, `in2`, `in3`）。
* **操作过程:** 系统从所有 "rank" 收集它们各自的数据，并将这些数据**按 "rank" 的顺序拼接**（concatenate）成一个完整的大数据集。然后，系统将这个完整的数据集分发给所有 "rank"。
* **操作后:** **所有 "rank"** 都拥有了完整的、拼接后的数据集（`out`），这个 `out` 包含了 `in0`, `in1`, `in2`, `in3` 的所有内容。
* **常见用途:** 在模型并行或张量并行中，如果一个张量被切分到不同GPU上，`All Gather` 可以将这些分片收集起来，在每个GPU上还原出完整的张量。

### 5） Reduce Scatter (归约-分散)

* **操作前:** 每个 "rank" 都持有一份**完整的输入数据**。请注意图中每个 "rank" 的输入数据都被分成了四块（对应4个 "rank"）。
* **操作过程:** 可以先将每一块上的数据看作被分为了四块，`in0` 变成了 `in00,in01,in02,in03`，`in1,in2,in3`同理，然后每块 `rank` 分别负责自己对应的那一部分，`rank0` 负责 `out0 = in00 + in10 + in20 + in30`，`rank1,rank2,rank3` 同理。
* **操作后:** 每个 "rank" 只接收到**对应于自己 "rank" 编号的那一块**数据的归约结果。
* **常见用途:** 这是 `All reduce` 的一种高效变体。在某些算法中，每个进程在归约后只需要自己负责的那部分结果，使用 `Reduce Scatter` 可以减少不必要的数据传输。

> 注意：这里只是简单的讲述了一下数据的起始和最终的分布情况，具体到实际计算中会有很多各种各样针对硬件设备的优化，会有着很多很多的方法。

## 1.2 Important detail

![Figure_2](../images/CS336/CS336_Lecture_7_Figure_3.png)

这两者在结果和性能能耗等各方面就是一样的，这里的意思就是 all reduce 可以看作是 reduce-scatter + all-gather 两次操作之和。

还有一个角度可以更清晰的理解为什么等式两边等价，那就是：为什么一次 All-Reduce 操作中，**每张 GPU** 需要发送和接收的数据量大约是数据量的 2 倍？
> 这里指 **Ring All-Reduce**（这是目前深度学习训练中最常用的算法，如 NVIDIA NCCL 库所采用）


如果待传输数据大小（比如梯度）为 $M$ 字节，GPU 数量为 $N$，那么每张 GPU 需要传输的总数据量是：
$$\text{Traffic per GPU} = 2 \cdot \frac{N-1}{N} \cdot M$$
当 GPU 数量 $N$ 较大时（比如 $N \ge 8$），$\frac{N-1}{N}$ 趋近于 1，因此我们通常直接估算为：**$2M$**。
Ring All-Reduce 详细推导过程:
为了理解这个 $2M$ 是怎么来的，我们需要看 Ring All-Reduce 算法的两个阶段。假设有 $N$ 个 GPU，围成一个环：
#### 第一阶段：Scatter-Reduce（分散归约）
* **目标**：把巨大的梯度向量切成 $N$ 块，让每个 GPU 最终负责计算其中 1 块的总和。
* **过程**：每个 GPU 将自己的数据传给下一个 GPU，经过 $N-1$ 步传输后，每个 GPU 都拥有了它负责的那一小块数据的“全剧终总和”。
* **通信量**：每个 GPU 发送了 $\frac{N-1}{N} \times M$ 的数据。
#### 第二阶段：All-Gather（全收集）
* **目标**：现在每个 GPU 只知道自己那一小块的和，它需要把这个和分享给其他所有 GPU，同时从其他 GPU 拿回剩下的部分。
* **过程**：每个 GPU 把它手里那块计算好的总和传给下一个 GPU，再经过 $N-1$ 步。
* **通信量**：每个 GPU 又发送了 $\frac{N-1}{N} \times M$ 的数据。
#### 总计
$$\text{Total} = \text{Scatter-Reduce} + \text{All-Gather} = \frac{N-1}{N} M + \frac{N-1}{N} M = 2 \frac{N-1}{N} M$$

**基本就是：梯度多大，通信量就是它的 2 倍。**

> 这是详细的理解过程：
> #### **1. 准备阶段**
> * **硬件：** $N$ 个 GPU，标记为 $GPU_0, GPU_1, \dots, GPU_{N-1}$。
> * **数据：** 模型梯度总大小为 $M$（字节）。
> * **切分：** 将每个 GPU 上的梯度向量逻辑切分为 $N$ 个小块（Chunks）。
>     * **单块大小：** $\frac{M}{N}$。
>     * **标记：** $GPU_i$ 持有的数据第 $j$ 块数据记为 $G_{i,j}$。
> #### **2. 计算阶段 (Local Computation)**
> * 各 GPU 根据分配给自己的那部分 Batch 数据，独立进行反向传播。
> * **结果：**
>     * $GPU_0$ 算出自己的梯度向量 $G_0$（包含 $G_{0,0}, \dots, G_{0,N-1}$）。
>     * $GPU_1$ 算出自己的梯度向量 $G_1$（包含 $G_{1,0}, \dots, G_{1,N-1}$）。
>     * ... 以此类推。
>     * *此时，所有 GPU 的梯度数值都不一样。*
> #### **3. 第一阶段：Reduce-Scatter (分散求和)**
> * **目标：** 让每个 GPU 负责计算**其中某一块**梯度的全局总和。
>     * 让 $GPU_0$ 拥有第 0 块的全局和：$\sum_{i=0}^{N-1} G_{i,0}$。
>     * 让 $GPU_1$ 拥有第 1 块的全局和：$\sum_{i=0}^{N-1} G_{i,1}$。
>     * ...
>     * 让 $GPU_k$ 拥有第 $k$ 块的全局和。
> * **过程：**
>     * 为了凑齐某一块的全局和，需要所有其他 $N-1$ 个 GPU 把它们手里的那块对应数据传出来。
>     * 每个 GPU 都需要把自己手里**除了自己负责的那一块以外**的所有数据块发送出去。
> * **通信量计算 (每张卡)：**
>     * 总共 $N$ 块，自己留 1 块，发送 $N-1$ 块。
>     * 发送量 = $(N-1) \times \text{单块大小}$
>     * $$\text{Traffic}_{scatter} = (N-1) \times \frac{M}{N} = \frac{N-1}{N} M$$
> #### **4. 第二阶段：All-Gather (全收集/广播)**
> * **当前状态：** Reduce-Scatter 结束后，$GPU_k$ 手里只有第 $k$ 块的完美梯度的和，其他 $N-1$ 块它是缺失的（或者说是旧的）。
> * **目标：** 让每个 GPU 都拥有**完整的**、包含所有 $N$ 块的全局梯度和。
> * **过程：**
>     * 每个 GPU 把自己刚刚算好的、负责的那**唯一一块**完整梯度和，分享给其他所有的 $N-1$ 个 GPU。
>     * （在 Ring 算法中，这意味着这块数据要在环里转 $N-1$ 次才能到达所有人手中）。
> * **通信量计算 (每张卡)：**
>     * 每个 GPU 需要发送自己负责的那 1 块数据给其他 $N-1$ 个节点（逻辑上）。
>     * 发送量 = $(N-1) \times \text{单块大小}$
>     * $$\text{Traffic}_{gather} = (N-1) \times \frac{M}{N} = \frac{N-1}{N} M$$
> #### **5. 总计 (Total Communication Cost)**
> 将两个阶段的通信量相加：
> $$
> \text{Total Traffic} = \text{Reduce-Scatter} + \text{All-Gather}
> $$
> $$
> = \left( \frac{N-1}{N} M \right) + \left( \frac{N-1}{N} M \right)
> $$
> $$
> = 2 \cdot \frac{N-1}{N} \cdot M
> $$
> #### **结论：**
> 当 $N$（GPU 数量）很大时，比如 $N=8, 64, 1024$：
> $$\frac{N-1}{N} \approx 1$$
> **所以，单次 All-Reduce 操作，每张 GPU 的总通信量近似为：**
> $$\mathbf{2M}$$

### 1） TPU 与 GPU的设计对比

![Figure_3](../images/CS336/CS336_Lecture_7_Figure_4.png)

| 维度 | **Google TPU** | **NVIDIA GPU** |
| :--- | :--- | :--- |
| **网络拓扑** | **2D/3D Toroidal Mesh (环形网格)** | **Hierarchical All-to-All (层级化全互联)** |
| **连接方式** | **芯片直连 (Direct Connect)**。<br>没有中心交换机，芯片只和邻居（上下左右）相连。 | **交换机网络 (Switch Fabric)**。<br>通过 NVSwitch/InfiniBand 交换机，实现任意节点间的高速跳转。 |
| **设计哲学** | **“专车专用，够用就好”**。<br>针对深度学习中最确定的通信模式（如矩阵乘法、All-Reduce）进行极致精简和优化。 | **“大力出奇迹，灵活第一”**。<br>用极其昂贵的网络带宽和交换设备，解决所有可能的通信问题，不让通信成为瓶颈。 |

### 2） Dense Training 计算场景

*代表模型：Llama 2, BERT, 早期 GPT-3*

在这类模型中，每一层的计算都需要所有参数参与，通信模式主要是 **All-Reduce**（同步梯度）。

  * **TPU 的优势：极致的性价比与能效**

      * **原因：** 最高效的 `All-Reduce` 算法通常是 **Ring-based（环状）** 的。这种算法只要求节点和“邻居”通信。
      * **结论：** TPU 的网格结构（Mesh）**天生匹配** Ring 算法。硬件上没有浪费任何一根连线，也没有昂贵的交换机闲置。这就是为什么课程中讲的“如果只考虑集体通信，TPU 的设计更合理”。它用最少的硬件实现了同等的效果。

  * **GPU 的表现：**

      * **原因：** GPU 的全互联网络当然也能跑 Ring 算法，但就像“开着法拉利去送快递”，虽然送得很快，但有些大材小用。NVIDIA 必须通过软件（NCCL）来模拟环状路径，虽然性能很强，但硬件成本（BOM Cost）极高。

### 3） Mixture-of-Experts 计算场景

*代表模型：GPT-4, Mixtral 8x7B, DeepSeek-V3*

这类模型引入了“稀疏性”。对于每一个 token，网络会动态选择几个“专家”（Experts）来处理，而这些专家可能分布在不同的显卡上。

  * **GPU 的优势：统治级的灵活性**

      * **原因：** MoE 的通信模式是 **All-to-All (Shuffle/Dispatch)**。
          * 比如：GPU 1 上的数据突然想找 GPU 56 上的专家，下一秒 GPU 1 上的数据又要找 GPU 200 上的专家。
          * 这种通信是**随机的、跳跃的**。
          * NVIDIA 的 **NVSwitch** 允许 GPU 1 直接跳到 GPU 56，延迟极低。
      * **结论：** 在处理这种乱序、长距离通信时，GPU 的全互联架构展现了碾压级的优势。

  * **TPU 的劣势：多跳带来的延迟 (Multi-hop Latency)**

      * **原因：** 在 Mesh 网络中，如果 TPU 1 想发数据给 TPU 56，数据可能需要经过 2-\>3-\>...-\>55 这么多跳。
          * 这不仅增加了**延迟**（Latency）。
          * 更糟糕的是会造成**数据拥塞**（Congestion）。中间的芯片不仅要算自己的数，还得帮别人传数据，带宽被挤占了。
      * **结论：** 虽然 TPU 也可以通过软件优化 MoE，但在大规模 MoE 训练上，TPU 的网格结构会面临严重的通信瓶颈，效率通常不如同档次的 GPU 集群。

# Part 2 Standard LLM parallelization primitives

这一部分讲述的是标准 LLM 的并行化原语。

并行化训练 LLM 的三大核心技术路线：
1. Data parallelism (数据并行，分割数据)：
   - Naïve data parallel
   - ZeRO levels 1-3
2. Model parallelism (模型并行，分割模型，训练大模型的方法)：
   - Pipeline parallel (流水线并行)
   - Tensor parallel (张量并行)
3. Activation parallelism (激活值并行，处理长序列)：
   - Sequence parallel (序列并行)

## 2.1 Naïve data parallel

### 1）**“朴素数据并行”（Naïve Data Parallelism）** 的基本原理及其优缺点。

它是分布式训练中最基础的形式，理解了它，就能理解为什么后来会出现 ZeRO、FSDP 这些更高级的技术。

1. 随机梯度下降（SGD）的标准公式：
$$\theta_{t+1} = \theta_t - \eta \sum_{i=1}^B \nabla f(x_i)$$
* **含义：** 为了更新模型参数 $\theta$，我们需要计算一个批次（Batch Size = $B$）中所有数据的梯度总和。
* **挑战：** 当 $B$ 很大时，单张卡算不过来，或者存不下这么多数据。

2. Naïve parallelism 的做法：
* **核心策略：** 将这个大小为 $B$ 的大批次数据，切分成 $M$ 份（假设有 $M$ 台机器/GPU）。
* **分工：** 每台机器只拿到一小部分数据（$B/M$），算出这一小部分的梯度。
* **同步：** 算完后，所有机器之间进行通信（Exchange gradients），把大家的梯度加起来平均，确保每个人更新后的模型参数是一模一样的。

3. How does this do?
从三个维度评估了这种方法：

* **计算扩展性 (Compute scaling) —— 好**
    * "each GPU gets B/M examples."
    * 任务被完美平分了。GPU 越多，每张卡处理的数据越少，计算速度越快。

* **通信开销 (Communication overhead) —— 一般**
    * "transmits 2x # params every batch."
    * 这正好验证了你刚才问的问题！每跑完一个 Batch，都需要进行一次 All-Reduce，通信量是模型参数量的 **2 倍**。
    * 如果 Batch 很大（计算时间很长），这部分通信时间可以被掩盖；如果模型太大或 Batch 太小，通信就会成为瓶颈。

* **内存扩展性 (Memory scaling) —— 极差**
    * "none. Every GPU needs # params at least"
    * **这是朴素数据并行的最大死穴。**
    * 即使你有 1000 张 GPU，**每张 GPU 都必须完整地存储一份模型参数**。
    * **结果：** 如果模型是 80GB，而 GPU 只有 40GB 显存，哪怕有 1 万张卡也跑不起来，因为单卡连模型都装不下。

4. **显存问题（Memory Issue）** 困境

在现代深度学习训练（特别是使用**混合精度训练**和 **Adam 优化器**）时，显存到底是被什么吃掉的？

- 基础部分（4 字节）：这是模型前向传播和反向传播必须的部分：
    * **2 字节 (FP/BF16 Model Parameters):** 模型当前的权重，通常使用半精度（FP16 或 BF16）来加速计算。
    * **2 字节 (FP/BF16 Gradients):** 反向传播计算出的梯度，也是半精度。

- 优化器状态部分（12 字节，即 "Optimizer State"）：这是为了保证训练稳定性和收敛效果（特别是使用 Adam 优化器时）所产生的巨大额外开销：
  * **4 字节 (FP32 Master Weights):** 这是一个**高精度（FP32）的权重备份**。
      * *为什么需要？* 因为 FP16 的精度不够，直接更新可能会导致数值下溢（Underflow）或精度丢失。优化器在 FP32 副本上进行更新，然后再转回 FP16 供下一次计算使用。
  * **4 字节 (Adam First Moment):** Adam 优化器记录的一阶动量（Momentum），通常需要 FP32 精度。
  * **4 字节 (Adam Second Moment):** Adam 优化器记录的二阶动量（Variance），通常也需要 FP32 精度。

* 也就是说总共需要 $2 + 2 + 4 + 4 + 4 = 16$ 字节/参数。
* **实际影响：** 如果使用朴素数据并行（Naïve Data Parallelism），**每张 GPU** 都必须完整地存储这一整套东西。
    * 例如：一个 **100亿参数（10B）** 的模型。
    * 显存需求：$10 \times 10^9 \times 16 \text{ Bytes} \approx 160 \text{ GB}$。
    * **结果：** 单张 A100 (80GB) 根本放不下，直接 OOM (Out of Memory)，连 Batch Size 为 1 都跑不起来。

### 2）ZeRO (Zero Redundancy Optimizer)的基本原理及优缺点

![Figure_4](../images/CS336/CS336_Lecture_7_Figure_5.png)

这张图是分布式训练领域非常经典的一张图，它展示了 **ZeRO (Zero Redundancy Optimizer)** 技术是如何一步步解决上一张图中提到的“显存爆炸”问题的。

核心思想是：**既然数据是并行的，为什么我们要让每块 GPU 都存一份完全一样的模型和状态呢？我们应该把这些数据切分（Partition/Shard）到不同的 GPU 上。**

图中从上到下展示了四个阶段（Baseline + ZeRO的三个阶段），每一行代表一种策略，右边的数字展示了在相同条件下显存占用的巨大变化（从 120GB 降到 1.9GB）。

1. 图例说明：首先看底部的三个颜色条，这对应了显存占用的三个主要部分：
   * 🟦 **蓝色 (Parameters):** 模型参数 (FP16/BF16)。
   * 🟧 **橙色 (Gradients):** 梯度 (FP16/BF16)。
   * 🟩 **绿色 (Optimizer States):** 优化器状态 (FP32 Master weights + Momentum + Variance)。**这是占用显存最大的部分**。

2. 符号说明
   * $\Psi$: 模型参数量（例如 7.5 Billion）。
   * $K$: 优化器状态的倍率（这里取 12，上一节说了）。
   * $N_d$: GPU 的数量（这里假设有 64 张卡）。

3. ZeRO 的三个阶段

    **第一行：Baseline (朴素数据并行)**
    * **状态：** 所有的 GPU ($gpu_0$ 到 $gpu_{N-1}$) 都拥有**完整**的蓝色、橙色和绿色条。
    * **逻辑：** 每张卡都存一份完整的副本。
    * **显存公式：** $(2 + 2 + K) * \Psi$。即所有东西都乘 1。
    * **结果：** **120 GB**。这非常大，单张 A100 (80GB) 直接报错 (OOM)。

    **第二行：$P_{os}$ (ZeRO Stage 1)**
    * **含义：** Partition Optimizer States (切分优化器状态)。
    * **变化：** 注意看 **绿色条（Optimizer States）**。它不再是完整的长条，而是被切碎了，每张 GPU 只维护其中的 $\frac{1}{N_d}$。
    * **逻辑：** $gpu_0$ 只负责更新模型的前 1/64 的参数，$gpu_1$ 负责接下来的 1/64，以此类推。蓝色和橙色条保持不变（依然复制）。
    * **显存公式：** $2\Psi + 2\Psi + \frac{K*\Psi}{N_d}$。
    * **结果：** **31.4 GB**。
    * **点评：** 仅仅切分了优化器状态，显存占用就下降了 4 倍！这是性价比最高的一步，也是目前工业界最常用的默认配置（DeepSpeed ZeRO-1）。

    **第三行：$P_{os+g}$ (ZeRO Stage 2)**
    * **含义：** Partition Optimizer States + Gradients (切分优化器状态 + 梯度)。
    * **变化：** 现在 **橙色条（Gradients）** 也被切分了。
    * **逻辑：** 当每张卡算出梯度后，不需要把完整梯度的结果汇总给每个人。只需要把自己那部分梯度发送给负责对应参数更新的 GPU 即可。
    * **显存公式：** $2\Psi + \frac{(2+K)*\Psi}{N_d}$。
    * **结果：** **16.6 GB**。
    * **点评：** 显存进一步减半，且通信开销并没有显著增加。

    **第四行：$P_{os+g+p}$ (ZeRO Stage 3)**
    * **含义：** Partition Optimizer States + Gradients + Parameters (全切分)。
    * **变化：** 连 **蓝色条（Parameters）** 也被切分了。
    * **逻辑：** 每张 GPU 显存里**只有自己负责的那一小部分参数**。
        * *这怎么计算呢？* 当 $gpu_0$ 做前向传播需要用到它没有的参数时，它会临时从其他 GPU 那里广播（Broadcast）过来，算完立刻删掉。
    * **显存公式：** $\frac{(2+2+K)*\Psi}{N_d}$。即总显存除以 GPU 数量。
    * **结果：** **1.9 GB**。
    * **点评：** 实现了真正的**显存线性扩展**。你有 64 张卡，就能跑比单卡大 64 倍的模型。虽然通信量会增加（因为要频繁广播参数），但在大模型训练中是必不可少的。

首先明确一下：**ABCDE** 符号的意义，**A=FP16权重, B=FP32权重, C=一阶动量, D=二阶动量, E=FP16梯度**，我们有神经网络 **4层（L1-L4）** 和 GPU 四张 **4卡（G0-G3）**。
在朴素数据并行中，GPU 算出数据后，**产出的只有 E（梯度）**。A、B、C、D 是用来做更新的，它们在反向传播过程中并不产生，而是在最后一步更新时才用到（因为大家都有完整的 ABCD，交换完 E 后直接更新就又都是完整的 ABCD ）。
 * **Naïve DP 最大的浪费在于：** 大家交换完 E 之后，所有人手里的 E 是一样的，A 也是一样的，所以大家**各自**在自己的显存里，用同样的公式计算了一遍 B、C、D 的更新。**明明算的是一样的东西，却算了 4 遍，存了 4 份。**
 * **模型：** 4层，$L_1, L_2, L_3, L_4$。
 * **数据：** 0, 1, 2, 3 号数据包。
 * **分工（ZeRO的核心）：** 我们规定：
      * **G0** 是 $L_1$ 的管家。
      * **G1** 是 $L_2$ 的管家。
      * **G2** 是 $L_3$ 的管家。
      * **G3** 是 $L_4$ 的管家。
1. 朴素数据并行 (Naïve DP)
  每个人都要扛所有的重物，做重复的计算。
  * **存储（显存里有什么）：**
      * **G0, G1, G2, G3 全都有：** $L_1-L_4$ 的全套 **ABCDE**。
  * **计算过程：**
      1.  **Forward/Backward:**
          * G0 算数据0，算出 $L_1-L_4$ 的梯度 **$E_0$**。
          * G1 算数据1，算出 **$E_1$**... 以此类推。
      2.  **通信 (All-Reduce):**
          * 大家停下来，互相交换 **E**。
          * **注意：只交换 E！** 不交换 B、C、D。
          * 交换后，G0-G3 手里都有了完整的**总梯度 E** (即 $E_{total} = E_0+E_1+E_2+E_3$)。
      3.  **Update (最浪费的一步):**
          * G0 用总 E，去更新自己的 B, C, D，算出新的 A。
          * G1 用**一样**的总 E，去更新自己的 B, C, D，算出**一模一样**的新 A。
          * ...
          * **结局：** 4张卡显存全满，且存的数值完全一样。
2. ZeRO Stage 1：切分优化器 ($P_{os}$)
既然 Update 步骤大家算的都一样，那我们**分工更新**。
  * **存储（显存里有什么）：**
      * **A (权重):** 大家都有完整的 $L_1-L_4$（为了算梯度）。
      * **E (梯度):** 大家都有完整的 $L_1-L_4$（为了方便）。
      * **B, C, D (优化器状态):** **切分了！**
          * G0 **只存** $L_1$ 的 B,C,D。
          * G1 **只存** $L_2$ 的 B,C,D...
  * **计算过程：**
    1.  **Forward/Backward:** 同上，大家各自算出局部梯度 E（这里的局部是指对数据的局部，每个人存储的梯度大小还是一样的）。
    2.  **通信 (Reduce Scatter):** 同上，大家交换 E，每个人都拿到各自负责那一部分参数 L 对应的梯度 E。
    3.  **Update (变化开始):**
          * **G0:** "我是 $L_1$ 的管家，我有 $L_1$ 的完整的梯度 E1，以此对 L1 的 B,C,D 进行更新。"G0 更新 $L_1$ 的 A。
          * **G1:** "我是 $L_2$ 的管家，我有 $L_2$ 的完整的梯度 E2，以此对 L2 的 B,C,D 进行更新。"G1 更新 $L_2$ 的 A。
          * 此时，G0 手里的 $L_1$ 是新的，但 $L_2, L_3, L_4$ 还是旧的。
    4.  **通信 (All Gather):**
          * G0 把新的 $L_1$ 权重 A 广播给所有人。
          * G1 把新的 $L_2$ 权重 A 广播给所有人...
          * **结局：** 大家又都有了完整的新 A，准备下一轮。**节省了 3/4 的 B,C,D 显存。**
3. ZeRO Stage 2：切分梯度 ($P_{os+g}$)
问题：G0 既然只负责更新 $L_1$，那它为什么要一直囤积 $L_2, L_3, L_4$ 的梯度 E 呢？
>
* **存储（显存里有什么）：**
    * **A (权重):** 大家都有完整的（为了进行计算）。
    * **B, C, D (优化器状态):** 切分了（同 Stage 1）。
    * **E (梯度):** **也切分了！** G0 显存里最终只留 $L_1$ 的总梯度，其他层的梯度算完就扔。
>
* **计算过程（时序变了！）：**
    1.  **Forward:** 正常前向传播，大家算出 Loss。
    2.  **Backward + Communication (边算、边传、边扔):**
        * **算 $L_4$:** 大家各自算出 $L_4$ 的梯度。
        * **立刻 Reduce-Scatter:** 大家把 $L_4$ 的梯度发给“管家 G3”。
        * **立刻 Delete:** 发完后，G0, G1, G2 **立刻删除** 显存里的 $L_4$ 梯度（释放空间！）。G3 保留 $L_4$ 的总梯度。
        * **算 $L_3$:** 显存腾出来了，大家接着算 $L_3$ 的梯度...
        * **立刻 Reduce-Scatter & Delete...**
        * **结局:** Backward 结束时，G0 显存里**只有 $L_1$ 的总梯度**。显存峰值从未爆表。
    3.  **Update:**
        * G0 用手里的 $L_1$ 总梯度 + $L_1$ 的 B,C,D 状态，更新 $L_1$ 的 A。
    4.  **通信 (All-Gather):**
        * G0 把更新好的 $L_1$ 权重 A 广播给大家。
        * G1 把更新好的 $L_2$ 权重 A 广播给大家...
* **结局：** **不但节省了 3/4 的 B,C,D，还节省了 3/4 的 E 显存。** 此时显存里只剩下一份完整的 A 是冗余的了。



4. ZeRO Stage 3：切分参数 ($P_{os+g+p}$)
如果说 Stage 2 是“用完梯度就扔”，那么 Stage 3 就是 **“连模型权重（Parameters）本身都不存完整的，用的时候现借，用完立马扔”**。

   - **核心概念 ：Shard Everything**

     1.  **极端切分：**
         * $P_{os+g+p}$：不仅仅是优化器和梯度被切分了，连**参数 Parameters** 也被切分了。
         * **现状：** G0 手里只有 $L_1$ 的权重。G1 手里只有 $L_2$ 的权重。**没有任何一个人拥有完整的模型。**
         * **收益：** 显存占用降到了极致（1.9GB）。显存占用随 GPU 数量线性减少。

     2.  **核心问题：**
         * 模型计算（Forward/Backward）是需要完整权重的。比如 G0 要算 $L_2$ 的输出，但他手里没有 $L_2$ 的权重，怎么办？
         * **策略：** **On-demand (按需获取)**。走到哪一层，就问别人借哪一层的权重。

   - **基础流程：how it works**:一个 **Layer（层）** 在训练时的生命周期核心逻辑是：**All-Gather (借) -> Compute -> Free**。

     1. **Forward Pass (前向传播):**
     假设我们要算 $L_2$ 层，但我是 G0（我不持有 $L_2$）：
        1.  **All-Gather Parameters:** G0 向持有 $L_2$ 的管家（比如 G1）发起请求，把 $L_2$ 的权重拉过来。
        2.  **Forward (Local):** 现在 G0 显存里有完整的 $L_2$ 权重了，开始计算前向传播。
        3.  **Free Full Weights (关键点):** 算完后，G0 **立刻删除** $L_2$ 的权重。
            * *为什么要删？* 为了省显存给下一层 ($L_3$) 腾地方。
            * *此时 G0 显存里又是空的了。*

     2. **Backward Pass (反向传播):**
     等到反向传播回来，又要算 $L_2$ 的梯度了：
        1.  **All-Gather Parameters:** 没错！因为刚才删了，所以现在**必须重新把 $L_2$ 的权重再借一遍**。
        2.  **Backward (Local):** 算出 $L_2$ 的梯度。
        3.  **Reduce-Scatter Gradients:** (同 Stage 2) 把算好的梯度发给 $L_2$ 的管家 G1。
        4.  **Free Full Weights:** 再次删除 $L_2$ 的权重。

   **代价：** 我们会发现通信量变大了（权重被反复传输了两次），是用**时间（通信带宽）换空间（显存）**。

   - **进阶优化 (Overlapping / Prefetching)**：解决了痛点，**“如果每次都要等权重传过来才能开始算，那 GPU 岂不是总是要停下来等人？”**，这就引入了 **Overlapping (通信与计算重叠)** 和 **Prefetching (预取)** 技术。

     1.  **三条流水线 (Streams):**
         * **GPU Comp. Stream:** 负责计算 (FWD/BWD)。
         * **GPU Comm. Stream:** 负责通信 (All-Gather / Reduce-Scatter)。
         * **CPU:** 发号施令。

     2.  **工作原理:**
         * **FWD0 (计算第0层):** 当 GPU 正在拼命计算第 0 层时...
         * **AG1 (All-Gather 第1层):** 通信流并没有闲着，它已经在**后台**偷偷把第 1 层的权重拉过来了（预取）。
         * **无缝衔接:** 等 GPU 算完第 0 层，第 1 层的权重刚好到货，直接开始算 `FWD1`，不需要停顿等待。

     3.  **公式 $ (W_1 W_0 + W_2 W_0)x = y $:**
         * 这表示把大矩阵拆成小块计算，利用线性代数的性质，使得结果等价。
