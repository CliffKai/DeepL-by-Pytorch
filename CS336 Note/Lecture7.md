# 1.Collective Communication

## 1.1 basics about collective communication

首先来认识一下并行计算中五种最核心的 “集体通信” (Collective Communication) 操作。
![Figure_1](../images/CS336/CS336_Lecture_7_Figure_2.png)

这张图详细解释了并行计算中五种最核心的 **“集体通信” (Collective Communication)** 操作。

"集体通信"是指在一个并行计算集群中（例如，由多个 GPU 或多个服务器节点组成），所有参与的进程（图中的 "rank" 可以理解为一个个独立的计算单元，比如一个 GPU）**同时参与**的数据交换操作。

这些操作是实现大规模分布式计算的基石。

以下是图中五种操作的详细讲解：

### 1） All reduce (全归约)

* **操作前:** 每个 "rank"（rank 0, 1, 2, 3）都持有一份自己的输入数据（`in0`, `in1`, `in2`, `in3`）。
* **操作过程:**
    1.  **Reduce (归约):** 系统对所有 "rank" 的输入数据执行一个归约操作（例如 `sum` 求和）。`result = in0 + in1 + in2 + in3`。
    2.  **All (全部):** 将这个最终的计算结果（`out`）分发给**所有**的 "rank"。
* **操作后:** **所有 "rank"** 都拥有了相同的、归约后的最终结果 `out`。
* **常见用途:** 在数据并行训练中，每个GPU计算出自己的梯度（`in0`...`in3`），`All reduce` 用来计算所有梯度的总和，并将这个总和（`out`）分发回给每个GPU，以便它们能同步更新模型。

### 2） Broadcast (广播)

* **操作前:** 只有一个 "rank"（称为 "root"，例如图的 `rank 2`）持有数据（`in`）。
* **操作过程:** "root" 进程将其数据（`in`）发送给组内所有其他的进程。
* **操作后:** **所有 "rank"** (包括 "root" 自己) 都拥有了 `rank 2` 原始数据的副本（`out`）。
* **常见用途:** 在训练开始时，由 "root" 进程（rank 0 或 2）加载模型权重，然后 `Broadcast` 给所有其他GPU，确保大家从同一个起点开始。

### 3） Reduce (归约)

* **操作前:** 和 `All reduce` 一样，每个 "rank" 都持有一份自己的输入数据（`in0`, `in1`, `in2`, `in3`）。
* **操作过程:** 系统对所有 "rank" 的输入数据执行一个归约操作（例如 `sum` 求和）。
* **操作后:** **只有 "root" 进程**（图中指定了 `rank 2`）接收并存储了这个最终的归约结果（`out`）。其他 "rank" 不会收到这个结果。
* **区别 (Reduce vs All reduce):** `Reduce` 只把结果给 "root"；`All reduce` 把结果给所有人。

### 4） All Gather (全收集)

* **操作前:** 每个 "rank" 都持有**一部分**数据（`in0`, `in1`, `in2`, `in3`）。
* **操作过程:** 系统从所有 "rank" 收集它们各自的数据，并将这些数据**按 "rank" 的顺序拼接**（concatenate）成一个完整的大数据集。然后，系统将这个完整的数据集分发给所有 "rank"。
* **操作后:** **所有 "rank"** 都拥有了完整的、拼接后的数据集（`out`），这个 `out` 包含了 `in0`, `in1`, `in2`, `in3` 的所有内容。
* **常见用途:** 在模型并行或张量并行中，如果一个张量被切分到不同GPU上，`All Gather` 可以将这些分片收集起来，在每个GPU上还原出完整的张量。

### 5） Reduce Scatter (归约-分散)

* **操作前:** 每个 "rank" 都持有一份**完整的输入数据**。请注意图中每个 "rank" 的输入数据都被分成了四块（对应4个 "rank"）。
* **操作过程:** 可以先将每一块上的数据看作被分为了四块，`in0` 变成了 `in00,in01,in02,in03`，`in1,in2,in3`同理，然后每块 `rank` 分别负责自己对应的那一部分，`rank0` 负责 `out0 = in00 + in10 + in20 + in30`，`rank1,rank2,rank3` 同理。
* **操作后:** 每个 "rank" 只接收到**对应于自己 "rank" 编号的那一块**数据的归约结果。
* **常见用途:** 这是 `All reduce` 的一种高效变体。在某些算法中，每个进程在归约后只需要自己负责的那部分结果，使用 `Reduce Scatter` 可以减少不必要的数据传输。

> 注意：这里只是简单的讲述了一下数据的起始和最终的分布情况，具体到实际计算中会有很多各种各样针对硬件设备的优化，会有着很多很多的方法。

## 1.2 Important detail

![Figure_2](../images/CS336/CS336_Lecture_7_Figure_3.png)

![Figure_3](../images/CS336/CS336_Lecture_7_Figure_4.png)

### 1） TPU 与 GPU的设计对比

| 维度 | **Google TPU** | **NVIDIA GPU** |
| :--- | :--- | :--- |
| **网络拓扑** | **2D/3D Toroidal Mesh (环形网格)** | **Hierarchical All-to-All (层级化全互联)** |
| **连接方式** | **芯片直连 (Direct Connect)**。<br>没有中心交换机，芯片只和邻居（上下左右）相连。 | **交换机网络 (Switch Fabric)**。<br>通过 NVSwitch/InfiniBand 交换机，实现任意节点间的高速跳转。 |
| **设计哲学** | **“专车专用，够用就好”**。<br>针对深度学习中最确定的通信模式（如矩阵乘法、All-Reduce）进行极致精简和优化。 | **“大力出奇迹，灵活第一”**。<br>用极其昂贵的网络带宽和交换设备，解决所有可能的通信问题，不让通信成为瓶颈。 |

### 2） Dense Training 计算场景

*代表模型：Llama 2, BERT, 早期 GPT-3*

在这类模型中，每一层的计算都需要所有参数参与，通信模式主要是 **All-Reduce**（同步梯度）。

  * **TPU 的优势：极致的性价比与能效**

      * **原因：** 最高效的 `All-Reduce` 算法通常是 **Ring-based（环状）** 的。这种算法只要求节点和“邻居”通信。
      * **结论：** TPU 的网格结构（Mesh）**天生匹配** Ring 算法。硬件上没有浪费任何一根连线，也没有昂贵的交换机闲置。这就是为什么课程中讲的“如果只考虑集体通信，TPU 的设计更合理”。它用最少的硬件实现了同等的效果。

  * **GPU 的表现：**

      * **原因：** GPU 的全互联网络当然也能跑 Ring 算法，但就像“开着法拉利去送快递”，虽然送得很快，但有些大材小用。NVIDIA 必须通过软件（NCCL）来模拟环状路径，虽然性能很强，但硬件成本（BOM Cost）极高。

### 3） Mixture-of-Experts 计算场景

*代表模型：GPT-4, Mixtral 8x7B, DeepSeek-V3*

这类模型引入了“稀疏性”。对于每一个 token，网络会动态选择几个“专家”（Experts）来处理，而这些专家可能分布在不同的显卡上。

  * **GPU 的优势：统治级的灵活性**

      * **原因：** MoE 的通信模式是 **All-to-All (Shuffle/Dispatch)**。
          * 比如：GPU 1 上的数据突然想找 GPU 56 上的专家，下一秒 GPU 1 上的数据又要找 GPU 200 上的专家。
          * 这种通信是**随机的、跳跃的**。
          * NVIDIA 的 **NVSwitch** 允许 GPU 1 直接跳到 GPU 56，延迟极低。
      * **结论：** 在处理这种乱序、长距离通信时，GPU 的全互联架构展现了碾压级的优势。

  * **TPU 的劣势：多跳带来的延迟 (Multi-hop Latency)**

      * **原因：** 在 Mesh 网络中，如果 TPU 1 想发数据给 TPU 56，数据可能需要经过 2-\>3-\>...-\>55 这么多跳。
          * 这不仅增加了**延迟**（Latency）。
          * 更糟糕的是会造成**数据拥塞**（Congestion）。中间的芯片不仅要算自己的数，还得帮别人传数据，带宽被挤占了。
      * **结论：** 虽然 Google 也可以通过软件优化 MoE，但在大规模 MoE 训练上，TPU 的网格结构会面临严重的通信瓶颈，效率通常不如同档次的 GPU 集群。

