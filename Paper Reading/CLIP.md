Learning Transferable Visual Models From Natural Language Supervision

# Abstract

1. 背景与问题提出

- 背景：当前（2021年）最先进的CV模型的训练是使用了一组固定的、预定义的类别来进行训练的。
- 问题1：这种“ **受限监督形式** ”的问题在于模型的泛化能力和可用性较差。
- 问题2：如果要让模型识别新的概念，就必须重新标注大量的新数据并且重新训练，费时费力。

2. 本文新方法：使用图文对进行训练

- 一种更具潜力的做法：直接利用 **与图像相关的原始文本** （如图像的标题、描述等）进行训练。
- 这种方法可以利用很多现有的图文数据，而不依赖人工标注的分类标签。

3. CLIP的训练任务

- 训练任务：在多个图像和文本描述中，**预测哪段文本与哪张图像是匹配的**
- 特性：这种对比学习的方式效率高且扩展性强
- 数据集：4亿对图像和文本数据

4. 模型的特殊之处

- 预训练之后，可以使用自然语言来指代模型学习到的视觉概念，从而能够实现模型在下游任务上的**zero-shot**

5. 评估与实验

- 作者团队使用了超过30个现有的计算机视觉数据集来进行模型的评估，其中包括：
    - OCR
    - 视频中动作识别
    - 细颗粒度 图像分类

6. 结果

- 在大部分任务上表现非常好，即使在一些没有针对具体任务进行预训练的数据集上也能达到与**完全监督模型相近或者更好的结果**
- 比如：CLIP没有使用任何一张ImageNet上的数据集来进行训练，但是却能和在ImageNet上训练出来的ResNet-50打成平手

# Introduction and Motivating Work

近年来（2021年前），NLP领域通过从原始文本中预训练取得了很多重大的突破，比如BERT、GPT、T5等。       
上述方法都使用了 **任务无关（task-agnostic）** 的目标，比如自回归（autoregressive）和掩码语言建模（masked language modeling），这些模型的目标函数与下游任务无关，只是通过预训练得到一个非常好的非常能泛化的特征，并且这些模型随着算力、数据量和模型容量的提升表现越来越好。          
这些方法都是 **text-to-text** 的结构，并没有做一个特殊的分类任务，模型架构与下游任务无关，所以的当模型直接运用于下游任务的时候就不需要再费尽心思去研究一个针对特定任务的输出头或是针对特定数据集的特殊处理（比如GPT3，并不是针对一个特定任务的模型，既能翻译又能产出文字内容又能规划等等，最重要的是在大多数任务上他并不需要特定领域的数据去专门训练，或者只需要一点点的数据进行微调即可）。     

这些结果证明了，在NLP领域，这种 text-to-text ，利用自监督信号去训练整个模型的框架下，大规模的未标注的数据集是要优于哪些高质量的标注了的数据集的。      
但是在CV领域，一般做法还是在ImageNet上进行预训练，这样训练出的模型会有着诸多限制。

接下来作者介绍了三个最相关的工作：

## 1. **VirTex: Learning Visual Representations from Textual Annotations**

**作者**：Desai & Johnson
**会议**：CVPR 2020
**链接**：[https://arxiv.org/abs/2006.06666](https://arxiv.org/abs/2006.06666)

### 核心思想：

VirTex 提出了一种用图像描述文本来训练视觉模型的方法。与以往用“图像分类标签”进行监督不同，VirTex 使用 **图像描述（Image Captions）** 训练图像编码器。

### 方法：

VirTex 将训练任务设计成一个**图像条件语言建模任务**：

* 给定一张图像，训练一个视觉编码器 + 文本解码器去“生成”与该图像相对应的描述文字。
* 用到了 ResNet（视觉编码器）+ Transformer（语言生成器）架构。

这个过程的本质是：训练图像编码器时，要让它提取到足够丰富的信息，使得语言模型能够基于这些视觉特征成功生成图像描述。

### 贡献：

* VirTex 展示了：视觉模型可以通过语言监督来获得与传统分类监督（如 ImageNet）相媲美甚至更好的表征。
* 提出了多种联合训练策略（如预测词、预测句子），并验证其有效性。


## 2. **ICMLM: Weakly Supervised Vision-and-Language Pretraining**

**作者**：Bulent Sariyildiz, Rambabu Ringgeler, Pascal Fua
**会议**：ECCV 2020
**链接**：[https://arxiv.org/abs/2006.07133](https://arxiv.org/abs/2006.07133)

### 核心思想：

ICMLM（Image Conditioned Masked Language Modeling）提出了一种“图像条件的掩码语言建模”方式，用于训练视觉表示。它强调利用**弱标注数据**（比如图文对）训练视觉模型。

### 方法：

* 类似于 BERT 的掩码语言建模（MLM），但在此基础上加入了图像作为条件：

  > 给定一张图像和一个对应的描述文本（部分词被 mask 掉），训练模型根据图像和上下文预测被 mask 的词。

* 模型结构上是：图像编码器（ResNet）提取视觉特征 → 加入到 Transformer 的输入中 → 执行 MLM。

### 贡献：

* 提出了一种更简单有效的图文联合预训练策略，仅用图像和文字即可学习强视觉特征。
* 与传统监督（如分类）方式相比，ICMLM 显示出更强的跨模态迁移能力。

## 3. **ConVIRT: Contrastive Learning of Medical Visual Representations from Paired Images and Text**

**作者**：Zhang et al.
**会议**：ACL 2020
**链接**：[https://arxiv.org/abs/2010.00747](https://arxiv.org/abs/2010.00747)

### 核心思想：

ConVIRT 是一项面向**医学图像分析**的研究工作，它提出了对比学习方法，用图文对训练视觉模型。

### 方法：

* 给定一张医学图像和一段对应的描述报告（如放射科诊断文本），ConVIRT 训练模型去**最大化图像和对应文本的相似度**，同时**最小化图像与非对应文本的相似度**。

* 使用对比损失（Contrastive Loss），和 CLIP 类似，基于 InfoNCE。

* 图像编码器和文本编码器分别为 ResNet 和 Transformer（或 GRU），训练时将图文对映射到同一个共享向量空间。

### 贡献：

* 在没有使用 ImageNet 标签的情况下，ConVIRT 训练出的视觉模型在多个医学图像下游任务中表现超过有监督模型。
* 证明了对比学习可以在图文配对数据上有效训练视觉特征。

## Motivate

总结一句话：能否训练一个视觉模型，它不依赖于固定的分类标签，而是可以通过自然语言描述来泛化处理几乎任何视觉任务？

