## ğŸ§© ç±»ï¼š`LLMBackbone`

### ğŸ“Œ ç±»çš„ä½œç”¨

* è¿™æ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼ˆç»§æ‰¿è‡ª `ABC` å’Œ `nn.Module`ï¼‰ï¼Œç”¨äºå®šä¹‰ **OpenVLA ä¸­ LLMï¼ˆå¦‚Qwenã€LLaMAã€OPTç­‰ï¼‰çš„ç»Ÿä¸€æ¥å£**ã€‚
* æ‰€æœ‰å­ç±»ï¼ˆå…·ä½“æ¨¡å‹å®ç°ï¼‰éƒ½éœ€è¦å®ç°å…¶ä¸­çš„æŠ½è±¡æ–¹æ³•ã€‚
* è¿™ä¸ªç±»çš„æ ¸å¿ƒä»»åŠ¡æ˜¯å®šä¹‰ï¼š**LLM çš„åŠ è½½ã€å‰å‘ä¼ æ’­ã€tokenizer è·å–ã€embedding ç”Ÿæˆã€FSDPåŒ…è£…ç­–ç•¥ã€gradient checkpointing ç­‰ç­‰ç»Ÿä¸€è¡Œä¸ºã€‚**

---

## ğŸ“š è¿™ä¸ªç±»ä¸­åŒ…å«çš„æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰å’Œå±æ€§ä¸€è§ˆï¼š

| æˆå‘˜ç±»å‹                              | åç§°   | ä½œç”¨ç®€è¿°                           |
| --------------------------------- | ---- | ------------------------------ |
| `__init__`                        | æ„é€ å‡½æ•° | åˆå§‹åŒ– LLM çš„åŸºæœ¬å±æ€§                  |
| `get_tokenizer()`                 | å®ä¾‹æ–¹æ³• | è·å– `self.tokenizer`            |
| `get_fsdp_wrapping_policy()`      | æŠ½è±¡æ–¹æ³• | ç”¨äºè¿”å› FSDP çš„åŒ…è£…ç­–ç•¥                |
| `enable_gradient_checkpointing()` | æŠ½è±¡æ–¹æ³• | å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹                        |
| `forward(...)`                    | æŠ½è±¡æ–¹æ³• | LLM å‰å‘æ¨ç†ä¸»å‡½æ•°                    |
| `embed_input_ids()`               | æŠ½è±¡æ–¹æ³• | å°† `input_ids` è½¬ä¸º embedding å‘é‡  |
| `prompt_builder_fn`               | æŠ½è±¡å±æ€§ | è¿”å› prompt æ„é€ å™¨ç±»å‹                |
| `transformer_layer_cls`           | æŠ½è±¡å±æ€§ | è¿”å› transformer å±‚çš„ç±»             |
| `half_precision_dtype`            | æŠ½è±¡å±æ€§ | è¿”å›æ··åˆç²¾åº¦æ¨ç†æ‰€ç”¨çš„ dtype              |
| `last_layer_finetune_modules`     | æŠ½è±¡å±æ€§ | è¿”å›éœ€è¦ finetune çš„æœ€åå‡ å±‚æ¨¡å—          |
| `embed_dim`                       | å±æ€§   | è¿”å› embedding ç»´åº¦å¤§å°ï¼ˆhidden sizeï¼‰ |
| `pad_token_id`                    | å±æ€§   | è¿”å› tokenizer çš„ pad token ID    |

---

## ğŸ” é€ä¸ªè¯¦ç»†è®²è§£ï¼š

---

### ### 1ï¸âƒ£ `__init__(self, llm_backbone_id: str)`

#### âœ… åŠŸèƒ½ï¼š

åˆå§‹åŒ– LLMBackbone ç±»çš„åŸºç¡€ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸€ä¸ªå”¯ä¸€æ ‡è¯†ç¬¦ `llm_backbone_id`ï¼ŒåŒæ—¶ä¸º `llm` æ¨¡å‹å’Œ `tokenizer` é¢„ç•™äº†ç©ºå€¼ï¼Œä¾›å­ç±»åˆå§‹åŒ–å¡«å……ã€‚

#### âœ… å‚æ•°ï¼š

* `llm_backbone_id`: ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¦‚ `"Qwen-2.5-VL-3B"`ï¼Œç”¨äºå”¯ä¸€æ ‡è¯†å½“å‰ LLMã€‚

#### âœ… å†…éƒ¨å®ç°ï¼š

```python
super().__init__()
```

è°ƒç”¨ `nn.Module` çš„åˆå§‹åŒ–å™¨ï¼Œå»ºç«‹åŸºç¡€çš„ PyTorch æ¨¡å—ã€‚

```python
self.identifier = llm_backbone_id
```

è®¾ç½®è¯¥æ¨¡å‹çš„å”¯ä¸€æ ‡è¯†ã€‚

```python
self.llm: PreTrainedModel = None
self.tokenizer: PreTrainedTokenizerBase = None
```

å£°æ˜å°†ç”±å­ç±»å¡«å……çš„ LLM å’Œ Tokenizer å®ä¾‹ï¼Œç±»å‹é™å®šä¸º HF çš„ `PreTrainedModel` å’Œ `PreTrainedTokenizerBase`ã€‚

---

### 2ï¸âƒ£ `get_tokenizer(self)`

#### âœ… åŠŸèƒ½ï¼š

è·å–å½“å‰æ¨¡å‹æ‰€ä½¿ç”¨çš„ tokenizerã€‚

#### âœ… è¿”å›å€¼ï¼š

* `PreTrainedTokenizerBase` ç±»å‹çš„ tokenizerã€‚

---

### 3ï¸âƒ£ æŠ½è±¡æ–¹æ³• `get_fsdp_wrapping_policy(self) -> Callable`

#### âœ… åŠŸèƒ½ï¼š

ç”±å­ç±»å®šä¹‰â€”â€”è¿”å›ç”¨äº FSDPï¼ˆFully Sharded Data Parallelï¼‰è®­ç»ƒçš„ transformer åŒ…è£…ç­–ç•¥ã€‚

#### âœ… ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

* ç”¨äºè‡ªåŠ¨ç¡®å®šå“ªäº›æ¨¡å—åº”è¯¥è¢« `FSDP` åŒ…è£¹ã€‚
* å¸¸ä¸ `transformer_auto_wrap_policy()` æ­é…ä½¿ç”¨ã€‚

---

### 4ï¸âƒ£ æŠ½è±¡æ–¹æ³• `enable_gradient_checkpointing(self)`

#### âœ… åŠŸèƒ½ï¼š

* å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰ä»¥èŠ‚çœæ˜¾å­˜ã€‚

---

### 5ï¸âƒ£ æŠ½è±¡æ–¹æ³• `forward(...) -> CausalLMOutputWithPast`

#### âœ… åŠŸèƒ½ï¼š

LLM çš„æ ¸å¿ƒå‰å‘æ¨ç†é€»è¾‘ï¼Œç”±å­ç±»å®ç°ã€‚

#### âœ… å‚æ•°è¯´æ˜ï¼š

| å‚æ•°å                    | ç±»å‹                  | å«ä¹‰                              |
| ---------------------- | ------------------- | ------------------------------- |
| `input_ids`            | `LongTensor`        | tokenized input ID              |
| `attention_mask`       | `Tensor`            | æ³¨æ„åŠ›é®ç½©                           |
| `position_ids`         | `LongTensor`        | ä½ç½®ç¼–ç ç´¢å¼•                          |
| `past_key_values`      | `List[FloatTensor]` | ä¸Šä¸€è½®ç¼“å­˜ KV å€¼ï¼ˆfor cacheï¼‰           |
| `inputs_embeds`        | `FloatTensor`       | embedding ç›´æ¥è¾“å…¥ï¼ˆä¸ input\_ids äº’æ–¥ï¼‰ |
| `labels`               | `LongTensor`        | è‹¥ç”¨äºè®­ç»ƒï¼Œæä¾› label ä»¥è®¡ç®— loss         |
| `use_cache`            | `bool`              | æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆç”Ÿæˆæ—¶åŠ é€Ÿï¼‰                   |
| `output_attentions`    | `bool`              | æ˜¯å¦è¿”å› attention scores           |
| `output_hidden_states` | `bool`              | æ˜¯å¦è¿”å›æ¯å±‚ hidden states            |
| `return_dict`          | `bool`              | æ˜¯å¦è¿”å› dict ç±»å‹è¾“å‡º                  |

#### âœ… è¿”å›å€¼ï¼š

* `transformers.modeling_outputs.CausalLMOutputWithPast`

---

### 6ï¸âƒ£ æŠ½è±¡æ–¹æ³• `embed_input_ids(self, input_ids: torch.LongTensor) -> torch.Tensor`

#### âœ… åŠŸèƒ½ï¼š

* å°†è¾“å…¥çš„ token ID è½¬æ¢ä¸º embedding è¡¨ç¤ºã€‚
* å¸¸ç”¨äºå¤–éƒ¨è¾“å…¥å·²ç»è¿‡ embedding æ¨¡å—çš„æ¨ç†æµç¨‹ã€‚

---

### 7ï¸âƒ£ æŠ½è±¡å±æ€§ `prompt_builder_fn`

#### âœ… åŠŸèƒ½ï¼š

* è¿”å›ä¸€ä¸ª `PromptBuilder` çš„ç±»ï¼ˆéå®ä¾‹ï¼‰ï¼Œç”¨äºæ„å»º promptï¼ˆå¦‚å¤šè½®é—®ç­”ã€task æè¿°ç­‰ï¼‰ã€‚
* å¸¸è§äº instruction-tuned æ¨¡å‹æˆ– chain-of-thought promptingã€‚

---

### 8ï¸âƒ£ æŠ½è±¡å±æ€§ `transformer_layer_cls`

#### âœ… åŠŸèƒ½ï¼š

* è¿”å› transformer ä¸­ä½¿ç”¨çš„å•å±‚æ¨¡å—ç±»ã€‚
* å¯¹åº”äºå¦‚ LLaMA ä¸­çš„ `LlamaDecoderLayer`ï¼Œç”¨äº FSDP åŒ…è£¹ç­–ç•¥ã€‚

---

### 9ï¸âƒ£ æŠ½è±¡å±æ€§ `half_precision_dtype`

#### âœ… åŠŸèƒ½ï¼š

* ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒçš„ç›®æ ‡ dtypeï¼ˆå¦‚ `torch.bfloat16` æˆ– `torch.float16`ï¼‰ã€‚

---

### ğŸ”Ÿ æŠ½è±¡å±æ€§ `last_layer_finetune_modules`

#### âœ… åŠŸèƒ½ï¼š

* è¿”å›æ¨¡å‹ä¸­éœ€è¦ finetune çš„æœ€åå‡ å±‚æ¨¡å—çš„é›†åˆã€‚
* é€‚ç”¨äºå†»ç»“å…¶ä»–éƒ¨åˆ†ï¼Œä»…è®­ç»ƒéƒ¨åˆ†æ¨¡å—çš„å¾®è°ƒæ–¹æ¡ˆã€‚

---

### ğŸ”¢ æ™®é€šå±æ€§ `embed_dim`

#### âœ… åŠŸèƒ½ï¼š

* è¿”å› LLM æ¨¡å‹çš„ hidden sizeï¼Œå³ embedding å‘é‡çš„ç»´åº¦ã€‚

#### âœ… å®ç°ï¼š

```python
return self.llm.config.hidden_size
```

---

### ğŸ”¢ æ™®é€šå±æ€§ `pad_token_id`

#### âœ… åŠŸèƒ½ï¼š

* è¿”å› tokenizer ä¸­ pad token çš„ IDã€‚

#### âœ… å®ç°ï¼š

```python
return self.tokenizer.pad_token_id
```

---

## âœ… æ€»ç»“ï¼š

è¿™ä¸ª `LLMBackbone` æ˜¯æ‰€æœ‰ LLM æ¨¡å—çš„ **â€œé€šç”¨æ¥å£å®šä¹‰ç±»â€**ï¼Œå°†å¦‚ä¸‹å†…å®¹æŠ½è±¡å‡ºæ¥ï¼š

* æ¨¡å‹çš„åˆå§‹åŒ–ã€æ¨ç†ï¼ˆforwardï¼‰ï¼›
* è¾“å…¥ embeddingï¼›
* Prompt æ„é€ ï¼›
* FSDP åˆ†å±‚ç­–ç•¥ï¼›
* ç²¾åº¦æ§åˆ¶å’Œè®­ç»ƒå†»ç»“æ¨¡å—ç®¡ç†ï¼›

å­ç±»åªéœ€è¦ç»§æ‰¿å®ƒå¹¶å¡«å…¥å…·ä½“å®ç°ï¼ˆå¦‚ `QwenBackbone`, `LLaMABackbone` ç­‰ï¼‰ï¼Œä¾¿èƒ½åœ¨ OpenVLA æ¡†æ¶ä¸­â€œæ— ç¼æ›¿æ¢â€ä¸åŒçš„ LLMã€‚

---

å¥½çš„ï¼Œæˆ‘ä»¬æ¥é€çº§ã€é€å‡½æ•°ã€é€è¡Œè®²è§£ `HFCausalLLMBackbone` è¿™ä¸ªç±»ã€‚å®ƒæ˜¯ OpenVLA ä¸­ä¸º HuggingFace-style Causal LLM æä¾›çš„é€šç”¨åŸºç±»ã€‚

---

# ğŸ” ç±»ï¼š`HFCausalLLMBackbone`

### ğŸ§  ç±»çš„åŠŸèƒ½æ¦‚è¿°

è¿™ä¸ªç±»ç»§æ‰¿è‡ª `LLMBackbone`ï¼Œæ˜¯å…¶ä¸€ä¸ªå…·ä½“å®ç°ï¼Œé€‚ç”¨äº HuggingFace å¹³å°ä¸Šçš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal Language Modelsï¼‰ï¼Œä¾‹å¦‚ï¼š

* LLaMA
* Phi-2
* Mistral
* Qwen ç­‰

å®ƒåšçš„äº‹æƒ…åŒ…æ‹¬ï¼š

* åŠ è½½ LLMï¼ˆä» HuggingFace Hubï¼‰
* åŠ è½½ tokenizer
* å¯ç”¨/ç¦ç”¨æ¨ç†æ¨¡å¼å’Œ gradient checkpointing
* å®ç°åµŒå…¥è·å–å’Œå‰å‘ä¼ æ’­
* å®ç° transformer FSDP åŒ…è£¹ç­–ç•¥

---

## 1ï¸âƒ£ `__init__` æ„é€ å‡½æ•°è¯¦è§£

```python
def __init__(
    self,
    llm_backbone_id: str,
    llm_family: str,
    llm_cls: Type[PreTrainedModel],
    hf_hub_path: str,
    llm_max_length: int = 2048,
    hf_token: Optional[str] = None,
    inference_mode: bool = False,
    use_flash_attention_2: bool = False,
)
```

### âœ… å‚æ•°è¯¦è§£ï¼š

| å‚æ•°                      | ä½œç”¨                                     |
| ----------------------- | -------------------------------------- |
| `llm_backbone_id`       | æ¨¡å‹å”¯ä¸€æ ‡è¯†ç¬¦                                |
| `llm_family`            | æ¨¡å‹ç±»åˆ«å­—ç¬¦ä¸²ï¼ˆå¦‚ `"Qwen"`ã€`"Phi-2"`ï¼‰          |
| `llm_cls`               | å®é™… HuggingFace æ¨¡å‹ç±»ï¼Œå¦‚ `QwenForCausalLM` |
| `hf_hub_path`           | æ¨¡å‹åœ¨ HuggingFace ä¸Šçš„åœ°å€                   |
| `llm_max_length`        | æœ€å¤§æ”¯æŒçš„è¾“å…¥é•¿åº¦ï¼Œé»˜è®¤ 2048                      |
| `hf_token`              | è®¿é—® HuggingFace ç§æœ‰æ¨¡å‹æ‰€éœ€çš„ Token           |
| `inference_mode`        | æ˜¯å¦åªè¿›è¡Œæ¨ç†ï¼ˆå³ä¸åŠ è½½æƒé‡ç”¨äºè®­ç»ƒï¼‰                    |
| `use_flash_attention_2` | æ˜¯å¦å¯ç”¨ Flash Attention 2                 |

---

### âœ… æ„é€ è¿‡ç¨‹é€è¡Œè§£é‡Šï¼š

#### ğŸ”¸ åŸºç¡€ä¿¡æ¯å­˜å‚¨

```python
super().__init__(llm_backbone_id)
self.llm_family = llm_family
self.llm_max_length = llm_max_length
self.inference_mode = inference_mode
```

åˆå§‹åŒ–åŸºç¡€ç±»ï¼Œè®°å½• LLM å®¶æ—ä¿¡æ¯ã€æœ€å¤§é•¿åº¦ã€æ˜¯å¦ä¸ºæ¨ç†æ¨¡å¼ã€‚

---

#### ğŸ”¸ æ¨¡å‹åŠ è½½é€»è¾‘ï¼ˆåˆ†ä¸ºè®­ç»ƒ/æ¨ç†ä¸¤ç§æ¨¡å¼ï¼‰

**è®­ç»ƒæ¨¡å¼åŠ è½½é¢„è®­ç»ƒæ¨¡å‹**

```python
if not self.inference_mode:
    self.llm = llm_cls.from_pretrained(...)
```

* ä» HF Hub ä¸‹è½½ LLM å¹¶å®ä¾‹åŒ–ï¼ˆæ˜¾å¼ä¸ä½¿ç”¨ `AutoModel`ï¼Œæ–¹ä¾¿æ§åˆ¶æ¨¡å‹å†…éƒ¨ç»†èŠ‚ï¼‰ã€‚
* è®¾ç½®ä¸º greedy decoding (`do_sample=False, temperature=1.0, top_p=1.0`)ã€‚
* å¯é€‰å¯ç”¨ FlashAttentionã€‚

**æ¨ç†æ¨¡å¼åªåŠ è½½ config**

```python
else:
    llm_config = AutoConfig.from_pretrained(...)
    self.llm = llm_cls._from_config(llm_config)
```

* ä»…æ„é€ æ¨¡å‹ç»“æ„ï¼Œä¸åŠ è½½æƒé‡ã€‚

---

#### ğŸ”¸ use\_cache é…ç½®

```python
self.llm.config.use_cache = False if not self.inference_mode else True
```

* åœ¨è®­ç»ƒæ—¶å…³é—­ `use_cache`ï¼Œå› ä¸ºå¼€å¯å®ƒä¼šç ´åæ¢¯åº¦æ£€æŸ¥ç‚¹åŠŸèƒ½ã€‚

---

#### ğŸ”¸ ä¿®å¤ requires\_grad=False å¯¼è‡´çš„åå‘ä¼ æ’­å¤±è´¥é—®é¢˜

```python
if not self.inference_mode:
    self.llm.enable_input_require_grads()
```

* å¦‚æœæ‰€æœ‰å‚æ•°éƒ½è¢«å†»ç»“ï¼Œä¼šå¯¼è‡´åå‘ä¼ æ’­å¤±è´¥ï¼›
* è¯¥å‡½æ•°é€šè¿‡æ³¨å†Œ hook ä¿®å¤è¿™ä¸€é—®é¢˜ï¼›
* å¯¹ LoRA/Full Fine-tune éƒ½å®‰å…¨ã€‚

---

#### ğŸ”¸ åŠ è½½ tokenizerï¼ˆä½¿ç”¨ Fast tokenizerï¼‰

```python
self.tokenizer = AutoTokenizer.from_pretrained(...)
```

* åŠ è½½ tokenizerï¼Œå¹¶è®¾ç½®ï¼š

  * æœ€å¤§è¾“å…¥é•¿åº¦ï¼›
  * padding æ–¹å‘ä¸º `"right"`ï¼›
  * tokenï¼ˆå¦‚æœæ˜¯ç§æœ‰æ¨¡å‹ï¼‰ã€‚

---

#### ğŸ”¸ Tokenizer è¡Œä¸ºéªŒè¯

```python
SPECIAL_CASES = { "phi-2-3b", }
if self.identifier in SPECIAL_CASES:
    return
```

* æœ‰äº›æ¨¡å‹ï¼ˆå¦‚ phi-2ï¼‰æ²¡æœ‰é»˜è®¤ BOS tokenï¼Œéœ€è¦äººå·¥æ’å…¥ï¼›
* å¯¹äºè¿™äº›ç‰¹æ®Šæƒ…å†µè·³è¿‡é»˜è®¤æ–­è¨€ã€‚

```python
assert (...), "..."
```

* éªŒè¯ tokenizer åœ¨ `add_special_tokens=True` æ—¶ï¼Œä¼šè‡ªåŠ¨æ·»åŠ  BOSï¼›
* è‹¥ä¸ç¬¦åˆè¿™ä¸ªå‡è®¾ï¼Œè®­ç»ƒ/æ¨ç†æ—¶å›¾æ–‡å¯¹é½ä¼šå‡ºé—®é¢˜ã€‚

---

## 2ï¸âƒ£ `get_fsdp_wrapping_policy`

```python
def get_fsdp_wrapping_policy(self) -> Callable:
```

### âœ… åŠŸèƒ½ï¼š

* è¿”å›ä¸€ä¸ª `transformer_auto_wrap_policy` å‡½æ•°ï¼Œç”¨äºå‘Šè¯‰ FSDP åº”è¯¥åŒ…è£¹å“ªäº›æ¨¡å—ã€‚

### âœ… å†…éƒ¨ï¼š

```python
transformer_block_policy = partial(
    transformer_auto_wrap_policy,
    transformer_layer_cls={self.transformer_layer_cls}
)
```

* ä½¿ç”¨å½“å‰æ¨¡å‹ä¸­çš„ transformer layer classï¼ˆåœ¨å­ç±»ä¸­å®šä¹‰ï¼‰ï¼›
* Partial ä½¿å¾— `transformer_auto_wrap_policy` æˆä¸ºåªä¸“æ³¨åŒ… transformer å±‚çš„åŒ…è£…å™¨ã€‚

---

## 3ï¸âƒ£ `enable_gradient_checkpointing`

```python
def enable_gradient_checkpointing(self) -> None:
    self.llm.gradient_checkpointing_enable()
```

### âœ… åŠŸèƒ½ï¼š

* å¯ç”¨ HF çš„ gradient checkpointingï¼ŒèŠ‚çœæ˜¾å­˜ï¼›
* å¤šç”¨äºå…¨å‚æ•°å¾®è°ƒï¼ˆFull fine-tuneï¼‰æˆ–ä½èµ„æºè®­ç»ƒã€‚

---

## 4ï¸âƒ£ `embed_input_ids`

```python
def embed_input_ids(self, input_ids: torch.LongTensor) -> torch.Tensor:
    return self.llm.get_input_embeddings()(input_ids)
```

### âœ… åŠŸèƒ½ï¼š

* è·å– `input_ids` å¯¹åº”çš„ token embeddingï¼›
* ç›¸å½“äºè°ƒç”¨æ¨¡å‹æœ€å‰é¢çš„åµŒå…¥å±‚ã€‚

---

## 5ï¸âƒ£ `forward`

```python
def forward(...) -> CausalLMOutputWithPast:
```

### âœ… åŠŸèƒ½ï¼š

* ä»£ç† HuggingFace æ¨¡å‹çš„ `forward()` å‡½æ•°ï¼›
* ä¿ç•™æ‰€æœ‰å‚æ•°æ¥å£ä¸€è‡´æ€§ï¼Œæ–¹ä¾¿è®­ç»ƒã€æ¨ç†ã€ç”Ÿæˆç­‰æµç¨‹å¯¹æ¥ã€‚

### âœ… å†…éƒ¨ï¼š

```python
output: CausalLMOutputWithPast = self.llm(...)
return output
```

---

## âœ… æ€»ç»“

| ç±»åŠŸèƒ½                                   | å®ç°                            |
| ------------------------------------- | ----------------------------- |
| åŠ è½½ HF æ¨¡å‹å’Œ tokenizer                   | æ˜ç¡®æ”¯æŒè®­ç»ƒå’Œæ¨ç†ä¸¤ç§æ¨¡å¼                 |
| æ”¯æŒ Flash Attentionã€Grad Checkpointing | å¯é€‰å¯ç”¨                          |
| åµŒå…¥å±‚è·å–å’Œ forward æ¨ç†                     | ä¿æŒ HuggingFace æ¥å£ä¸€è‡´æ€§          |
| Tokenizer è¡Œä¸ºæ–­è¨€                        | ä¿è¯ä¸å›¾æ–‡ patch æ’å…¥é€»è¾‘å¯¹é½            |
| æä¾› FSDP transformer å±‚åŒ…è£…ç­–ç•¥             | åŸºäº `transformer_layer_cls` å®ç° |

---
