# 核心问题

核心问题：**如何高效地处理长文本的上下文信息？** 特别是 LLM 和 VLM 中，如何通过视觉模态实现对文本的有效压缩。

注意力机制计算的时间复杂度是 $n^2 * d$ ，所以当输入文本很多，或者上下文很长的时候

# 1 引言

这部分是论文 **DeepSeek-OCR** 的 **引言（Introduction）**，主要阐述了研究背景、核心动机、主要方法与贡献。可以分为四个要点来理解👇：

## 1.1 研究背景：LLM 的长上下文问题

LLM 在处理长文本时，因为 Transformer 的注意力机制随序列长度呈**平方级增长**，导致内存和计算成本极高，引出关键问题“如何压缩长文本上下文？”，以此来让模型：
1. 减少计算量；
2. 记住更多上下文。

## 1.2 核心思想：用视觉模态进行“光学压缩（Optical Compression）”

论文新思路：

> “一张图片能表示一整页文档的文字，用图像形式表达文本，是一种天然的压缩。”

* 一个文档图像（例如PDF截图）用少量视觉tokens就能表达大量文本信息。
* 相比原始文本tokens，这种“光学压缩”可以带来**7～20倍的token压缩率**。
* 因此，可以用视觉模态作为LLM的**高效信息存储与压缩媒介**。

OCR 任务正好是一个天然的“压缩—解压缩”过程，所以作者使用该方法。

## 1.3 方法与贡献

作者提出了 **DeepSeek-OCR** 作为验证这一思想的模型，主要有三个贡献：

1. **量化验证压缩能力**

   * 在 Fox 基准上，9–10× 压缩下OCR精度仍超过96%；
   * 10–12×压缩精度约90%；
   * 即使20×压缩，也能保持约60%准确率。
     → 证明通过视觉模态进行文本压缩是**可行且高效**的。

2. **提出新型视觉编码器 DeepEncoder**

   * 架构结合**局部窗口注意力（window attention）**和**全局注意力（global attention）**；
   * 通过 **16× 卷积下采样模块** 减少视觉token数量；
   * 能处理高分辨率输入，同时保持**低激活内存和少量视觉tokens**。

3. **构建完整系统 DeepSeek-OCR（基于 DeepEncoder + MoE解码器）**

   * 使用 DeepSeek-3B-MoE 作为解码器；
   * 在 **OmniDocBench** 上性能超过其他端到端OCR模型；
   * 同时支持解析图表、化学式、几何图形等复杂视觉结构；
   * 在生产环境中可实现**每日生成3300万页训练数据**的能力。

这一部分（**2. Related Works**）主要回顾了与 **DeepSeek-OCR** 相关的两类研究方向：
👉 一类是 **多模态视觉编码器（Vision Encoders in VLMs）**，
👉 另一类是 **端到端OCR模型（End-to-end OCR Models）**。

作者通过分析这些已有方法的优缺点，指出了当前存在的核心问题，并引出了自己提出 **DeepEncoder** 的动机。
下面是详细总结👇：

# 2 相关工作

## 2.1 Typical Vision Encoders in VLMs

当前开源 VLM 中常见的三种视觉编码器架构：

### （1）双塔结构（Dual-Tower Architecture）—— 代表：**Vary**

* **做法**：使用类似 **SAM** 的并行视觉编码器结构，以提升高分辨率图像的表示能力。
* **优点**：参数量可控，激活内存较低。
* **缺点**：

  * 需要两次图像预处理（dual image preprocessing）；
  * 训练时难以实现高效的并行；
  * 部署复杂。

**问题本质**：结构复杂、难以扩展，影响模型训练和推理效率。



### （2）分块式结构（Tile-based Encoding）—— 代表：**InternVL2.0**

* **做法**：将高分辨率图像切成小块并并行处理，以降低显存占用。
* **优点**：能处理极高分辨率图像。
* **缺点**：

  * 原生分辨率较低（通常小于512×512）；
  * 图像被切得过碎，导致产生大量视觉tokens；
  * 影响整体语义连续性。

**问题本质**：虽然节省显存，但牺牲了语义完整性与token效率。



### （3）自适应分辨率编码（Adaptive Resolution Encoding）—— 代表：**Qwen2-VL**

* **做法**：采用 **NaViT** 范式，直接在整张图上进行patch分割，无需分块。
* **优点**：可灵活适配不同分辨率输入。
* **缺点**：

  * 对高分辨率图像激活内存消耗极大（易GPU溢出）；
  * 训练时需要极长序列（大量vision tokens）；
  * 推理时prefill和generation阶段都很慢。

**问题本质**：虽然灵活但计算开销过高，不适合实际大规模部署。

> 现有视觉编码器要么计算成本高、要么生成的视觉tokens太多、要么结构复杂难部署。
> 因此作者提出了 **DeepEncoder** —— 一种既能处理高分辨率输入，又能在少量视觉tokens下高效工作的新型编码器。



## 2.2 End-to-End OCR Models

近几年OCR领域的发展趋势：

### （1）传统OCR → 端到端OCR

* 过去的OCR需要**检测模型**（文字区域）+**识别模型**（文字内容）两个阶段。
* 随着VLM的发展，OCR逐渐转向 **端到端（end-to-end）** 形式，直接从图像输出文本，系统结构更简单、性能更强。



### （2）典型代表

* **Nougat**：首个在学术论文OCR中使用端到端架构的模型，展示了处理密集文本（如论文PDF）的潜力。
* **GOT-OCR2.0**：扩展OCR2.0概念，加入更多合成图像解析任务，在性能与效率之间做了平衡。
* **Qwen-VL、InternVL** 等通用视觉模型也不断提升OCR能力，推动了密集视觉理解的发展。



### （3）仍未解决的问题

> 当前模型虽然在精度上表现优秀，但有一个**关键问题**：
> “对于一份含1000个词的文档，至少需要多少个视觉tokens才能完整解码？”

这个问题直接关系到视觉压缩的本质，也呼应论文的主题：

> “一张图到底能抵得上多少字？”（A picture is worth a thousand words.）



## ✅ 总结一句话：

> 相关工作部分的核心逻辑是：
>
> * 现有VLM的视觉编码器不是太慢、太碎、太占显存，就是生成太多视觉tokens；
> * 现有OCR模型虽然能端到端识别文字，但没人研究**视觉tokens与文本tokens之间的压缩关系**；
>
> **DeepSeek-OCR** 的提出正是为了解决这两个问题：
> 1. 设计一种高效、可压缩的视觉编码器（DeepEncoder）；
> 2. 定量研究“视觉压缩”能在多大程度上替代文本表示。
