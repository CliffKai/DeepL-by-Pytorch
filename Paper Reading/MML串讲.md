传统多模态学习论文串讲：
- 图文检索（Image Text Retrieval）
- VQA（视觉问答）
- Visual Reasoning（视觉推理）
- Visual Entailment（视觉蕴含）

# 第一部分：只用Transformer Encoder的一些方法

![Figure1](../images/MML论文串讲_Figure1.png)

## 1.1 ALBEF



# 第二部分：Transformer Encoder和Decoder一起用的一些方法

CLIP模型也是非常简单的一个结构，它是一个典型的双塔模型，他有两个Model，一个对应文本，一个对应视觉，在训练的时候通过对比学习，让已有的图像文本对在空间上拉的更近，然而让不是一个对的图片文本就拉的尽量更远，从而最后学到了非常好的图像文本特征，然后一旦学到很好的图像文本特征之后CLIP只需要做这种很简单的点乘，就能去做多模态任务，尤其是对那种图像文本匹配，或者图像文本检索的这种任务来说，CLIP简直就是神一样的存在，因为它不光效果好，而且很高效，因为往往你去做这种图像文本匹配或者图像文本检索任务的时候，你是有一个很大的已有的数据库的，这个时候如果你新来一张图片，或者新来一个文本，你要去和已有的数据库去做匹配，那其他所有的方法都会非常的慢，因为其他所有的数据都要过一遍编码器，但是CLIP模型就不用，他可以提前把那个数据库里所有的图像文本的特征提前都抽好，而且是想什么时候抽就什么时候抽，抽好放在那就行，等你真正想用的时候直接就做一个点乘就好了。