{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bd09d30ca47d67",
   "metadata": {},
   "source": [
    "该内容是对simple_vit中内容的讲解，主要分为：\n",
    "- 代码\n",
    "    - attention\n",
    "    - transformer\n",
    "    - embedding\n",
    "    - simple_vit\n",
    "- 训练\n",
    "    - train\n",
    "- 测试\n",
    "    - test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f371d8fc601812",
   "metadata": {},
   "source": [
    "# 一、代码\n",
    "\n",
    "这一部分包含：\n",
    "- attention\n",
    "- transformer\n",
    "- embedding\n",
    "- simple_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T08:55:38.548335Z",
     "start_time": "2025-05-05T08:55:35.525985Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cfb9a7-f897-44c8-9ea9-c4c14516cb70",
   "metadata": {},
   "source": [
    "## 1 attention\n",
    "\n",
    "dim：输入特征的维度（embedding 的维度）。            \n",
    "heads：注意力头的数量。            \n",
    "dim_head：每个头的维度。             \n",
    "\n",
    "这里self.to_qkv()等价于把输入 X向右扩展为 [X, X, X]，同时左乘拼接起来的矩阵 [[Wq], [Wk], [Wv]]，从而一次性得到 Q、K、V。\n",
    "然后后面非常关键的一步：\n",
    "```python\n",
    "map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "```\n",
    "将三维的h个QKV首先分别拆分为Q、K和V，然后再将三维的Q、K和V分别拆为三维的Q_i、K_i和V_i，然后以四维的形式存储在三个变量q, k, v（相当于q = [Q_0, Q_1, ... , Q_h]，kv同理），所以 torch.matmul 这一步才能分别计算Q_i × K_i^T。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d105ae5-d9f4-4f8f-9ee5-b6054912ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads               # Q、K、V 的拼接维度。\n",
    "        self.heads = heads                         \n",
    "        self.scale = dim_head ** -0.5              # 用于缩放 QK 的点积，防止 softmax 后梯度过小或过大\n",
    "        self.norm = nn.LayerNorm(dim)              # 层归一化，通常用于稳定训练。\n",
    "        self.attend = nn.Softmax(dim = -1)         # 对最后一个维度（注意力得分）做 softmax，形成注意力权重（也就是对Score做softmax，得到Attention）\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)     # 这是非常关键的一步处理\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3879c6b-b591-4c94-bc5d-a7be112a87a8",
   "metadata": {},
   "source": [
    "## 2 transformer\n",
    "\n",
    "这一部分是利用前面的注意力计算方法搭配一个前馈神经网络 FeedForward Neural Network 来进行 transformer encoder 块的构建。\n",
    "\n",
    "### 2.1 FeedForward\n",
    "              \n",
    "主要目的是为了引入非线性，提高模型的表达能力。    \n",
    "\n",
    "### 2.2 ModuleList    \n",
    "\n",
    "torch.nn.ModuleList 是一个容器，专门用于存放多个子模块，和 nn.Sequential 不同，Sequential 是一个可执行模型，自动按顺序执行子模块，而 nn.ModuleList 是一个模块列表容器，不执行，只注册，执行逻辑自己写，就比如本模块中的前向传播部分：\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    for attn, ff in self.layers:\n",
    "        x = attn(x) + x\n",
    "        x = ff(x) + x\n",
    "    return self.norm(x)\n",
    "```\n",
    "\n",
    "Sequential 只适用于线性、固定的前向流程，但是本模型中要使用残差连接，所以要使用 ModuleList。\n",
    "\n",
    "### 2.3 Transformer\n",
    "\n",
    "dim：输入 token 的维度（即 embedding 维度）            \n",
    "depth：Block 的层数，也就是 Transformer 的堆叠深度             \n",
    "heads：注意力头的数量              \n",
    "dim_head：每个注意力头的维度             \n",
    "mlp_dim：FeedForward 中间层的维度（即升维后的维度）                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eea08c2-a627-4901-8cad-860d57e6875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                Attention(dim, heads=heads, dim_head=dim_head),\n",
    "                FeedForward(dim, mlp_dim)\n",
    "            ])\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1461c14-9e22-46af-9a06-d3ec0371be08",
   "metadata": {},
   "source": [
    "## 3 embedding\n",
    "\n",
    "这段负责图像切分、Patch 嵌入和位置编码，也就是将图像输入转为 token 表示的模块。\n",
    "\n",
    "### 3.1 pair\n",
    "\n",
    "如果输入是整数，就把它变成形如 (t, t) 的二元组，e.g：\n",
    "pair(32) → (32, 32)\n",
    "pair((16, 8)) → (16, 8)\n",
    "\n",
    "### 3.2 posemb_sincos_2d\n",
    "\n",
    "h、w：图像划分 patch 后的高和宽（即 patch 网格大小）                \n",
    "dim：每个位置编码的维度，必须是 4 的倍数               \n",
    "temperature：控制正余弦波的频率范围，默认为 10000（和原始 Transformer 中一致）             \n",
    "dtype：输出的张量类型（float32）              \n",
    "\n",
    "这里不详细介绍这个编码模式了，只需要简单了解一下即可。\n",
    "\n",
    "在ViT原论文中，位置编码是一维的、可学习的（learnable 1D positional embedding），但是 vit-pytorch 中的 simple_vit.py 实现中，作者 lucidrains 使用了不可学习的二维正余弦位置编码（sinusoidal 2D positional encoding），原因如下：\n",
    "- 不需要训练，因此泛化性好\n",
    "- 对小模型更稳定\n",
    "- 简洁性与泛化性优先\n",
    "- 避免可学习位置编码带来的 patch 数不匹配问题\n",
    "\n",
    "### 3.3 PatchEmbedding\n",
    "\n",
    "image_size：输入图像尺寸（可以是 int 或 tuple）          \n",
    "patch_size：每个 patch 的大小            \n",
    "dim：输出 token 的维度（Transformer 的输入维度）            \n",
    "channels：图像的通道数，默认为 3（RGB）              \n",
    "\n",
    "Transformer 只能处理固定维度的 token，所以先\n",
    "\n",
    "```python\n",
    "self.rearrange = Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width)\n",
    "```\n",
    "\n",
    "将h×w×c的三维图片转换成(h×w/(p^2))×(p^2×c)的二维矩阵。\n",
    "\n",
    "```python\n",
    "self.net = nn.Sequential(\n",
    "    self.rearrange,\n",
    "    nn.LayerNorm(patch_dim),\n",
    "    nn.Linear(patch_dim, dim),\n",
    "    nn.LayerNorm(dim)\n",
    ")\n",
    "```\n",
    "\n",
    "用Linear将patch_dim转化为transformer可处理的dim，防止因图片大小变化而引起的变化导致无法处理。       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e5063c-9b36-4376-bc78-d6b0bb7e2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "def posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype=torch.float32):\n",
    "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
    "    assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
    "    omega = torch.arange(dim // 4) / (dim // 4 - 1)\n",
    "    omega = 1.0 / (temperature ** omega)\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :]\n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
    "    return pe.type(dtype)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, dim, channels=3):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.rearrange = Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width)\n",
    "        self.net = nn.Sequential(\n",
    "            self.rearrange,\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3f8e4-c16b-435a-a267-dd8cb201bfdc",
   "metadata": {},
   "source": [
    "## 4 simple\n",
    "\n",
    "定义完整Simple-ViT模型，该模型中没有添加原论文中的CLS，而是在最后添加了一层平均池化，目的还是为了简洁稳定、收敛快，但是同样的，模型的能力相对来讲会弱很多。\n",
    "\n",
    "image_size：输入图像尺寸（可为 int 或 tuple）\n",
    "patch_size：patch 的高宽\n",
    "num_classes：分类类别数（用于输出层）\n",
    "dim：token 向量的维度，也是 Transformer 的输入维度\n",
    "depth：Transformer block 的层数\n",
    "heads：多头注意力的头数\n",
    "dim_head：每个注意力头的维度\n",
    "mlp_dim\tTransformer：中前馈层的中间维度\n",
    "channels：图像通道数，默认 3（RGB）\n",
    "\n",
    "```python\n",
    "image_height, image_width = pair(image_size)\n",
    "patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "assert image_height % patch_height == 0 and image_width % patch_width == 0\n",
    "```\n",
    "\n",
    "确保图像尺寸可以整除 patch 尺寸，便于划分为整齐的 patch 网格\n",
    "\n",
    "```python\n",
    "self.pool = \"mean\"\n",
    "```\n",
    "\n",
    "用来取代CLS的平均池化\n",
    "\n",
    "```python\n",
    "x += self.pos_embedding.to(device, dtype=x.dtype)\n",
    "```\n",
    "\n",
    "中to device是为了确保位置编码和输入在同一个设备上，否则会报错\n",
    "\n",
    "```python\n",
    "self.to_latent = nn.Identity()\n",
    "\n",
    "x = self.to_latent(x)\n",
    "```\n",
    "\n",
    "这一部分什么也没做，是为了将来接口占位置（如果想用MLP之类的进行分类，只需要将其修改即可，不用重新写forward）\n",
    "\n",
    "```python\n",
    "nn.Linear(dim, num_classes)\n",
    "```\n",
    "\n",
    "是一个全连接层，用于最后的分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a4885a9-4474-4143-b16e-3acab8c8cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dim_head=64):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "\n",
    "        self.to_patch_embedding = PatchEmbedding(image_size, patch_size, dim, channels)\n",
    "\n",
    "        self.pos_embedding = posemb_sincos_2d(\n",
    "            h = image_height // patch_height,\n",
    "            w = image_width // patch_width,\n",
    "            dim = dim,\n",
    "        )\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
    "\n",
    "        self.pool = \"mean\"\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.linear_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        device = img.device\n",
    "        x = self.to_patch_embedding(img)\n",
    "        x += self.pos_embedding.to(device, dtype=x.dtype)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.to_latent(x)\n",
    "        return self.linear_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38602b-3276-4e7a-833b-59232d8ee0e9",
   "metadata": {},
   "source": [
    "# 训练\n",
    "\n",
    "我们使用[Tiny ImageNet](http://cs231n.stanford.edu/tiny-imagenet-200.zip)来进行训练和验证，该数据集是 ImageNet 数据集的一个精简版本，共有200个类，每个类有 500 张train、50 张val，另外还有10000张无标签的test，图像尺寸为64*64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01930455-93bb-4d2c-a55f-55de04a5054e",
   "metadata": {},
   "source": [
    "## 准备数据集\n",
    "\n",
    "Tiny ImageNet 中的 val 目录结构不是标准的 ImageFolder 格式，需要处理一下\n",
    "\n",
    "其他部分正常准备即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09087b6f-ff1c-4075-a589-3fe1ea88169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 val\n",
    "def organize_val_folder(val_dir):\n",
    "    val_img_dir = os.path.join(val_dir, 'images')\n",
    "    with open(os.path.join(val_dir, 'val_annotations.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split('\\t')\n",
    "        img, label = tokens[0], tokens[1]\n",
    "        label_dir = os.path.join(val_dir, label)\n",
    "        os.makedirs(label_dir, exist_ok=True)\n",
    "        shutil.move(os.path.join(val_img_dir, img), os.path.join(label_dir, img))\n",
    "\n",
    "    shutil.rmtree(val_img_dir)\n",
    "\n",
    "organize_val_folder('../../datasets/tiny-imagenet-200/val')\n",
    "\n",
    "\n",
    "# 准备数据集\n",
    "data_dir = '../../datasets/tiny-imagenet-200'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=f\"{data_dir}/train\", transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=f\"{data_dir}/val\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede26af-af8c-4d12-814d-4a6b4352c065",
   "metadata": {},
   "source": [
    "## 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ddf5a0c-eee2-49db-8f2a-cbcee89d2c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleViT(\n",
    "    image_size=224,\n",
    "    patch_size=16,\n",
    "    num_classes=200,\n",
    "    dim=512,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    mlp_dim=1024\n",
    ").cuda()  # 没有GPU的可以不.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512692d-8474-45c3-8f9c-4848903cd951",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab45ebb-da8a-44a3-a804-97de789012e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "batch_idx:0\n",
      "batch_idx:1\n",
      "batch_idx:2\n",
      "batch_idx:3\n",
      "batch_idx:4\n",
      "batch_idx:5\n",
      "batch_idx:6\n",
      "batch_idx:7\n",
      "batch_idx:8\n",
      "batch_idx:9\n",
      "batch_idx:10\n",
      "batch_idx:11\n",
      "batch_idx:12\n",
      "batch_idx:13\n",
      "batch_idx:14\n",
      "batch_idx:15\n",
      "batch_idx:16\n",
      "batch_idx:17\n",
      "batch_idx:18\n",
      "batch_idx:19\n",
      "batch_idx:20\n",
      "batch_idx:21\n",
      "batch_idx:22\n",
      "batch_idx:23\n",
      "batch_idx:24\n",
      "batch_idx:25\n",
      "batch_idx:26\n",
      "batch_idx:27\n",
      "batch_idx:28\n",
      "batch_idx:29\n",
      "batch_idx:30\n",
      "batch_idx:31\n",
      "batch_idx:32\n",
      "batch_idx:33\n",
      "batch_idx:34\n",
      "batch_idx:35\n",
      "batch_idx:36\n",
      "batch_idx:37\n",
      "batch_idx:38\n",
      "batch_idx:39\n",
      "batch_idx:40\n",
      "batch_idx:41\n",
      "batch_idx:42\n",
      "batch_idx:43\n",
      "batch_idx:44\n",
      "batch_idx:45\n",
      "batch_idx:46\n",
      "batch_idx:47\n",
      "batch_idx:48\n",
      "batch_idx:49\n",
      "batch_idx:50\n",
      "batch_idx:51\n",
      "batch_idx:52\n",
      "batch_idx:53\n",
      "batch_idx:54\n",
      "batch_idx:55\n",
      "batch_idx:56\n",
      "batch_idx:57\n",
      "batch_idx:58\n",
      "batch_idx:59\n",
      "batch_idx:60\n",
      "batch_idx:61\n",
      "batch_idx:62\n",
      "batch_idx:63\n",
      "batch_idx:64\n",
      "batch_idx:65\n",
      "batch_idx:66\n",
      "batch_idx:67\n",
      "batch_idx:68\n",
      "batch_idx:69\n",
      "batch_idx:70\n",
      "batch_idx:71\n",
      "batch_idx:72\n",
      "batch_idx:73\n",
      "batch_idx:74\n",
      "batch_idx:75\n",
      "batch_idx:76\n",
      "batch_idx:77\n",
      "batch_idx:78\n",
      "batch_idx:79\n",
      "batch_idx:80\n",
      "batch_idx:81\n",
      "batch_idx:82\n",
      "batch_idx:83\n",
      "batch_idx:84\n",
      "batch_idx:85\n",
      "batch_idx:86\n",
      "batch_idx:87\n",
      "batch_idx:88\n",
      "batch_idx:89\n",
      "batch_idx:90\n",
      "batch_idx:91\n",
      "batch_idx:92\n",
      "batch_idx:93\n",
      "batch_idx:94\n",
      "batch_idx:95\n",
      "batch_idx:96\n",
      "batch_idx:97\n",
      "batch_idx:98\n",
      "batch_idx:99\n",
      "batch_idx:100\n",
      "batch_idx:101\n",
      "batch_idx:102\n",
      "batch_idx:103\n",
      "batch_idx:104\n",
      "batch_idx:105\n",
      "batch_idx:106\n",
      "batch_idx:107\n",
      "batch_idx:108\n",
      "batch_idx:109\n",
      "batch_idx:110\n",
      "batch_idx:111\n",
      "batch_idx:112\n",
      "batch_idx:113\n",
      "batch_idx:114\n",
      "batch_idx:115\n",
      "batch_idx:116\n",
      "batch_idx:117\n",
      "batch_idx:118\n",
      "batch_idx:119\n",
      "batch_idx:120\n",
      "batch_idx:121\n",
      "batch_idx:122\n",
      "batch_idx:123\n",
      "batch_idx:124\n",
      "batch_idx:125\n",
      "batch_idx:126\n",
      "batch_idx:127\n",
      "batch_idx:128\n",
      "batch_idx:129\n",
      "batch_idx:130\n",
      "batch_idx:131\n",
      "batch_idx:132\n",
      "batch_idx:133\n",
      "batch_idx:134\n",
      "batch_idx:135\n",
      "batch_idx:136\n",
      "batch_idx:137\n",
      "batch_idx:138\n",
      "batch_idx:139\n",
      "batch_idx:140\n",
      "batch_idx:141\n",
      "batch_idx:142\n",
      "batch_idx:143\n",
      "batch_idx:144\n",
      "batch_idx:145\n",
      "batch_idx:146\n",
      "batch_idx:147\n",
      "batch_idx:148\n",
      "batch_idx:149\n",
      "batch_idx:150\n",
      "batch_idx:151\n",
      "batch_idx:152\n",
      "batch_idx:153\n",
      "batch_idx:154\n",
      "batch_idx:155\n",
      "batch_idx:156\n",
      "batch_idx:157\n",
      "batch_idx:158\n",
      "batch_idx:159\n",
      "batch_idx:160\n",
      "batch_idx:161\n",
      "batch_idx:162\n",
      "batch_idx:163\n",
      "batch_idx:164\n",
      "batch_idx:165\n",
      "batch_idx:166\n",
      "batch_idx:167\n",
      "batch_idx:168\n",
      "batch_idx:169\n",
      "batch_idx:170\n",
      "batch_idx:171\n",
      "batch_idx:172\n",
      "batch_idx:173\n",
      "batch_idx:174\n",
      "batch_idx:175\n",
      "batch_idx:176\n",
      "batch_idx:177\n",
      "batch_idx:178\n",
      "batch_idx:179\n",
      "batch_idx:180\n",
      "batch_idx:181\n",
      "batch_idx:182\n",
      "batch_idx:183\n",
      "batch_idx:184\n",
      "batch_idx:185\n",
      "batch_idx:186\n",
      "batch_idx:187\n",
      "batch_idx:188\n",
      "batch_idx:189\n",
      "batch_idx:190\n",
      "batch_idx:191\n",
      "batch_idx:192\n",
      "batch_idx:193\n",
      "batch_idx:194\n",
      "batch_idx:195\n",
      "batch_idx:196\n",
      "batch_idx:197\n",
      "batch_idx:198\n",
      "batch_idx:199\n",
      "batch_idx:200\n",
      "batch_idx:201\n",
      "batch_idx:202\n",
      "batch_idx:203\n",
      "batch_idx:204\n",
      "batch_idx:205\n",
      "batch_idx:206\n",
      "batch_idx:207\n",
      "batch_idx:208\n",
      "batch_idx:209\n",
      "batch_idx:210\n",
      "batch_idx:211\n",
      "batch_idx:212\n",
      "batch_idx:213\n",
      "batch_idx:214\n",
      "batch_idx:215\n",
      "batch_idx:216\n",
      "batch_idx:217\n",
      "batch_idx:218\n",
      "batch_idx:219\n",
      "batch_idx:220\n",
      "batch_idx:221\n",
      "batch_idx:222\n",
      "batch_idx:223\n",
      "batch_idx:224\n",
      "batch_idx:225\n",
      "batch_idx:226\n",
      "batch_idx:227\n",
      "batch_idx:228\n",
      "batch_idx:229\n",
      "batch_idx:230\n",
      "batch_idx:231\n",
      "batch_idx:232\n",
      "batch_idx:233\n",
      "batch_idx:234\n",
      "batch_idx:235\n",
      "batch_idx:236\n",
      "batch_idx:237\n",
      "batch_idx:238\n",
      "batch_idx:239\n",
      "batch_idx:240\n",
      "batch_idx:241\n",
      "batch_idx:242\n",
      "batch_idx:243\n",
      "batch_idx:244\n",
      "batch_idx:245\n",
      "batch_idx:246\n",
      "batch_idx:247\n",
      "batch_idx:248\n",
      "batch_idx:249\n",
      "batch_idx:250\n",
      "batch_idx:251\n",
      "batch_idx:252\n",
      "batch_idx:253\n",
      "batch_idx:254\n",
      "batch_idx:255\n",
      "batch_idx:256\n",
      "batch_idx:257\n",
      "batch_idx:258\n",
      "batch_idx:259\n",
      "batch_idx:260\n",
      "batch_idx:261\n",
      "batch_idx:262\n",
      "batch_idx:263\n",
      "batch_idx:264\n",
      "batch_idx:265\n",
      "batch_idx:266\n",
      "batch_idx:267\n",
      "batch_idx:268\n",
      "batch_idx:269\n",
      "batch_idx:270\n",
      "batch_idx:271\n",
      "batch_idx:272\n",
      "batch_idx:273\n",
      "batch_idx:274\n",
      "batch_idx:275\n",
      "batch_idx:276\n",
      "batch_idx:277\n",
      "batch_idx:278\n",
      "batch_idx:279\n",
      "batch_idx:280\n",
      "batch_idx:281\n",
      "batch_idx:282\n",
      "batch_idx:283\n",
      "batch_idx:284\n",
      "batch_idx:285\n",
      "batch_idx:286\n",
      "batch_idx:287\n",
      "batch_idx:288\n",
      "batch_idx:289\n",
      "batch_idx:290\n",
      "batch_idx:291\n",
      "batch_idx:292\n",
      "batch_idx:293\n",
      "batch_idx:294\n",
      "batch_idx:295\n",
      "batch_idx:296\n",
      "batch_idx:297\n",
      "batch_idx:298\n",
      "batch_idx:299\n",
      "batch_idx:300\n",
      "batch_idx:301\n",
      "batch_idx:302\n",
      "batch_idx:303\n",
      "batch_idx:304\n",
      "batch_idx:305\n",
      "batch_idx:306\n",
      "batch_idx:307\n",
      "batch_idx:308\n",
      "batch_idx:309\n",
      "batch_idx:310\n",
      "batch_idx:311\n",
      "batch_idx:312\n",
      "batch_idx:313\n",
      "batch_idx:314\n",
      "batch_idx:315\n",
      "batch_idx:316\n",
      "batch_idx:317\n",
      "batch_idx:318\n",
      "batch_idx:319\n",
      "batch_idx:320\n",
      "batch_idx:321\n",
      "batch_idx:322\n",
      "batch_idx:323\n",
      "batch_idx:324\n",
      "batch_idx:325\n",
      "batch_idx:326\n",
      "batch_idx:327\n",
      "batch_idx:328\n",
      "batch_idx:329\n",
      "batch_idx:330\n",
      "batch_idx:331\n",
      "batch_idx:332\n",
      "batch_idx:333\n",
      "batch_idx:334\n",
      "batch_idx:335\n",
      "batch_idx:336\n",
      "batch_idx:337\n",
      "batch_idx:338\n",
      "batch_idx:339\n",
      "batch_idx:340\n",
      "batch_idx:341\n",
      "batch_idx:342\n",
      "batch_idx:343\n",
      "batch_idx:344\n",
      "batch_idx:345\n",
      "batch_idx:346\n",
      "batch_idx:347\n",
      "batch_idx:348\n",
      "batch_idx:349\n",
      "batch_idx:350\n",
      "batch_idx:351\n",
      "batch_idx:352\n",
      "batch_idx:353\n",
      "batch_idx:354\n",
      "batch_idx:355\n",
      "batch_idx:356\n",
      "batch_idx:357\n",
      "batch_idx:358\n",
      "batch_idx:359\n",
      "batch_idx:360\n",
      "batch_idx:361\n",
      "batch_idx:362\n",
      "batch_idx:363\n",
      "batch_idx:364\n",
      "batch_idx:365\n",
      "batch_idx:366\n",
      "batch_idx:367\n",
      "batch_idx:368\n",
      "batch_idx:369\n",
      "batch_idx:370\n",
      "batch_idx:371\n",
      "batch_idx:372\n",
      "batch_idx:373\n",
      "batch_idx:374\n",
      "batch_idx:375\n",
      "batch_idx:376\n",
      "batch_idx:377\n",
      "batch_idx:378\n",
      "batch_idx:379\n",
      "batch_idx:380\n",
      "batch_idx:381\n",
      "batch_idx:382\n",
      "batch_idx:383\n",
      "batch_idx:384\n",
      "batch_idx:385\n",
      "batch_idx:386\n",
      "batch_idx:387\n",
      "batch_idx:388\n",
      "batch_idx:389\n",
      "batch_idx:390\n",
      "batch_idx:391\n",
      "batch_idx:392\n",
      "batch_idx:393\n",
      "batch_idx:394\n",
      "batch_idx:395\n",
      "batch_idx:396\n",
      "batch_idx:397\n",
      "batch_idx:398\n",
      "batch_idx:399\n",
      "batch_idx:400\n",
      "batch_idx:401\n",
      "batch_idx:402\n",
      "batch_idx:403\n",
      "batch_idx:404\n",
      "batch_idx:405\n",
      "batch_idx:406\n",
      "batch_idx:407\n",
      "batch_idx:408\n",
      "batch_idx:409\n",
      "batch_idx:410\n",
      "batch_idx:411\n",
      "batch_idx:412\n",
      "batch_idx:413\n",
      "batch_idx:414\n",
      "batch_idx:415\n",
      "batch_idx:416\n",
      "batch_idx:417\n",
      "batch_idx:418\n",
      "batch_idx:419\n",
      "batch_idx:420\n",
      "batch_idx:421\n",
      "batch_idx:422\n",
      "batch_idx:423\n",
      "batch_idx:424\n",
      "batch_idx:425\n",
      "batch_idx:426\n",
      "batch_idx:427\n",
      "batch_idx:428\n",
      "batch_idx:429\n",
      "batch_idx:430\n",
      "batch_idx:431\n",
      "batch_idx:432\n",
      "batch_idx:433\n",
      "batch_idx:434\n",
      "batch_idx:435\n",
      "batch_idx:436\n",
      "batch_idx:437\n",
      "batch_idx:438\n",
      "batch_idx:439\n",
      "batch_idx:440\n",
      "batch_idx:441\n",
      "batch_idx:442\n",
      "batch_idx:443\n",
      "batch_idx:444\n",
      "batch_idx:445\n",
      "batch_idx:446\n",
      "batch_idx:447\n",
      "batch_idx:448\n",
      "batch_idx:449\n",
      "batch_idx:450\n",
      "batch_idx:451\n",
      "batch_idx:452\n",
      "batch_idx:453\n",
      "batch_idx:454\n",
      "batch_idx:455\n",
      "batch_idx:456\n",
      "batch_idx:457\n",
      "batch_idx:458\n",
      "batch_idx:459\n",
      "batch_idx:460\n",
      "batch_idx:461\n",
      "batch_idx:462\n",
      "batch_idx:463\n",
      "batch_idx:464\n",
      "batch_idx:465\n",
      "batch_idx:466\n",
      "batch_idx:467\n",
      "batch_idx:468\n",
      "batch_idx:469\n",
      "batch_idx:470\n",
      "batch_idx:471\n",
      "batch_idx:472\n",
      "batch_idx:473\n",
      "batch_idx:474\n",
      "batch_idx:475\n",
      "batch_idx:476\n",
      "batch_idx:477\n",
      "batch_idx:478\n",
      "batch_idx:479\n",
      "batch_idx:480\n",
      "batch_idx:481\n",
      "batch_idx:482\n",
      "batch_idx:483\n",
      "batch_idx:484\n",
      "batch_idx:485\n",
      "batch_idx:486\n",
      "batch_idx:487\n",
      "batch_idx:488\n",
      "batch_idx:489\n",
      "batch_idx:490\n",
      "batch_idx:491\n",
      "batch_idx:492\n",
      "batch_idx:493\n",
      "batch_idx:494\n",
      "batch_idx:495\n",
      "batch_idx:496\n",
      "batch_idx:497\n",
      "batch_idx:498\n",
      "batch_idx:499\n",
      "batch_idx:500\n",
      "batch_idx:501\n",
      "batch_idx:502\n",
      "batch_idx:503\n",
      "batch_idx:504\n",
      "batch_idx:505\n",
      "batch_idx:506\n",
      "batch_idx:507\n",
      "batch_idx:508\n",
      "batch_idx:509\n",
      "batch_idx:510\n",
      "batch_idx:511\n",
      "batch_idx:512\n",
      "batch_idx:513\n",
      "batch_idx:514\n",
      "batch_idx:515\n",
      "batch_idx:516\n",
      "batch_idx:517\n",
      "batch_idx:518\n",
      "batch_idx:519\n",
      "batch_idx:520\n",
      "batch_idx:521\n",
      "batch_idx:522\n",
      "batch_idx:523\n",
      "batch_idx:524\n",
      "batch_idx:525\n",
      "batch_idx:526\n",
      "batch_idx:527\n",
      "batch_idx:528\n",
      "batch_idx:529\n",
      "batch_idx:530\n",
      "batch_idx:531\n",
      "batch_idx:532\n",
      "batch_idx:533\n",
      "batch_idx:534\n",
      "batch_idx:535\n",
      "batch_idx:536\n",
      "batch_idx:537\n",
      "batch_idx:538\n",
      "batch_idx:539\n",
      "batch_idx:540\n",
      "batch_idx:541\n",
      "batch_idx:542\n",
      "batch_idx:543\n",
      "batch_idx:544\n",
      "batch_idx:545\n",
      "batch_idx:546\n",
      "batch_idx:547\n",
      "batch_idx:548\n",
      "batch_idx:549\n",
      "batch_idx:550\n",
      "batch_idx:551\n",
      "batch_idx:552\n",
      "batch_idx:553\n",
      "batch_idx:554\n",
      "batch_idx:555\n",
      "batch_idx:556\n",
      "batch_idx:557\n",
      "batch_idx:558\n",
      "batch_idx:559\n",
      "batch_idx:560\n",
      "batch_idx:561\n",
      "batch_idx:562\n",
      "batch_idx:563\n",
      "batch_idx:564\n",
      "batch_idx:565\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m loss.backward()\n\u001b[32m     22\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m running_loss += loss.item()\n\u001b[32m     25\u001b[39m _, preds = outputs.max(\u001b[32m1\u001b[39m)                           \u001b[38;5;66;03m# 取每行最大值和索引，得到预测类别\u001b[39;00m\n\u001b[32m     26\u001b[39m correct += (preds == labels).sum().item()           \u001b[38;5;66;03m# 比较预测和真实标签是否一致；\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='../../tensorboard/simple_vit')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch:{}\".format(epoch))\n",
    "    # Train\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        print(\"batch_idx:{}\".format(batch_idx))\n",
    "        \n",
    "        images, labels = images.cuda(), labels.cuda()       # 若无GPU去掉.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds = outputs.max(1)                           # 取每行最大值和索引，得到预测类别\n",
    "        correct += (preds == labels).sum().item()           # 比较预测和真实标签是否一致；\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # 每 step 记录一次 loss\n",
    "        writer.add_scalar('Loss/train_step', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    writer.add_scalar('Loss/train_epoch', epoch_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train_epoch', epoch_acc, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {epoch_loss:.4f}, Train Acc = {epoch_acc:.4f}\")\n",
    "\n",
    "    # Val\n",
    "    model.eval()\n",
    "    val_total, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            _, preds = outputs.max(1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "\n",
    "    val_acc = val_correct / val_total\n",
    "    writer.add_scalar('Accuracy/val_epoch', val_acc, epoch)\n",
    "    print(f\"Epoch {epoch+1}: Validation Accuracy = {val_acc:.4f}\")\n",
    "\n",
    "    save_path = f'../../model/simple_vit/simple_vit_epoch_{epoch+1}.pth'\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Saved model checkpoint to {save_path}\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e184e-444f-4327-a85f-8cfa0f74c67b",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81965982-1ccd-4be7-a3fd-ce2aec3cc90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_vit_output_shape():\n",
    "    model = SimpleViT(\n",
    "        image_size=64,\n",
    "        patch_size=16,\n",
    "        num_classes=10,\n",
    "        dim=128,\n",
    "        depth=6,\n",
    "        heads=8,\n",
    "        mlp_dim=256\n",
    "    )\n",
    "    img = torch.randn(2, 3, 64, 64)\n",
    "    out = model(img)\n",
    "    assert out.shape == (2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d952b6-0e98-4a4e-bfca-3f7a48244390",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
