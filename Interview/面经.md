### 1. Transformer的自注意力机制及相比RNN的优势？

| 对比点        | RNN/LSTM            | Transformer（自注意力）      |
| :--------- | :------------------ | :--------------------- |
| **并行性**    | 序列依赖，必须一步步处理（无法并行）  | 所有位置可同时计算（完全并行）        |
| **长程依赖**   | 随时间步增加，梯度容易消失或爆炸    | 任意两个位置的依赖都通过注意力直接建模    |
| **信息路径长度** | 从第一个到最后一个词路径长度 O(n) | 任意两点间路径长度 O(1)         |
| **捕捉全局语义** | 主要依赖隐藏状态的递归传播       | 每层直接全局建模所有词间关系         |
| **效率与扩展性** | 难以利用 GPU 并行；长序列性能下降 | 计算可完全矩阵化，高效训练；易于扩展到大模型 |
| **记忆能力**   | 隐状态是压缩信息瓶颈          | 多头注意力能显式建模多种依赖关系       |


### 2. 自注意力计算中为何除以 $\sqrt{d_k} $？

随着向量维度 $d_k$ 增大：

* $ Q_i $ 和 $ K_j $ 的每个分量大多为随机变量；
* 点积是所有分量乘积的和；
* 因此：
  $$
  \text{Var}(Q_i \cdot K_j^T) \propto d_k
  $$
  → 方差会随维度线性增长。

这意味着：

* 维度大 → 点积的值变得很大；
* 进入 softmax 前的分数过大 → softmax 输出非常尖锐（几乎只有一个值接近 1，其他接近 0）；
* 导致梯度变得非常小（梯度消失），训练不稳定。

这里举一个例子：
```python
import numpy as np
import pandas as pd
from math import sqrt
import matplotlib.pyplot as plt

# Set up RNG
rng = np.random.default_rng(42)

def simulate(d_k_list=(4, 64, 256), m=64, trials=2000):
    rows = []
    last = {}
    for d_k in d_k_list:
        Q = rng.normal(size=(trials, d_k))
        K = rng.normal(size=(trials, m, d_k))
        logits = np.einsum('td,tmd->tm', Q, K)  # (trials, m)
        
        var_unscaled = logits.var(ddof=1)
        
        max_logits = logits.max(axis=1, keepdims=True)
        exp_logits = np.exp(logits - max_logits)  # numerical stability
        p_unscaled = exp_logits / exp_logits.sum(axis=1, keepdims=True)
        
        max_p_u = p_unscaled.max(axis=1).mean()
        entropy_u = (- (p_unscaled * np.log(np.clip(p_unscaled, 1e-12, 1))).sum(axis=1)).mean()
        frac_over_0_9_u = (p_unscaled.max(axis=1) > 0.9).mean()
        frac_over_0_99_u = (p_unscaled.max(axis=1) > 0.99).mean()
        
        scale = sqrt(d_k)
        logits_s = logits / scale
        
        var_scaled = logits_s.var(ddof=1)
        
        max_logits_s = logits_s.max(axis=1, keepdims=True)
        exp_logits_s = np.exp(logits_s - max_logits_s)
        p_scaled = exp_logits_s / exp_logits_s.sum(axis=1, keepdims=True)
        
        max_p_s = p_scaled.max(axis=1).mean()
        entropy_s = (- (p_scaled * np.log(np.clip(p_scaled, 1e-12, 1))).sum(axis=1)).mean()
        frac_over_0_9_s = (p_scaled.max(axis=1) > 0.9).mean()
        frac_over_0_99_s = (p_scaled.max(axis=1) > 0.99).mean()
        
        rows.append({
            "d_k": d_k,
            "keys(m)": m,
            "logit_var (no scale)": var_unscaled,
            "logit_var (/√d_k)": var_scaled,
            "avg max prob (no scale)": max_p_u,
            "avg max prob (/√d_k)": max_p_s,
            "avg entropy (nats) (no scale)": entropy_u,
            "avg entropy (nats) (/√d_k)": entropy_s,
            "frac max>0.9 (no scale)": frac_over_0_9_u,
            "frac max>0.9 (/√d_k)": frac_over_0_9_s,
            "frac max>0.99 (no scale)": frac_over_0_99_u,
            "frac max>0.99 (/√d_k)": frac_over_0_99_s,
        })
        
        last = {"logits": logits, "logits_s": logits_s, "p_unscaled": p_unscaled, "p_scaled": p_scaled}
        
    df = pd.DataFrame(rows)
    return df, last

# Run simulation
df, last = simulate()

# Round for readability and print
df_rounded = df.round(4)
print("缩放前后注意力数值对比（模拟）")
print(df_rounded.to_string(index=False))

# Save summary to CSV for download
csv_path = "attention_scaling_simulation_summary.csv"
df_rounded.to_csv(csv_path, index=False)

# Plot histograms for the last (d_k=256)
logits_last = last["logits"]
logits_s_last = last["logits_s"]
p_u_last = last["p_unscaled"]
p_s_last = last["p_scaled"]

# Unscaled logits
plt.figure()
plt.hist(logits_last.flatten(), bins=80, alpha=0.7)
plt.title("未缩放 logits 分布（示例 d_k=256, m=64）")
plt.xlabel("logit 值")
plt.ylabel("频数")
plot1_path = "unscaled_logits_hist.png"
plt.savefig(plot1_path, bbox_inches="tight")
plt.show()

# Scaled logits
plt.figure()
plt.hist(logits_s_last.flatten(), bins=80, alpha=0.7)
plt.title("缩放后 logits 分布（/√d_k，示例 d_k=256, m=64）")
plt.xlabel("logit 值")
plt.ylabel("频数")
plot2_path = "scaled_logits_hist.png"
plt.savefig(plot2_path, bbox_inches="tight")
plt.show()

# Max prob (no scale)
plt.figure()
plt.hist(p_u_last.max(axis=1), bins=50, alpha=0.7)
plt.title("未缩放：最大注意力权重分布（d_k=256, m=64）")
plt.xlabel("max softmax prob")
plt.ylabel("频数")
plot3_path = "unscaled_maxprob_hist.png"
plt.savefig(plot3_path, bbox_inches="tight")
plt.show()

# Max prob (scaled)
plt.figure()
plt.hist(p_s_last.max(axis=1), bins=50, alpha=0.7)
plt.title("缩放后：最大注意力权重分布（/√d_k，d_k=256, m=64）")
plt.xlabel("max softmax prob")
plt.ylabel("频数")
plot4_path = "scaled_maxprob_hist.png"
plt.savefig(plot4_path, bbox_inches="tight")
plt.show()

(csv_path, plot1_path, plot2_path, plot3_path, plot4_path)
```

### 3. 现在LLM微调的方式有哪些？

1. Lora；
2. Adapter。

### 4. LLM的架构有哪些？

dense、moe

### 5. Prefix LM与Causal LM区别？

两者在推理阶段（也就是生成时）完全一致，区别在于训练时。

在**推理阶段**（也就是生成时），模型都是通过前面的 token，预测下一个，
所以无论是 **Causal LM** 还是 **Prefix LM**，生成逻辑是完全一样的（都是自回归生成）。


#### Causal LM 训练方式：

它把整个序列 `[A, B, C, D, a, b, c, d]` 都当作训练样本，
模型的目标是：
$$
P(B|A),\ P(C|A,B),\ P(D|A,B,C),\ P(a|A,B,C,D),\ P(b|A,B,C,D,a), ...
$$

也就是说：

* 模型在每个位置都要预测下一个词；
* 没有区分“输入部分”和“输出部分”；
* 整段 `[A,B,C,D,a,b,c,d]` 都会计算 loss。

> 它并不知道前面哪一段是“题目/条件”，哪一段是“答案”。

#### Prefix LM 训练方式：

这里我们人为指定一个“前缀部分”和一个“生成部分”，
比如：

* 前缀 = `[A, B, C, D]`
* 生成 = `[a, b, c, d]`

训练目标是：
$$
P(a,b,c,d \mid A,B,C,D)
$$

也就是说：

* 前缀 `[A,B,C,D]` 是**条件输入**，模型可以完全看到，但不预测；
* 生成部分 `[a,b,c,d]` 是要预测的目标；
* loss 只计算在 `[a,b,c,d]` 上。

> 所以模型训练时不会去“浪费”算力学习预测前缀部分的 token。

### 6. RLHF 是什么？流程如何？

Reinforcement Learning from Human Feedback

1. 监督微调（Supervised Fine-Tuning, SFT）
2. 奖励模型训练（Reward Model, RM）
3. 强化学习优化（RL Fine-Tuning）

### 7. LLM训练中遇到Loss突增如何解决？

1. 训练数据问题
2. 数值 或 梯度问题
3. 学习率 与 优化器超参
4. MoE架构本身问题

### 8. 