{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d8c8f2-3b27-4b10-9037-c1f4d3515fdc",
   "metadata": {},
   "source": [
    "该内容是对simple_vit中内容的讲解，主要分为：\n",
    "- 代码\n",
    "    - attention\n",
    "    - transformer\n",
    "    - vit\n",
    "- 训练\n",
    "    - train\n",
    "- 测试\n",
    "    - test\n",
    " \n",
    "> 与simple_vit中相重合的部分不会再说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d96706-86c9-4de9-8ce3-f5060eca54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7242c7-632e-479d-a7d1-3158e6ca9af1",
   "metadata": {},
   "source": [
    "# 一、代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f06ef-fc65-4cf6-a0f1-790e8489e772",
   "metadata": {},
   "source": [
    "## 1 attention\n",
    "\n",
    "```python\n",
    "project_out = not (heads == 1 and dim_head == dim)\n",
    "```\n",
    "\n",
    "如果是 单头注意力，并且dim_head = dim（即注意力输出和输入维度相同），那么就不需要额外线性映射，直接输出，也就是后面的将to_out 设为 Identity()\n",
    "> 这里其实没太大区别，主要是后面加了个Dropout\n",
    "\n",
    "\n",
    "```python\n",
    "self.dropout = nn.Dropout(dropout)\n",
    "...\n",
    "self.to_out = nn.Sequential(\n",
    "    nn.Linear(inner_dim, dim),\n",
    "    nn.Dropout(dropout)\n",
    ") if project_out else nn.Identity()\n",
    "```\n",
    "主要是两个地方，多加了两个Dropout，原因还是那么几个：\n",
    "- 在深层结构中，避免过拟合（核心目的）\n",
    "- 与 Transformer 论文一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a35f056-74f5-4016-832e-95db8d9f1bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa42f22-a561-4dc5-946e-0c24cd241c71",
   "metadata": {},
   "source": [
    "## 2 transformer\n",
    "\n",
    "### 2.1 FeedForward\n",
    "\n",
    "```python\n",
    "self.net = nn.Sequential(\n",
    "    nn.LayerNorm(dim),\n",
    "    nn.Linear(dim, hidden_dim),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(dropout),\n",
    "    nn.Linear(hidden_dim, dim),\n",
    "    nn.Dropout(dropout)\n",
    ")\n",
    "```\n",
    "相比于simple，多了两个Dropout，原因与前面相同。\n",
    "其余部分与simple相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683eeda5-0df7-4376-b572-1299b0727bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831f19df-91b0-45a2-800a-05010a085d22",
   "metadata": {},
   "source": [
    "## 3 ViT\n",
    "\n",
    "有了simple_vit的基础，ViT中我就不再将Patch Embedding部分分出去了，因为ViT中没有用到单独的位置编码方式，所以整体理解起来反而会更简单。\n",
    "\n",
    "### 3.1 ViT\n",
    "\n",
    "image_size: 图像尺寸，可以是整数或 (高, 宽)。               \n",
    "patch_size: patch 的大小，同样可以是整数或 (高, 宽)。                \n",
    "num_classes: 分类类别数量。                \n",
    "dim: transformer 中 token 的特征维度。              \n",
    "depth: Transformer 层的数量。           \n",
    "heads: 多头注意力的头数。                 \n",
    "mlp_dim: MLP 层的中间维度。                  \n",
    "pool: 表示在输出时是使用 cls token 还是 mean pooling。                        \n",
    "channels: 输入图像的通道数，默认为 3（RGB）。             \n",
    "dim_head: 每个注意力头的维度。                \n",
    "dropout: Transformer 中 dropout 概率。             \n",
    "emb_dropout: 位置嵌入后的 dropout。                        \n",
    "\n",
    "#### 3.1.1 Patch Embedding 部分\n",
    "\n",
    "```python\n",
    "image_height, image_width = pair(image_size)\n",
    "patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "assert image_height % patch_height == 0 and image_width % patch_width == 0, ...\n",
    "```\n",
    "\n",
    "将 image_size 和 patch_size 转为二元组 (height, width)，同时确保图像能被 patch 大小整除。\n",
    "\n",
    "```python\n",
    "num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "```\n",
    "\n",
    "计算：\n",
    "- num_patches: 图像总共被划分成多少个 patch。\n",
    "- patch_dim: 每个 patch 被展平后的维度。\n",
    "\n",
    "```python\n",
    "self.to_patch_embedding = nn.Sequential(\n",
    "    Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
    "    nn.LayerNorm(patch_dim),\n",
    "    nn.Linear(patch_dim, dim),\n",
    "    nn.LayerNorm(dim),\n",
    ")\n",
    "```\n",
    "\n",
    "和simple_vit中相同。\n",
    "\n",
    "### 3.1.2 位置编码和 CLS token\n",
    "\n",
    "```python\n",
    "self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "self.dropout = nn.Dropout(emb_dropout)\n",
    "```\n",
    "\n",
    "相较于simple_vit，ViT中使用可学习的位置编码（原论文便是这么做的），同时加入了cls_token。\n",
    "- pos_embedding: 可学习的位置编码（包括 1 个 cls token）。\n",
    "- cls_token: 类别标识 token，类似于 NLP 中 BERT 的 [CLS]。\n",
    "- dropout: 对加完位置编码的向量进行 dropout。\n",
    "\n",
    "### 3.1.3 Transformer encoder-only\n",
    "\n",
    "```python\n",
    "self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "```\n",
    "\n",
    "就是前面实现的Transformer Encoder。\n",
    "\n",
    "### 3.1.4 Pooling 与分类头\n",
    "\n",
    "```python\n",
    "self.pool = pool\n",
    "self.to_latent = nn.Identity()\n",
    "self.mlp_head = nn.Linear(dim, num_classes)\n",
    "```\n",
    "\n",
    "这里的self.pool是lucidrains自己加的，原论文中只是用了CLS，并没有用这部分，使用 mean pooling 是将所有 patch token（不包括 cls token）或包括 cls token 取平均作为图像表示，有时候在小数据集或迁移学习场景中会表现的更好一些。\n",
    "- self.pool: 控制是取 cls_token 还是取所有 token 的平均。\n",
    "- to_latent 是占位的，保留了进一步处理的可能性（simple_vit也有类似处理）。\n",
    "- mlp_head: 最终分类头，将 Transformer 的输出映射为 num_classes 个类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a20b8-d42f-4125-ad1d-8094179e7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc72d8-531a-4185-bf06-976fffdf79d4",
   "metadata": {},
   "source": [
    "# 训练 & 测试\n",
    "\n",
    "这一部分我就不再赘述了，与simple_vit中一样。       \n",
    "但是后面我可能会继续写一个使用HuggingFace中开源的模型库，跳过训练步骤，直接微调来使用模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
