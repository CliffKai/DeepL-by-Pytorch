# ğŸ” ç±»ï¼š`MistralLLMBackbone`

### ğŸ§  ç±»çš„ä½œç”¨

* æ˜¯ OpenVLA æ¡†æ¶ä¸­ç”¨äºæ¥å…¥ **Mistral ç³»åˆ—æ¨¡å‹** çš„ä¸“ç”¨ LLM æ¥å£ç±»ï¼›
* ç»§æ‰¿è‡ª `HFCausalLLMBackbone`ï¼Œå°è£… HuggingFace çš„ `MistralForCausalLM`ï¼›
* ä¸º OpenVLA çš„è®­ç»ƒã€æ¨ç†ã€å¾®è°ƒæä¾›ä¸ Mistral æ¨¡å‹çš„å¯¹æ¥æ”¯æŒã€‚

---

## ğŸ“š ç±»ä¸­å®šä¹‰çš„å‡½æ•°å’Œå±æ€§æ€»è§ˆï¼š

| ç±»å‹   | åç§°                      | åŠŸèƒ½                        |
| ---- | ----------------------- | ------------------------- |
| æ„é€ å‡½æ•° | `__init__`              | åˆå§‹åŒ– mistral æ¨¡å‹å’Œ tokenizer |
| å±æ€§   | `prompt_builder_fn`     | è¿”å› prompt æ„é€ å™¨ç±»å‹           |
| å±æ€§   | `transformer_layer_cls` | è¿”å› transformer å•å±‚ç±»        |
| å±æ€§   | `half_precision_dtype`  | è¿”å›åŠç²¾åº¦æ•°æ®ç±»å‹                 |

---

## 1ï¸âƒ£ æ„é€ å‡½æ•° `__init__`

```python
def __init__(...)
```

### âœ… å‚æ•°è§£æï¼š

| å‚æ•°å                     | è¯´æ˜                             |
| ----------------------- | ------------------------------ |
| `llm_backbone_id`       | å¦‚ `"mistral-v0.1-7b-instruct"` |
| `llm_max_length`        | æ¨¡å‹æ”¯æŒçš„æœ€å¤§è¾“å…¥ token æ•°              |
| `hf_token`              | HuggingFace ä¸‹è½½ token           |
| `inference_mode`        | æ˜¯å¦å¯ç”¨æ¨ç†æ¨¡å¼ï¼ˆéè®­ç»ƒï¼‰                  |
| `use_flash_attention_2` | æ˜¯å¦å¯ç”¨ Flash Attention v2        |

---

### âœ… æ ¸å¿ƒé€»è¾‘é€è¡Œè®²è§£ï¼š

```python
super().__init__(..., **MISTRAL_MODELS[llm_backbone_id])
```

* ä¼ å…¥å½“å‰æ¨¡å‹çš„å…ƒä¿¡æ¯ç»™çˆ¶ç±»æ„é€ å™¨ï¼›
* `MISTRAL_MODELS` æ˜¯ä¸€ä¸ªæ³¨å†Œå­—å…¸ï¼Œè¿”å›ç»“æ„å¦‚ä¸‹ï¼š

  ```python
  {
    "llm_family": "mistral",
    "llm_cls": MistralForCausalLM,
    "hf_hub_path": "mistralai/Mistral-7B-v0.1"
  }
  ```

---

### ğŸ”¸ ç‰¹æ®Šå¤„ç†ï¼šæ·»åŠ  `<PAD>` token

```python
self.tokenizer.add_special_tokens({"pad_token": "<PAD>"})
```

* HuggingFace çš„åŸå§‹ `MistralTokenizer` å¯èƒ½æ²¡æœ‰ `pad_token`ï¼›
* ä¸ºäº†è®­ç»ƒæ—¶ batch å¯¹é½ï¼Œå¿…é¡»æ˜¾å¼æ·»åŠ  `<PAD>`ã€‚

```python
self.llm.config.pad_token_id = self.tokenizer.pad_token_id
```

* è®¾ç½® `pad_token_id`ï¼Œå¦åˆ™æ¨¡å‹åœ¨æ¨ç†æ—¶æ— æ³•æ­£ç¡®åŒºåˆ† paddingã€‚

```python
self.llm.resize_token_embeddings(len(self.tokenizer), pad_to_multiple_of=64)
```

* é‡æ–° resize åµŒå…¥å±‚ï¼Œä½¿å…¶æ”¯æŒæ–°å¢çš„ tokenï¼›
* padding åˆ° 64 çš„å€æ•°æ˜¯ä¸ºäº†åŠ é€Ÿå’Œå¯¹é½ã€‚

---

## 2ï¸âƒ£ å±æ€§ `prompt_builder_fn`

```python
@property
def prompt_builder_fn(self) -> Type[PromptBuilder]:
```

### âœ… åŠŸèƒ½ï¼š

æ ¹æ®æ¨¡å‹ ID çš„åç¼€é€‰æ‹©åˆé€‚çš„ PromptBuilderï¼š

| åç¼€          | æ„é€ å™¨ç±»                                 |
| ----------- | ------------------------------------ |
| `-pure`     | `PurePromptBuilder`ï¼ˆçº¯æ–‡æœ¬ï¼‰             |
| `-instruct` | `MistralInstructPromptBuilder`ï¼ˆæŒ‡ä»¤å¾®è°ƒï¼‰ |

è‹¥éƒ½ä¸åŒ¹é…åˆ™æŠ›å‡ºé”™è¯¯ï¼š

```python
raise ValueError(f"No PromptBuilder defined for LLM Backbone `{self.identifier}`")
```

---

## 3ï¸âƒ£ å±æ€§ `transformer_layer_cls`

```python
@property
def transformer_layer_cls(self) -> Type[nn.Module]:
    return MistralDecoderLayer
```

### âœ… åŠŸèƒ½ï¼š

* è¿”å› Mistral æ¨¡å‹ä¸­ decoder å±‚çš„ç±»ï¼ˆå®šä¹‰åœ¨ `transformers.models.mistral.modeling_mistral`ï¼‰ï¼›
* è¿™ä¸ªç±»ä¼šç”¨äº FSDP çš„ transformer è‡ªåŠ¨åŒ…è£¹ç­–ç•¥ä¸­ï¼›
* å¿…é¡»å‡†ç¡®æŒ‡å®š decoder å±‚ï¼Œå¦åˆ™è®­ç»ƒæ—¶çš„åˆ†å¸ƒå¼åŒ…è£…ç­–ç•¥æ— æ³•ç”Ÿæ•ˆã€‚

---

## 4ï¸âƒ£ å±æ€§ `half_precision_dtype`

```python
@property
def half_precision_dtype(self) -> torch.dtype:
    return torch.bfloat16
```

### âœ… åŠŸèƒ½ï¼š

* æŒ‡å®šè¯¥æ¨¡å‹æ¨èçš„åŠç²¾åº¦ç±»å‹ï¼›
* Mistral åŸå§‹è®­ç»ƒä½¿ç”¨çš„æ˜¯ `bfloat16`ï¼Œæ‰€ä»¥è¿™é‡Œä¹Ÿä¸€è‡´è®¾ç½®ã€‚

---

## âœ… æ€»ç»“ï¼š`MistralLLMBackbone`

| æ¨¡å‹ç±»åˆ«             | å¯¹æ¥çš„ HF æ¨¡å‹            |
| ---------------- | -------------------- |
| Mistral-7B       | `MistralForCausalLM` |
| Mistral Instruct | æŒ‡ä»¤å¾®è°ƒç‰ˆ                |

| åŠŸèƒ½æ¨¡å—         | å®ç°è¯´æ˜                        |
| ------------ | --------------------------- |
| æ¨¡å‹åŠ è½½         | é€šè¿‡ `HFCausalLLMBackbone` å®Œæˆ |
| Tokenizer å¤„ç† | æ·»åŠ  `<PAD>`ï¼Œè°ƒæ•´åµŒå…¥ç»´åº¦           |
| Prompt æ„é€ å™¨   | æ ¹æ® ID åç¼€è‡ªåŠ¨é€‰æ‹©                |
| æ··åˆç²¾åº¦         | æ”¯æŒ bfloat16                 |
| FSDP æ”¯æŒ      | æä¾› decoder å±‚ç±»å‹ä¾›åŒ…è£¹           |

---

è¿™ä¸ªç±»å°è£…å¾—éå¸¸æ¸…æ™°ï¼Œå’Œ `LLaMa2LLMBackbone` æ˜¯ä¸€è‡´çš„è®¾è®¡æ¨¡å¼ã€‚å¯ä»¥åšåˆ°ç»Ÿä¸€è°ƒç”¨ã€è‡ªåŠ¨æ¨ç† tokenizerã€æ”¯æŒå¤šç§ finetuning æ¨¡å¼ã€‚

å¦‚æœä½ æ¥ä¸‹æ¥è¦è®²è§£ `qwen.py` æˆ– `idefics.py`ï¼Œæˆ‘ä¼šç»§ç»­ç”¨æ­¤æ ‡å‡†åˆ†æã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ
