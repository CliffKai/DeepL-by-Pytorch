在这篇论文中，作者提出了两种检索增强生成（RAG）的架构：RAG-Sequence 和 RAG-Token。这两者都将检索到的文档 $z$ 视为隐变量（Latent Variable），但在对这些文档进行边缘化处理（Marginalize，即加权求和）的时间点上有所不同。

### 1. RAG-Sequence 模型（序列模型）

**核心思想：** 该模型假设对于整个生成序列，都使用**同一个**检索到的文档作为上下文。它先针对每个文档生成完整的候选答案，最后再在序列层面进行概率汇总。

**数学推导与公式：** RAG-Sequence 将文档 $z$ 视为单一隐变量。为了得到生成序列 $y$ 的总概率，它对 Top-K 个文档生成的序列概率进行边缘化 ：
$$p_{RAG-Sequence}(y|x) \approx \sum_{z \in top-k(p(\cdot|x))} p_{\eta}(z|x) p_{\theta}(y|x, z)$$

利用链式法则展开生成器部分，公式如下：
$$p_{RAG-Sequence}(y|x) \approx \sum_{z \in top-k(p(\cdot|x))} p_{\eta}(z|x) \prod_{i}^{N} p_{\theta}(y_i | x, z, y_{1:i-1})$$

* **特点：** 生成的内容在逻辑上高度一致，因为整个句子都源于同一参考资料 。

* **解码：** 无法通过单一的集束搜索（Beam Search）求解，需要为每个文档单独生成候选序列，再计算总和（称为“彻底解码”或“快速解码”） 。

### 2. RAG-Token 模型（逐词模型）

**核心思想：** 该模型允许在生成序列的**每一个 Token**（单词/符号）时，都根据不同的文档来抽取信息 。它在每一步生成时都会对文档分布进行汇总 。

**数学推导与公式：** 在该模型中，隐变量的边缘化发生在每一个 Token 的预测步骤中 12。序列 $y$ 的概率是每一步预测概率的乘积：
$$p_{RAG-Token}(y|x) \approx \prod_{i}^{N} \sum_{z \in top-k(p(\cdot|x))} p_{\eta}(z|x) p_{\theta}(y_i | x, z, y_{1:i-1})$$
* **特点：** 极高的灵活性，模型可以结合来自多个不同文档的知识片段来形成一个完整的答案 。

* **解码：** 可以直接使用标准的自回归集束搜索解码器，因为在每一步预测 $y_i$ 时，模型已经通过求和得到了当前步的综合概率分布。

### 3. 公式中的关键组件说明

在上述公式中，包含两个核心可学习组件 ：

1. **检索器 (Retriever) $p_{\eta}(z|x)$：**
* 基于 DPR（稠密通路检索器），计算查询 $x$ 和文档 $z$ 的向量内积分布；
* $$p_{\eta}(z|x) \propto \exp(d(z)^{\top}q(x))$$
* 其中 $d(z)$ 是文档编码，$q(x)$ 是查询编码；

2. **生成器 (Generator) $p_{\theta}(y_i | x, z, y_{1:i-1})$：**
* 基于预训练的 BART-large 模型，根据输入、选定文档及已生成的上下文来预测下一个词。

### 总结对比

| 维度             | RAG-Sequence (序列级)      | RAG-Token (词级)                |
| ---------------- | -------------------------- | ------------------------------- |
| **边缘化时机**   | 生成完整序列 $y$ 之后         | 生成每个词 $y_i$ 之时                 |
| **文档使用逻辑** | 一个句子 = 一个文档上下文  | 一个句子 = 多个文档上下文的混合 |
| **主要优势**     | 序列连贯性强，适合事实抽取 | 知识密度高，适合整合多方信息    |
| **解码方式**     | 针对每个文档分别搜索再求和 | 标准的自回归集束搜索            |
