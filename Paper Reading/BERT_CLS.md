## 一、什么是 `[CLS]`？

在 BERT 的输入中，每个句子（或句子对）都会在最前面添加一个特殊的 token，叫做 `[CLS]`，表示 “classification（分类）”。

- 是 BERT 特别引入的，不对应任何实际的词。
- 用于聚合整个输入序列的语义（代表“整个输入句子/句子对”的整体语义）。
- 它的位置固定在输入序列的最前面，输出的 `[CLS]` 向量就是“整句语义表示”。

---

## 二、BERT 的输出是什么？

BERT 接收一个 token 序列，输出每个 token 的语义向量（隐藏状态）：

**输入示例：**

```
[CLS] 我 爱 机器 学习 [SEP]
```

**输出：**

```
H_[CLS], H_我, H_爱, H_机器, H_学习, H_[SEP]
```

每个 `H_x` 是一个向量（如 768维），表示对应 token 的语义。

---

## 三、为什么用 `[CLS]` 向量进行分类？

这是 BERT 的设计逻辑：

> BERT 会专门训练 `[CLS]` token 聚合整个输入句子的语义信息。

因此，在做分类任务（如文本情感分析、文本蕴含、句子关系判断）时，我们可以只用 H_[CLS] 这个向量，接一个全连接层（Linear Layer），就可以直接做分类了。

**分类方式：**

```text
类别预测 = softmax(W · H_[CLS] + b)
```

- `H_[CLS]`: BERT 输出的 `[CLS]` 向量
- `W, b`: 线性分类层的参数
- `softmax`: 用于输出概率分布，选出最可能的类别

---

## 四、举例：情感分类任务

**输入句子：**

```
[CLS] This movie is amazing ! [SEP]
```

**输出向量：**

```
H_[CLS], H_This, H_movie, H_is, ...
```

我们用 `H_[CLS]` 送入线性层 + softmax，输出两个类别的概率：

- `Positive`
- `Negative`

训练目标就是让 H_[CLS] 经过线性层输出这两个类的概率。

---

## 五、其他位置的向量用途？

| Token位置   | 语义代表         | 用途示例                           |
|-------------|------------------|------------------------------------|
| `[CLS]`     | 整体句子表示     | 用于句子级分类任务（如情感分析）   |
| 中间 token  | 每个词的语义表示 | 用于序列标注任务（如命名实体识别NER） |
| `[SEP]`     | 分隔两个句子     | 用于句子对分隔                     |

---

## 六、图示理解

```text
输入：
[CLS]  我  喜欢  AI  [SEP]

输出：
H_[CLS], H_我, H_喜欢, H_AI, H_[SEP]

H_[CLS] → 线性层 → softmax → 分类结果
```

---

## 七、总结一句话：

> `[CLS]` 向量就像是“整句话的语义快照”，我们用它来代表整个句子，然后用来进行分类、判断等全句任务。

---