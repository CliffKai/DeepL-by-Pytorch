{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bd09d30ca47d67",
   "metadata": {},
   "source": [
    "该内容是对simple_vit中内容的讲解，主要分为：\n",
    "- 代码\n",
    "    - attention\n",
    "    - transformer\n",
    "    - embedding\n",
    "    - simple_vit\n",
    "- 测试\n",
    "    - test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f371d8fc601812",
   "metadata": {},
   "source": [
    "# 代码\n",
    "\n",
    "这一部分包含：\n",
    "- attention\n",
    "- transformer\n",
    "- embedding\n",
    "- simple_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T08:55:38.548335Z",
     "start_time": "2025-05-05T08:55:35.525985Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cfb9a7-f897-44c8-9ea9-c4c14516cb70",
   "metadata": {},
   "source": [
    "## attention\n",
    "\n",
    "dim：输入特征的维度（embedding 的维度）。            \n",
    "heads：注意力头的数量。            \n",
    "dim_head：每个头的维度。             \n",
    "\n",
    "这里self.to_qkv()等价于把输入 X向右扩展为 [X, X, X]，同时左乘拼接起来的矩阵 [[Wq], [Wk], [Wv]]，从而一次性得到 Q、K、V。\n",
    "然后后面非常关键的一步：\n",
    "```python\n",
    "map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "```\n",
    "将三维的h个QKV首先分别拆分为Q、K和V，然后再将三维的Q、K和V分别拆为三维的Q_i、K_i和V_i，然后以四维的形式存储在三个变量q, k, v（相当于q = [Q_0, Q_1, ... , Q_h]，kv同理），所以 torch.matmul 这一步才能分别计算Q_i × K_i^T。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d105ae5-d9f4-4f8f-9ee5-b6054912ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads               # Q、K、V 的拼接维度。\n",
    "        self.heads = heads                         \n",
    "        self.scale = dim_head ** -0.5              # 用于缩放 QK 的点积，防止 softmax 后梯度过小或过大\n",
    "        self.norm = nn.LayerNorm(dim)              # 层归一化，通常用于稳定训练。\n",
    "        self.attend = nn.Softmax(dim = -1)         # 对最后一个维度（注意力得分）做 softmax，形成注意力权重（也就是对Score做softmax，得到Attention）\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)     # 这是非常关键的一步处理\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3879c6b-b591-4c94-bc5d-a7be112a87a8",
   "metadata": {},
   "source": [
    "## transformer\n",
    "\n",
    "这一部分是利用前面的注意力计算方法搭配一个前馈神经网络 FeedForward Neural Network 来进行 transformer encoder 块的构建。\n",
    "\n",
    "### FeedForward\n",
    "              \n",
    "主要目的是为了引入非线性，提高模型的表达能力。    \n",
    "\n",
    "### ModuleList    \n",
    "\n",
    "torch.nn.ModuleList 是一个容器，专门用于存放多个子模块，和 nn.Sequential 不同，Sequential 是一个可执行模型，自动按顺序执行子模块，而 nn.ModuleList 是一个模块列表容器，不执行，只注册，执行逻辑自己写，就比如本模块中的前向传播部分：\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    for attn, ff in self.layers:\n",
    "        x = attn(x) + x\n",
    "        x = ff(x) + x\n",
    "    return self.norm(x)\n",
    "```\n",
    "\n",
    "Sequential 只适用于线性、固定的前向流程，但是本模型中要使用残差连接，所以要使用 ModuleList。\n",
    "\n",
    "### Transformer\n",
    "\n",
    "dim：输入 token 的维度（即 embedding 维度）            \n",
    "depth：Block 的层数，也就是 Transformer 的堆叠深度             \n",
    "heads：注意力头的数量              \n",
    "dim_head：每个注意力头的维度             \n",
    "mlp_dim：FeedForward 中间层的维度（即升维后的维度）                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eea08c2-a627-4901-8cad-860d57e6875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                Attention(dim, heads=heads, dim_head=dim_head),\n",
    "                FeedForward(dim, mlp_dim)\n",
    "            ])\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1461c14-9e22-46af-9a06-d3ec0371be08",
   "metadata": {},
   "source": [
    "## embedding\n",
    "\n",
    "这段负责图像切分、Patch 嵌入和位置编码，也就是将图像输入转为 token 表示的模块。\n",
    "\n",
    "### pair\n",
    "\n",
    "如果输入是整数，就把它变成形如 (t, t) 的二元组，e.g：\n",
    "pair(32) → (32, 32)\n",
    "pair((16, 8)) → (16, 8)\n",
    "\n",
    "### posemb_sincos_2d\n",
    "\n",
    "h、w：图像划分 patch 后的高和宽（即 patch 网格大小）                \n",
    "dim：每个位置编码的维度，必须是 4 的倍数\n",
    "temperature：控制正余弦波的频率范围，默认为 10000（和原始 Transformer 中一致）\n",
    "dtype：输出的张量类型（float32）\n",
    "\n",
    "1. 根据高和宽创建二维网格坐标：torch.arange(h) → 创建 [0, 1, 2, ..., h-1]\n",
    "2. meshgrid 构造二维坐标网格：y垂直，x水平\n",
    "3. indexing=\"ij\" 表示按矩阵坐标生成（即第一个维度是行，第二个是列）\n",
    "\n",
    "torch.meshgrid(*tensors, indexing='ij')      \n",
    "\n",
    "\n",
    "在ViT原论文中，位置编码是一维的、可学习的（learnable 1D positional embedding），但是 vit-pytorch 中的 simple_vit.py 实现中，作者 lucidrains 使用了不可学习的二维正余弦位置编码（sinusoidal 2D positional encoding），原因如下：\n",
    "- 不需要训练，因此泛化性好\n",
    "- 对小模型更稳定\n",
    "- 简洁性与泛化性优先\n",
    "- 避免可学习位置编码带来的 patch 数不匹配问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5063c-9b36-4376-bc78-d6b0bb7e2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "def posemb_sincos_2d(h, w, dim, temperature: int = 10000, dtype=torch.float32):\n",
    "    y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\")\n",
    "    assert (dim % 4) == 0, \"feature dimension must be multiple of 4 for sincos emb\"\n",
    "    omega = torch.arange(dim // 4) / (dim // 4 - 1)\n",
    "    omega = 1.0 / (temperature ** omega)\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :]\n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
    "    return pe.type(dtype)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, dim, channels=3):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.rearrange = Rearrange(\"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\", p1=patch_height, p2=patch_width)\n",
    "        self.net = nn.Sequential(\n",
    "            self.rearrange,\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3f8e4-c16b-435a-a267-dd8cb201bfdc",
   "metadata": {},
   "source": [
    "## simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4885a9-4474-4143-b16e-3acab8c8cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dim_head=64):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "\n",
    "        self.to_patch_embedding = PatchEmbedding(image_size, patch_size, dim, channels)\n",
    "\n",
    "        self.pos_embedding = posemb_sincos_2d(\n",
    "            h = image_height // patch_height,\n",
    "            w = image_width // patch_width,\n",
    "            dim = dim,\n",
    "        )\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
    "\n",
    "        self.pool = \"mean\"\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.linear_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        device = img.device\n",
    "        x = self.to_patch_embedding(img)\n",
    "        x += self.pos_embedding.to(device, dtype=x.dtype)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.to_latent(x)\n",
    "        return self.linear_head(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e184e-444f-4327-a85f-8cfa0f74c67b",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81965982-1ccd-4be7-a3fd-ce2aec3cc90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_vit_output_shape():\n",
    "    model = SimpleViT(\n",
    "        image_size=64,\n",
    "        patch_size=16,\n",
    "        num_classes=10,\n",
    "        dim=128,\n",
    "        depth=6,\n",
    "        heads=8,\n",
    "        mlp_dim=256\n",
    "    )\n",
    "    img = torch.randn(2, 3, 64, 64)\n",
    "    out = model(img)\n",
    "    assert out.shape == (2, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
