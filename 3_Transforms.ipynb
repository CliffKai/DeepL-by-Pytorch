{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32f7fc4-7721-4906-a0ab-9c6179c7b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e168d-8f23-4922-abd2-ed0fb940df4a",
   "metadata": {},
   "source": [
    "通过transforms.ToTensor解决两个问题   \n",
    "1.transforms如何使用   \n",
    "2.Tensor数据类型相较于普通图片数据类型有何不同   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dbdbced-1a2f-47f1-9242-d801e3577769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=768x512 at 0x107365EB0>\n"
     ]
    }
   ],
   "source": [
    "img_path = \"../data/train/ants_image/0013035.jpg\"\n",
    "# Image读取到的是PIL类型\n",
    "img = Image.open(img_path)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272cd99c-defa-4047-aa70-99ba360af6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[233 151  80]\n",
      "  [233 151  80]\n",
      "  [233 151  80]\n",
      "  ...\n",
      "  [234 152  81]\n",
      "  [232 150  79]\n",
      "  [229 147  76]]\n",
      "\n",
      " [[234 152  81]\n",
      "  [234 152  81]\n",
      "  [234 152  81]\n",
      "  ...\n",
      "  [234 152  81]\n",
      "  [232 150  79]\n",
      "  [229 147  76]]\n",
      "\n",
      " [[235 153  82]\n",
      "  [235 153  82]\n",
      "  [235 153  82]\n",
      "  ...\n",
      "  [233 151  80]\n",
      "  [232 150  79]\n",
      "  [230 148  77]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[237 160  87]\n",
      "  [237 160  87]\n",
      "  [236 159  86]\n",
      "  ...\n",
      "  [141  92  44]\n",
      "  [235 158  95]\n",
      "  [228 157  90]]\n",
      "\n",
      " [[237 160  87]\n",
      "  [237 160  87]\n",
      "  [236 159  86]\n",
      "  ...\n",
      "  [226 147  84]\n",
      "  [255 160  90]\n",
      "  [233 152  84]]\n",
      "\n",
      " [[237 160  87]\n",
      "  [237 160  87]\n",
      "  [236 159  86]\n",
      "  ...\n",
      "  [242 160  79]\n",
      "  [250 159  78]\n",
      "  [233 161  84]]]\n"
     ]
    }
   ],
   "source": [
    "# cv2读取到的数据是numpy类型\n",
    "cv_img = cv2.imread(img_path)\n",
    "print(cv_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dcbe4a5-d3e8-417f-9bc9-00a7bb620b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.Tensor数据类型相较于普通图片数据类型有何不同？为什么需要Tensor\n",
    "# 自行百度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f60fdb-7fda-4742-841a-92d61148f03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3137, 0.3137, 0.3137,  ..., 0.3176, 0.3098, 0.2980],\n",
      "         [0.3176, 0.3176, 0.3176,  ..., 0.3176, 0.3098, 0.2980],\n",
      "         [0.3216, 0.3216, 0.3216,  ..., 0.3137, 0.3098, 0.3020],\n",
      "         ...,\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.1725, 0.3725, 0.3529],\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.3294, 0.3529, 0.3294],\n",
      "         [0.3412, 0.3412, 0.3373,  ..., 0.3098, 0.3059, 0.3294]],\n",
      "\n",
      "        [[0.5922, 0.5922, 0.5922,  ..., 0.5961, 0.5882, 0.5765],\n",
      "         [0.5961, 0.5961, 0.5961,  ..., 0.5961, 0.5882, 0.5765],\n",
      "         [0.6000, 0.6000, 0.6000,  ..., 0.5922, 0.5882, 0.5804],\n",
      "         ...,\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.3608, 0.6196, 0.6157],\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.5765, 0.6275, 0.5961],\n",
      "         [0.6275, 0.6275, 0.6235,  ..., 0.6275, 0.6235, 0.6314]],\n",
      "\n",
      "        [[0.9137, 0.9137, 0.9137,  ..., 0.9176, 0.9098, 0.8980],\n",
      "         [0.9176, 0.9176, 0.9176,  ..., 0.9176, 0.9098, 0.8980],\n",
      "         [0.9216, 0.9216, 0.9216,  ..., 0.9137, 0.9098, 0.9020],\n",
      "         ...,\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.5529, 0.9216, 0.8941],\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.8863, 1.0000, 0.9137],\n",
      "         [0.9294, 0.9294, 0.9255,  ..., 0.9490, 0.9804, 0.9137]]])\n"
     ]
    }
   ],
   "source": [
    "# 2.transforms如何使用\n",
    "# 将其赋予一个变量，该变量就成为了一个可以处理其他图片的对象，可以理解为下面这样：\n",
    "# tool = transforms.ToTensor()\n",
    "# result = tool(input)\n",
    "trans_tensor = transforms.ToTensor()\n",
    "tensor_img = trans_tensor(img)\n",
    "\n",
    "print(tensor_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "557568ea-126a-4578-a90c-f6f641626ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"../logs/3_Transforms\")\n",
    "# writer.add_image(\"Tensor_img\", tensor_img)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df22abbf-f765-49ac-8ce5-53619f1d8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常见Transfoms中的工具的使用\n",
    "# 重点关注：\n",
    "# 输入、输出、作用\n",
    "\n",
    "img_path = \"../imgs/test.png\"\n",
    "# Image读取到的是PIL类型\n",
    "img = Image.open(img_path)\n",
    "# print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63df2163-55a1-45c8-8f30-c7cec246e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToTensor:Convert a PIL Image or ndarray to tensor and scale the values accordingly\n",
    "# 将其他类型转换为Tensor类型\n",
    "trans_totensor = transforms.ToTensor()\n",
    "img_tensor = trans_totensor(img)\n",
    "writer.add_image(\"ToTensor\", img_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69519ef7-c39a-4135-9a2c-e92adde4cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7882)\n",
      "tensor(0.5765)\n"
     ]
    }
   ],
   "source": [
    "# Normalize:Normalize a tensor image with mean and standard deviation\n",
    "# 将tensor类型进行归一化（通过均值和标准差）\n",
    "# 作用：改变图片中的一些值，使图片更加契合后面要进入的网络\n",
    "# 举例：input[0, 1],使用mean[0.5], std[0.5]经过计算公式后就成了output[-1, 1]\n",
    "# 计算公式：output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "    # Args:\n",
    "    #     mean (sequence): Sequence of means for each channel.\n",
    "    #     std (sequence): Sequence of standard deviations for each channel.\n",
    "    #     mean和std需要根据图片的信道来进行填充，eg：RGB图片为三通道，PNG为四通道\n",
    "    #     inplace(bool,optional): Bool to make this operation in-place.     默认为False\n",
    "print(img_tensor[0][0][0])\n",
    "trans_norm = transforms.Normalize([0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.5, 0.5]) \n",
    "img_norm = trans_norm(img_tensor)\n",
    "print(img_norm[0][0][0])\n",
    "writer.add_image(\"Normalize\", img_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a09861-6ab2-4e4a-a52a-b549274b34bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1800, 3200])\n",
      "tensor([[[0.8008, 0.8183, 0.8183,  ..., 0.1188, 0.1202, 0.1241],\n",
      "         [0.6520, 0.8126, 0.8218,  ..., 0.0894, 0.0933, 0.0955],\n",
      "         [0.3291, 0.7746, 0.8064,  ..., 0.0034, 0.0027, 0.0036],\n",
      "         ...,\n",
      "         [0.1096, 0.2304, 0.2316,  ..., 0.1141, 0.1136, 0.1141],\n",
      "         [0.1130, 0.2307, 0.2317,  ..., 0.1137, 0.1137, 0.1137],\n",
      "         [0.1136, 0.2308, 0.2318,  ..., 0.1137, 0.1137, 0.1137]],\n",
      "\n",
      "        [[0.2769, 0.2739, 0.2757,  ..., 0.2092, 0.2092, 0.2080],\n",
      "         [0.2322, 0.2725, 0.2722,  ..., 0.1460, 0.1425, 0.1409],\n",
      "         [0.1314, 0.2770, 0.2751,  ..., 0.0049, 0.0042, 0.0048],\n",
      "         ...,\n",
      "         [0.1279, 0.3019, 0.3059,  ..., 0.2075, 0.2076, 0.2074],\n",
      "         [0.1296, 0.3028, 0.3056,  ..., 0.2078, 0.2078, 0.2078],\n",
      "         [0.1305, 0.3029, 0.3057,  ..., 0.2078, 0.2078, 0.2078]],\n",
      "\n",
      "        [[0.2105, 0.2042, 0.1889,  ..., 0.2368, 0.2334, 0.2336],\n",
      "         [0.1819, 0.2008, 0.1949,  ..., 0.1666, 0.1655, 0.1656],\n",
      "         [0.1171, 0.2176, 0.2072,  ..., 0.0117, 0.0068, 0.0084],\n",
      "         ...,\n",
      "         [0.1893, 0.4557, 0.4710,  ..., 0.2471, 0.2471, 0.2471],\n",
      "         [0.1907, 0.4536, 0.4710,  ..., 0.2471, 0.2471, 0.2471],\n",
      "         [0.1917, 0.4536, 0.4711,  ..., 0.2471, 0.2471, 0.2471]],\n",
      "\n",
      "        [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])\n",
      "torch.Size([4, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# Resize:Resize the input image to the given size\n",
    "# 将传入的图像进行尺寸缩放\n",
    "# Args:\n",
    "#         size (sequence or int): Desired output size. If size is a sequence like\n",
    "#             (h, w), output size will be matched to this. If size is an int,\n",
    "#             smaller edge of the image will be matched to this number.\n",
    "#             i.e, if height > width, then image will be rescaled to\n",
    "#             (size * height / width, size).\n",
    "print(img_tensor.shape)\n",
    "trans_resize = transforms.Resize((512, 512))\n",
    "img_resize = trans_resize(img_tensor)\n",
    "print(img_resize)\n",
    "print(img_resize.shape)\n",
    "writer.add_image(\"Resize\", img_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec2c2ec4-e95e-4866-b58c-3350d0449bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose:Composes several transforms together\n",
    "# 将多个Transforms的工具整合到一起，一次调用，方便使用\n",
    "    # Example:\n",
    "    #     >>> transforms.Compose([\n",
    "    #     >>>     transforms.CenterCrop(10),\n",
    "    #     >>>     transforms.PILToTensor(),\n",
    "    #     >>>     transforms.ConvertImageDtype(torch.float),\n",
    "    #     >>> ])\n",
    "\n",
    "# 这里将前面的几种变化结合起来:ToTensor,Normalize,Resize\n",
    "trans_compose = transforms.Compose([\n",
    "    trans_tensor,\n",
    "    trans_norm,\n",
    "    trans_resize,\n",
    "])\n",
    "img_compose = trans_compose(img)\n",
    "writer.add_image(\"Compose\", img_compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "826c6601-074d-499a-b4b0-724fa99f7e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomCrop:Crop the given image at a random location\n",
    "    # Args:\n",
    "    #     size (sequence or int): Desired output size of the crop. If size is an\n",
    "    #         int instead of sequence like (h, w), a square crop (size, size) is\n",
    "    #         made. If provided a sequence of length 1, it will be interpreted as (size[0], size[0]).\n",
    "    #     padding (int or sequence, optional): Optional padding on each border\n",
    "    #         of the image. Default is None. If a single int is provided this\n",
    "    #         is used to pad all borders. If sequence of length 2 is provided this is the padding\n",
    "    #         on left/right and top/bottom respectively. If a sequence of length 4 is provided\n",
    "    #         this is the padding for the left, top, right and bottom borders respectively.\n",
    "\n",
    "    #         .. note::\n",
    "    #             In torchscript mode padding as single int is not supported, use a sequence of\n",
    "    #             length 1: ``[padding, ]``.\n",
    "    #     pad_if_needed (boolean): It will pad the image if smaller than the\n",
    "    #         desired size to avoid raising an exception. Since cropping is done\n",
    "    #         after padding, the padding seems to be done at a random offset.\n",
    "    #     fill (number or tuple): Pixel fill value for constant fill. Default is 0. If a tuple of\n",
    "    #         length 3, it is used to fill R, G, B channels respectively.\n",
    "    #         This value is only used when the padding_mode is constant.\n",
    "    #         Only number is supported for torch Tensor.\n",
    "    #         Only int or tuple value is supported for PIL Image.\n",
    "    #     padding_mode (str): Type of padding. Should be: constant, edge, reflect or symmetric.\n",
    "    #         Default is constant.\n",
    "\n",
    "    #         - constant: pads with a constant value, this value is specified with fill\n",
    "\n",
    "    #         - edge: pads with the last value at the edge of the image.\n",
    "    #           If input a 5D torch Tensor, the last 3 dimensions will be padded instead of the last 2\n",
    "\n",
    "    #         - reflect: pads with reflection of image without repeating the last value on the edge.\n",
    "    #           For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n",
    "    #           will result in [3, 2, 1, 2, 3, 4, 3, 2]\n",
    "\n",
    "    #         - symmetric: pads with reflection of image repeating the last value on the edge.\n",
    "    #           For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n",
    "    #           will result in [2, 1, 1, 2, 3, 4, 4, 3]\n",
    "\n",
    "    # 参数有点多，其他都有默认值一般不用管，一般只需要第一个size\n",
    "    # size:和resize不同，(h, w)是按照高和宽来进行裁剪,如果是单个数字x则会裁剪为边长为x的正方形\n",
    "trans_randomcrop_1 = transforms.RandomCrop(512)\n",
    "trans_randomcrop_2 = transforms.RandomCrop((180, 320))\n",
    "\n",
    "for i in range(100):\n",
    "    img_randomcrop = trans_randomcrop_1(img_tensor)\n",
    "    writer.add_image(\"RandomCrop1\", img_randomcrop, i)\n",
    "    img_randomcrop = trans_randomcrop_2(img_tensor)\n",
    "    writer.add_image(\"RandomCrop2\", img_randomcrop, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23d833d8-6d2c-458a-ad2e-74446e08d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266ffa29-fe5c-411b-8842-7ecf181933f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关注输入输出\n",
    "# 多看官方文档\n",
    "\n",
    "# 不知道返回类型的时候\n",
    "# print()\n",
    "# print(type())\n",
    "# debug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
