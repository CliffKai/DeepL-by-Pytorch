import torch
import math
import torch.nn as nn

from torch import Tensor
from jaxtyping import Float, Bool, Int
from einops import einsum, rearrange

from ..nn_utils import softmax
from .Embedding import RotaryPositionalEmbedding
from .Linear import Linear

def scaled_dot_product_attention(
    q: Float[Tensor, "... quaries d_k"],
    k: Float[Tensor, "... key d_k"],
    v: Float[Tensor, "... value d_k"],
    mask: Bool[Tensor, "... quaries value"] | None=None,
) -> Float[Tensor, "... quaries d_k"]:
    d_k = q.size(-1)
    scores = einsum(q, k, "... q d, ... k d -> ... q k") / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == False, float("-inf"))
    attn_weights = softmax(scores, dim=-1)
    output = einsum(attn_weights, v, "... q k, ... k d -> ... q d")
    return output

# attention test code
'''
import torch
from torch import Tensor
from einops import einsum

from src.model.attention import scaled_dot_product_attention

# 1ï¸âƒ£ å›ºå®šéšæœºç§å­
torch.manual_seed(0)

# 2ï¸âƒ£ åŸºæœ¬è¶…å‚æ•°
batch_size = 2
seq_len_q = 3
seq_len_kv = 4
d_k = 8

# 3ï¸âƒ£ æ„é€  Q, K, Vï¼ˆå¯ä»¥å¸¦ batch ç»´åº¦ï¼‰
q = torch.randn(batch_size, seq_len_q, d_k, requires_grad=True)
k = torch.randn(batch_size, seq_len_kv, d_k, requires_grad=True)
v = torch.randn(batch_size, seq_len_kv, d_k, requires_grad=True)

# 4ï¸âƒ£ æ„é€  maskï¼šå…è®¸å‰ä¸¤ä¸ª query çœ‹åˆ°æ‰€æœ‰ keyï¼Œæœ€åä¸€ä¸ª query åªçœ‹å‰ 2 ä¸ª key
mask = torch.ones(batch_size, seq_len_q, seq_len_kv, dtype=torch.bool)
mask[:, -1, 2:] = 0  # æ¨¡æ‹Ÿä¸‹ä¸‰è§’æˆ–å±€éƒ¨æ³¨æ„åŠ›çš„æƒ…å†µ

print("Q shape:", q.shape)
print("K shape:", k.shape)
print("V shape:", v.shape)
print("Mask shape:", mask.shape)

# 5ï¸âƒ£ å‰å‘ä¼ æ’­
output = scaled_dot_product_attention(q, k, v, mask)
print("Output shape:", output.shape)
print("Output sample:\n", output[0, 0, :5])

# 6ï¸âƒ£ æ‰‹åŠ¨å®ç°å¯¹æ¯”ï¼ˆéªŒè¯ softmax & einsum é€»è¾‘ï¼‰
with torch.no_grad():
    d_k_sqrt = d_k ** 0.5
    scores_ref = einsum(q, k, "... q d_k, ... k d_k -> ... q k") / d_k_sqrt
    scores_ref = scores_ref.masked_fill(mask == False, float("-inf"))
    aw_ref = torch.softmax(scores_ref, dim=-1)
    output_ref = einsum(aw_ref, v, "... q k, ... k d_k -> ... q d_k")

print("\nAll close to manual reference?", torch.allclose(output, output_ref, atol=1e-6))

# 7ï¸âƒ£ æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
assert output.shape == (batch_size, seq_len_q, d_k), "âŒ è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…ï¼"
print("âœ… å½¢çŠ¶æ£€æŸ¥é€šè¿‡ã€‚")

# 8ï¸âƒ£ é›¶è¾“å…¥ç¨³å®šæ€§ï¼ˆåº”è¾“å‡ºå…¨ 0ï¼‰
with torch.no_grad():
    q_zero = torch.zeros_like(q)
    k_zero = torch.zeros_like(k)
    v_zero = torch.zeros_like(v)
    mask_all = torch.ones_like(mask, dtype=torch.bool)
    out_zero = scaled_dot_product_attention(q_zero, k_zero, v_zero, mask_all)
    assert torch.count_nonzero(out_zero) == 0, "âŒ é›¶è¾“å…¥æ—¶è¾“å‡ºåº”ä¸ºå…¨ 0"
print("âœ… é›¶è¾“å…¥ç¨³å®šæ€§é€šè¿‡ã€‚")

# 9ï¸âƒ£ åå‘ä¼ æ’­æ£€æŸ¥
loss = output.sum()
loss.backward()
assert q.grad is not None, "âŒ Q æ²¡æœ‰æ¢¯åº¦"
assert k.grad is not None, "âŒ K æ²¡æœ‰æ¢¯åº¦"
assert v.grad is not None, "âŒ V æ²¡æœ‰æ¢¯åº¦"
print("âœ… åå‘ä¼ æ’­æ£€æŸ¥é€šè¿‡ã€‚")

# 1ï¸âƒ£0ï¸âƒ£ CUDA åŠç²¾åº¦æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
if torch.cuda.is_available():
    print("\nğŸš€ åœ¨ CUDA + float16 ä¸‹æµ‹è¯•ï¼š")
    device = torch.device("cuda")
    q_half = q.detach().to(device).half().requires_grad_(True)
    k_half = k.detach().to(device).half().requires_grad_(True)
    v_half = v.detach().to(device).half().requires_grad_(True)
    mask_half = mask.to(device)
    out_half = scaled_dot_product_attention(q_half, k_half, v_half, mask_half)
    assert out_half.shape == (batch_size, seq_len_q, d_k)
    print("âœ… CUDA float16 å½¢çŠ¶æ£€æŸ¥é€šè¿‡ã€‚")
'''

class MultiHeadSelfAttention(nn.Module):
    def __init__(
        self,
        d_model: int,
        num_heads: int,
        rope: RotaryPositionalEmbedding,
        device: torch.device | str | None=None,
        dtype: torch.dtype | None=None,
    ):
        super().__init__()
        if d_model % num_heads != 0:
            raise ValueError("d_model must be divisible by num_heads")
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_head = d_model // num_heads
        self.rope = rope
        factory_kwargs = {"device": device, "dtype": dtype}

        self.q_proj = Linear(d_model, d_model, **factory_kwargs)
        self.k_proj = Linear(d_model, d_model, **factory_kwargs)
        self.v_proj = Linear(d_model, d_model, **factory_kwargs)
        self.output_proj = Linear(d_model, d_model, **factory_kwargs)

        self.register_buffer("causal_mask", None, persistent=False)
    
    def get_causal_mask(
        self,
        seq_len: int,
        device: torch.device | str | None=None,
    ) -> Bool[Tensor, "seq_len seq_len"]:
        if self.causal_mask is None or self.causal_mask.size(0) < seq_len:
            mask = torch.triu(torch.ones(seq_len, seq_len, device=device, dtype=torch.bool), diagonal=1)
            self.causal_mask = ~mask
        return self.causal_mask[:seq_len, :seq_len]
    
    def forward(
        self,
        x: Float[Tensor, "batch seq_len d_model"],
        token_positions: Int[Tensor, "batch seq_len"],
    ) -> Float[Tensor, "batch seq_len d_model"]:
        q = self.q_proj(x)
        k = self.k_proj(x)
        v = self.v_proj(x)
        q = rearrange(q, "b s (h d) -> b h s d", h = self.num_heads)
        k = rearrange(k, "b s (h d) -> b h s d", h = self.num_heads)
        v = rearrange(v, "b s (h d) -> b h s d", h = self.num_heads)
        q = self.rope(q, token_positions)
        k = self.rope(k, token_positions)
        causal_mask = self.get_causal_mask(seq_len=x.size(1), device=x.device)
        attn_output = scaled_dot_product_attention(q, k, v, mask=causal_mask)
        attn_output = rearrange(attn_output, "b h s d -> b s (h d)")
        return self.output_proj(attn_output)
    

# MHA test code
'''
import math
import torch
import torch.nn as nn
from einops import rearrange

from src.model.attention import MultiHeadSelfAttention, scaled_dot_product_attention
from src.model.Embedding import RotaryPositionalEmbedding, Embedding

torch.manual_seed(0)

def build_inputs(B=2, S=6, d_model=16, num_heads=4, device="cpu"):
    x = torch.randn(B, S, d_model, device=device, requires_grad=True)
    # ç¬¬äºŒä¸ª batch æ•…æ„å¹³ç§»ä½ç½®ï¼Œæµ‹è¯• RoPE çš„å¹¿æ’­
    pos = torch.arange(S, device=device).unsqueeze(0).repeat(B, 1)
    pos[1] += 2
    return x, pos

def test_mha_forward_and_shapes():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    B, S, d_model, H = 2, 6, 16, 4
    d_head = d_model // H

    # --- æ„é€  RoPEï¼ˆd_k = d_headï¼‰ ---
    rope = RotaryPositionalEmbedding(d_k=d_head, max_seq_len=1024, device=device)

    # --- æ„é€  MHA ---
    mha = MultiHeadSelfAttention(
        d_model=d_model, num_heads=H, rope=rope, device=device
    ).to(device)

    assert mha.d_head == d_head

    # --- è¾“å…¥ ---
    x, token_positions = build_inputs(B, S, d_model, H, device)

    # --- å‰å‘ ---
    y = mha(x, token_positions)
    print("Output shape:", y.shape)

    # å½¢çŠ¶æ£€æŸ¥
    assert y.shape == (B, S, d_model), "âŒ è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…ï¼"

    # åå‘ä¼ æ’­ï¼ˆæ£€æŸ¥å‚æ•°ä¸è¾“å…¥å‡æœ‰æ¢¯åº¦ï¼‰
    loss = y.sum()
    loss.backward()

    for name, p in mha.named_parameters():
        assert p.grad is not None, f"âŒ å‚æ•° {name} æ²¡æœ‰æ¢¯åº¦"
    assert x.grad is not None, "âŒ è¾“å…¥ x æ²¡æœ‰æ¢¯åº¦"

    print("âœ… å½¢çŠ¶ä¸åå‘ä¼ æ’­æ£€æŸ¥é€šè¿‡ã€‚")

def test_causal_mask_effect():
    """
    æ©ç æœ‰æ•ˆæ€§ï¼šéªŒè¯æ³¨æ„åŠ›åœ¨ä½ç½® t ä¸ä¼šå…³æ³¨åˆ°æœªæ¥ä½ç½® (>t)ã€‚
    åšæ³•ï¼š
      1) æ‰‹åŠ¨ç”¨ä¸ MHA ä¸€è‡´çš„ Q/K/V è®¡ç®— scoresï¼›
      2) åˆ†åˆ«ç”¨ â€œæ— æ©ç â€ ä¸ â€œå› æœæ©ç â€ å¾—åˆ°æ³¨æ„åŠ› awï¼›
      3) æ–­è¨€åœ¨ masked æƒ…å†µä¸‹ aw[..., t, t+1:] â‰ˆ 0ã€‚
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    B, S, d_model, H = 2, 6, 16, 4
    d_head = d_model // H

    rope = RotaryPositionalEmbedding(d_k=d_head, max_seq_len=1024, device=device)
    mha = MultiHeadSelfAttention(d_model=d_model, num_heads=H, rope=rope, device=device).to(device)

    x, token_positions = build_inputs(B, S, d_model, H, device)
    x = x.detach().requires_grad_(True)

    # --- ä¸ forward ä¸­ä¸€è‡´çš„æŠ•å½±ä¸é‡æ’ ---
    q = mha.q_proj(x)  # (B,S,d_model)
    k = mha.k_proj(x)
    v = mha.v_proj(x)

    q = rearrange(q, "b s (h d) -> b h s d", h=H)
    k = rearrange(k, "b s (h d) -> b h s d", h=H)
    v = rearrange(v, "b s (h d) -> b h s d", h=H)

    # RoPE åªä½œç”¨ Q/K
    q = mha.rope(q, token_positions)
    k = mha.rope(k, token_positions)

    # --- è®¡ç®— unmasked æ³¨æ„åŠ›æƒé‡ ---
    # å¤åˆ¶ä½ å®ç°é‡Œçš„é€»è¾‘ï¼šscores = (q @ k^T) / sqrt(d_k)ï¼›softmax(-1)
    scores_un = torch.einsum("... q d, ... k d -> ... q k", q, k) / math.sqrt(d_head)
    aw_un = torch.softmax(scores_un, dim=-1)  # (B,H,S,S)

    # --- æ„é€ å› æœæ©ç ï¼ˆä¸‹ä¸‰è§’ True=å¯è§ï¼‰ï¼Œå¹¶åº”ç”¨ ---
    causal_mask = mha.get_causal_mask(seq_len=S, device=device)   # (S,S) bool
    # ä½ çš„ scaled_dot_product_attention å®šä¹‰é‡Œï¼šmask==False çš„ä½ç½®ä¼šè¢«è®¾ä¸º -inf
    scores_ma = scores_un.masked_fill(causal_mask == False, float("-inf"))
    aw_ma = torch.softmax(scores_ma, dim=-1)

    # --- æ–­è¨€æœªæ¥æ³¨æ„åŠ›ä¸º ~0 ---
    # éšæœºæŠ½å‡ è¡Œæ£€æŸ¥ï¼šå¯¹æ¯ä¸ª tï¼Œaw_ma[..., t, t+1:] åº”æ¥è¿‘ 0
    with torch.no_grad():
        upper = torch.triu(torch.ones(S, S, device=device, dtype=torch.bool), diagonal=1)  # ä¸Šä¸‰è§’ True=æœªæ¥
        leaked = aw_ma.masked_select(upper.expand(B, H, -1, -1)).abs().max().item()
        print("Max attention on future positions (should be ~0):", leaked)
        assert leaked < 1e-6, "âŒ å› æœæ©ç å¤±æ•ˆï¼šä»ç„¶å…³æ³¨äº†æœªæ¥ä½ç½®"

    print("âœ… å› æœæ©ç æœ‰æ•ˆæ€§é€šè¿‡ã€‚")

def test_cuda_half_optional():
    """
    å¯é€‰ï¼šåœ¨ CUDA ä¸Šæ£€æŸ¥ float16ã€‚è‹¥æ—  CUDA åˆ™è·³è¿‡ã€‚
    """
    if not torch.cuda.is_available():
        print("(è·³è¿‡ CUDA/float16 æµ‹è¯•ï¼šCUDA ä¸å¯ç”¨)")
        return

    device = torch.device("cuda")
    B, S, d_model, H = 2, 8, 32, 4
    d_head = d_model // H

    rope = RotaryPositionalEmbedding(d_k=d_head, max_seq_len=1024, device=device).to(device)
    mha = MultiHeadSelfAttention(d_model=d_model, num_heads=H, rope=rope, device=device).to(device)

    x = torch.randn(B, S, d_model, device=device, dtype=torch.float16, requires_grad=True)
    pos = torch.arange(S, device=device).unsqueeze(0).repeat(B, 1)

    y = mha(x, pos)
    assert y.shape == (B, S, d_model)
    # å…è®¸åŠç²¾åº¦æœ‰æ›´æ¾çš„ allclose æ ‡å‡†ï¼Œè¿™é‡Œåªæ£€æŸ¥æ˜¯å¦èƒ½è·‘é€šä¸åå‘
    y.sum().backward()
    assert x.grad is not None
    print("âœ… CUDA float16 å‰å‘/åå‘é€šè¿‡ã€‚")

if __name__ == "__main__":
    test_mha_forward_and_shapes()
    test_causal_mask_effect()
    test_cuda_half_optional()
    print("\nğŸ‰ All MultiHeadSelfAttention tests finished.")
'''