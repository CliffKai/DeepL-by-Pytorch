
整篇论文的核心工作流程如下所示，主要分为三个支柱：**记忆存储**、**记忆检索**和**记忆更新** 。

### 1. 记忆预处理与存储 (Memory Storage)

在对话开始之前或过程中，系统会将所有的交互信息存入“仓库” ：

* **多层级存储**：不仅存储原始对话，还会利用 LLM 将长对话压缩成“每日事件摘要”和“全局总结” ；

* **动态画像建模**：模型会分析对话，推断出用户的性格特征和情绪，生成“用户画像”并持续更新 ；

* **向量化索引**：所有记忆片段（原始对话、摘要、画像）都会通过编码器转换为向量，并使用 **FAISS** 进行索引，以便后续快速搜索 ；



### 2. 实时检索与提示词增强 (Memory Retrieval)

当用户发送一条新消息（Query）时，流程如下 ：

* **语义搜索**：系统将用户的当前输入转化为向量，在记忆库中检索出语义最相关的历史片段 。

* **构建增强 Prompt**：系统会将以下四类信息拼装成一个超强的上下文（Memory Augmented Prompt）喂给 LLM ：

1. **相关记忆**：搜索出来的历史细节 ；

2. **用户画像**：关于用户性格的全局描述 ；

3. **事件摘要**：过去的互动背景大纲 ；

4. **当前对话**：当下的聊天内容 ；

### 3. 回复生成与“拟人化”更新 (Memory Updating)

在生成回复后，系统会进行记忆库的维护，这是该论文最核心的“拟人”环节 ：

* **模拟艾宾浩斯遗忘曲线**：系统根据公式 $R=e^{-\frac{t}{s}}$ 计算每条记忆的留存率 ；

* **（流逝时间）**：自该信息产生以来经过的时间 ；

* **（记忆强度）**：初始为 1 ；

* **强化与衰减**：

* **强化**：如果某条记忆被成功检索并使用，系统会将 $S$ 增加 1，并将 $t$ 重置为 0，使其更难被遗忘；

* **遗忘**：长时间未被提及且重要性低的记忆，其留存率会持续下降，直到被系统“选择性遗忘” ；
