传统多模态学习论文串讲：
- 图文检索（Image Text Retrieval）
- VQA（视觉问答）
- Visual Reasoning（视觉推理）
- Visual Entailment（视觉蕴含）

# 第一部分：只用 Transformer Encoder 的一些方法

![Figure1](../images/MML论文串讲_Figure1.png)

CLIP模型也是非常简单的一个结构，它是一个典型的双塔模型，他有两个Model，一个对应文本，一个对应视觉，在训练的时候通过对比学习，让已有的图像文本对在空间上拉的更近，然而让不是一个对的图片文本就拉的尽量更远，从而最后学到了非常好的图像文本特征，然后一旦学到很好的图像文本特征之后CLIP只需要做这种很简单的点乘，就能去做多模态任务，尤其是对那种图像文本匹配，或者图像文本检索的这种任务来说，CLIP简直就是神一样的存在，因为它不光效果好，而且很高效，因为往往你去做这种图像文本匹配或者图像文本检索任务的时候，你是有一个很大的已有的数据库的，这个时候如果你新来一张图片，或者新来一个文本，你要去和已有的数据库去做匹配，那其他所有的方法都会非常的慢，因为其他所有的数据都要过一遍编码器，但是CLIP模型就不用，他可以提前把那个数据库里所有的图像文本的特征提前都抽好，而且是想什么时候抽就什么时候抽，抽好放在那就行，等你真正想用的时候直接就做一个点乘就好了。

> 解释：为什么CLIP就可以离线抽取特征，而其他的（比如BLIP、ALBEF、UNITER、ViLT等）就不可以？

> 首先我们看Figure1，基本上绝大部分多模态的模型都是使用这种前向的方式进行推理的，也就是说 Text 先经过一个 Textual Encoder，Image 也先经过一个 Visual Encoder，然后两者经过 Encoder 的输出一起再送入一个 Modality 的 Cross-Encoder。     
CLIP 的特殊之处在于它没有传统意义上的 Cross-Modal Encoder。图像和文本只分别经过各自的 Encoder，输出的向量直接用于计算相似度（如点乘或余弦相似度），这一相似度计算是无参的，仅仅是一个数学操作，不涉及模型推理。所以任何新加入检索任务的数据库的数据只需要经过 Encoder 一次将特征存储起来，要进行对比的时候点乘一下即可。      
而其他模型不行，其他的模型需要同时进过两个前面的 Encoder 后再一起经过一个 Cross-Encoder，而这个过程就很麻烦了，因为同样的 Image 搭配不同的 Text 的输出肯定是不同的，而每一次前向的过程有很费时间和算力，举个例子，假设数据库中原有 1000 条文本和 500 张图片，CLIP 只需要对每个样本编码一次即可得到向量。如果再加入 1000 条新文本和 500 张新图片，只需进行 1500 次 encoder 前向推理。最终只需对所有 2000 个文本向量和 1000 个图像向量计算点乘，开销是 O(N+M)，计算效率非常高；而传统 Cross-Encoder 结构中，每一条文本都需要与所有图像组合编码，例如加入一条新文本就要与 1000 张图像分别组合输入模型做一次推理，计算复杂度是 O(N×M)，所以很难扩展。CLIP 新加入数据的计算量只是相加，而其他模型是直接相乘。

CLIP 做图文匹配效果很好，但是其他VQA、VR、VE等任务就不太行了，因为在模态融合部分只是一个简单的点乘无法涵盖模态融合中比较复杂的情况。

模型：现在多模态领域的共识是想要做一个好的模型视觉部分和模态融合部分一定要够大，也就是 ViT 部分和后面的 Cross-Encoder 要大。      
训练：有了模型后如何训练？CLIP 只用了一个对比学习的 Loss（ITC Loss：Image Text Contrastive Loss）效果就已经很好了，ITC Loss 的效果是很不错的，而且训练也很高效。

## 1.1 ALBEF



# 第二部分：Transformer Encoder 和 Decoder 一起用的一些方法


