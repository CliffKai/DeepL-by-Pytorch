{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 环境需求\n",
    "\n",
    "transformer\n",
    "qwen-vl-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "MODEL_DIR = r\"D:\\Models\\Qwen\\Qwen2.5-VL-3B-Instruct\" # 模型路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 加载模型\n",
    "\n",
    "## 2.1 准备模型\n",
    "\n",
    "可以讲模型下载到本地，也可以直接从 Hugging Face 上加载。\n",
    "\n",
    "```python\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "这样会自动下载模型到本地缓存目录（通常是 `~/.cache/huggingface/transformers`，不做特殊设置在 C 盘，不推荐）。\n",
    "\n",
    "如果想指定模型存放目录，可以先把模型下载到本地，然后再从本地加载：\n",
    "\n",
    "```python\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    r\"D:\\Models\\Qwen\\Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa85f115ca3e476ea986b0df3772aaf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_DIR, dtype=\"auto\", device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 参数设置\n",
    "\n",
    "```python\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    pretrained_model_name_or_path,  # 模型名称或路径\n",
    "    cache_dir=None,                 # 模型缓存目录\n",
    "    revision=None,                  # 模型版本（分支/tag/commit）\n",
    "    torch_dtype=None,               # 加载时的精度\n",
    "    device_map=None,                # 模型放在哪些设备上\n",
    "    low_cpu_mem_usage=False,        # 大模型加载时常用，减少 CPU 内存占用\n",
    "    local_files_only=False,         # 只用本地文件，不联网下载\n",
    ")\n",
    "```\n",
    "\n",
    "1. **`pretrained_model_name_or_path`**\n",
    "\n",
    "   * 必填，模型名称或路径。\n",
    "   * 例子：`\"bert-base-uncased\"` 或 `\"./my_model_checkpoint/\"`\n",
    "\n",
    "2. **`cache_dir`**\n",
    "\n",
    "   * 设置下载模型的缓存路径。\n",
    "   * 避免每次都从 Hugging Face Hub 下载。\n",
    "\n",
    "3. **`revision`**\n",
    "\n",
    "   * 指定模型的版本（分支/tag/commit）。\n",
    "   * 例子：`revision=\"v1.0.0\"`\n",
    "\n",
    "4. **`torch_dtype`**\n",
    "\n",
    "   * 指定加载时的精度。\n",
    "   * 常用：`torch.float16`（省显存）、`\"auto\"`（自动选择）。\n",
    "\n",
    "5. **`device_map`**\n",
    "\n",
    "   * 指定模型放在哪些设备上。\n",
    "   * 常用：`\"auto\"`（自动把大模型切分到多个 GPU/CPU）。\n",
    "\n",
    "6. **`low_cpu_mem_usage`**\n",
    "\n",
    "   * 大模型加载时常用，减少 CPU 内存占用。\n",
    "   * 常用：`True`\n",
    "\n",
    "7. **`local_files_only`**\n",
    "\n",
    "   * 只用本地文件，不联网下载。\n",
    "   * 常用：`True`（离线环境）\n",
    "\n",
    "对于 Qwen2.5vl，官方推荐如果做多图或者视频等长序列任务，可以使用`attn_implementation=\"flash_attention_2\"`将注意力机制替换为 Flash Attention 2，推理更快、更省显存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 准备 processor\n",
    "\n",
    "```python\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "```\n",
    "\n",
    "在使用 Hugging Face 加载模型时，需要同时加载对应的 processor。processor 负责将输入数据（文本、图像等）转换为模型可以处理的格式。\n",
    "\n",
    "Qwen官方提供了一个能够同时处理文本、图像和视频的 processor。\n",
    "\n",
    "同理，如果已经下好了模型可以直接加载本地的：\n",
    "\n",
    "```python\n",
    "processor = AutoProcessor.from_pretrained(r\"D:\\Models\\Qwen\\Qwen2.5-VL-3B-Instruct\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 准备数据\n",
    "\n",
    "## 3.1 加载数据\n",
    "\n",
    "构建一个列表，格式为：\n",
    "```python\n",
    "messages = [\n",
    "    {...},   # 第一轮消息\n",
    "    {...},   # 第二轮消息\n",
    "    {...}    # 第三轮消息\n",
    "]\n",
    "```\n",
    "每一轮消息是一个字典，一般包含：\n",
    "- `\"role\"`：角色，通常是 `\"user\"`（用户）、 `\"assistant\"`（助手）、 `\"system\"`（系统），其中：\n",
    "    - `\"user\"`：用户输入\n",
    "    - `\"assistant\"`：模型输出\n",
    "    - `\"system\"`：系统提示，通常在对话开始时使用，提供背景信息或指导\n",
    "- `\"content\"`：消息内容，可以是文本、图像或视频等，context 也是一个列表，格式为：\n",
    "```python\n",
    "\"context\": [\n",
    "    {\"type\": \"text\", \"text\": \"文本内容\"},\n",
    "    {\"type\": \"image\", \"image\": image},  # image 是 PIL.Image 对象\n",
    "    {\"type\": \"video\", \"video\": video_path}  # video_path 是视频文件路径\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"images/demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"What is on the left side of the picture?\"},\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 处理数据\n",
    "\n",
    "加载好的数据不能直接给模型，都是要经过处理的。\n",
    "\n",
    "1. 文本处理：\n",
    "    ```python\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,                   # 是否进行分词，处理为 token id，这里不需要，因为后面有 processor(...) 会统一进行处理\n",
    "        add_generation_prompt=True        # 是否在最后加上助手的提示符（类似于下面格式中的 <|assistant|>），告诉模型现在该它说话了\n",
    "    )\n",
    "    ```\n",
    "    apply_chat_template 会将 messages 中的文本进行处理为模型所需的聊天格式，一般来讲模型都会有聊天格式，类似这样：\n",
    "    ```\n",
    "    <|system|>\n",
    "    You are a helpful assistant.\n",
    "\n",
    "    <|user|>\n",
    "    Describe this image.\n",
    "\n",
    "    <|assistant|>\n",
    "    ```\n",
    "    这样有两个好处：\n",
    "    1. 让模型更好地理解对话的结构和角色\n",
    "    2. 如果有多轮对话，它会按顺序拼好，保持上下文。\n",
    "\n",
    "\n",
    "2. 图像和视频处理：\n",
    "    ```python\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    ```\n",
    "    从 messages 里把视觉模态的内容（图片、视频）抽取出来，并转成后续 processor(...) 可以直接处理的标准格式。\n",
    "\n",
    "\n",
    "3. 拼接\n",
    "\n",
    "```python\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,             # 是否补 <pad> token，对齐到同一长度\n",
    "    return_tensors=\"pt\",      # 返回 PyTorch 张量，\"tf\" 返回 TensorFlow 张量，\"np\" 返回 NumPy 数组\n",
    ")\n",
    "```\n",
    "这样 inputs 就是一个字典，里面包含了模型需要的所有输入。\n",
    "\n",
    "\n",
    "4. 移动到设备\n",
    "\n",
    "```python\n",
    "inputs = inputs.to(\"cuda\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 推理\n",
    "\n",
    "## 4.1 生成\n",
    "\n",
    "```python\n",
    "generated_ids = model.generate(\n",
    "    **inputs,                   # 解包 inputs 字典，传给模型\n",
    "    max_new_tokens=128          # 生成的最大长度\n",
    "    )\n",
    "```\n",
    "这一步会把预处理好的 inputs 喂进模型，模型会进入解码循环（generation loop），生成新的 token id，直到达到 `max_new_tokens` 或者遇到结束符，返回生成的 token 序列。\n",
    "\n",
    "## 4.2 解码\n",
    "\n",
    "```python\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "```\n",
    "这一步是把生成的 token id 序列进行裁剪，去掉输入部分，只保留模型新生成的部分。\n",
    "\n",
    "## 4.3 转文本\n",
    "\n",
    "```python\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, \n",
    "    skip_special_tokens=True,                 # 是否去除特殊符号，如 <pad>、<eos> 等\n",
    "    clean_up_tokenization_spaces=False        # 是否清理多余的空格\n",
    ")\n",
    "```\n",
    "这一步是把生成的 token id 转回文本字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 输出\n",
    "\n",
    "```python\n",
    "print(output_text)\n",
    "```\n",
    "打印模型输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On the left side of the picture, there is an ocean with waves crashing onto the shore. The sky above the ocean appears to be clear and bright, suggesting it might be a sunny day.']\n"
     ]
    }
   ],
   "source": [
    "print(output_text)\n",
    "\n",
    "# 这部分不用管，看完后面多轮对话后再回来看\n",
    "messages.append({\n",
    "    \"role\": \"assistant\", \n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"text\", \n",
    "            \"text\": output_text\n",
    "        }\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 拓展\n",
    "\n",
    "## 6.1 生成参数值\n",
    "\n",
    "```python\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,   # 设置想要的温度\n",
    "    top_p=0.9          # 可选，常用组合\n",
    ")\n",
    "```\n",
    "可以在 generate 时设置 temperature 和 top_p。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 多轮对话\n",
    "\n",
    "多轮对话的核心就是每一轮对话始终带上历史 messages，方法就是每一次新对话都将消息`append`进 messages 中即可。\n",
    "\n",
    "这里需要说明一点，因为我们是使用`append`方法来加入新消息的，所以第一轮传入的图片模型第二轮第三轮等后面都能看见，如果是对同一张图片无需传多次。\n",
    "\n",
    "后面继续前面的操作即可。\n",
    "\n",
    "当模型回答完后也要将模型的回答`append`到 messages 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[\"[\\'On the right side of the picture, there is a woman sitting on the beach with her dog. She is smiling and appears to be interacting with the dog, who is also sitting on the sand and looking at her.\\']\"]']\n"
     ]
    }
   ],
   "source": [
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"text\", \n",
    "            \"text\": \"What are on the right and in the middle of the picture?\"\n",
    "        }\n",
    "    ],\n",
    "})\n",
    "\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,   # 设置想要的温度\n",
    "    top_p=0.9          # 可选，常用组合\n",
    ")\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "print(output_text)\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"assistant\", \n",
    "    \"content\": [\n",
    "        {\n",
    "            \"type\": \"text\", \n",
    "            \"text\": output_text\n",
    "        }\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch(torch==2.6.0)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
