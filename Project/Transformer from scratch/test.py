import torch
from src.model.FFN import SwiGLU   # å‡è®¾æ–‡ä»¶è·¯å¾„ä¸º model/SwiGLU.py
from src.nn_utils import silu   # è‹¥ nn_utils.py åœ¨åŒçº§ç›®å½•ä¸‹

# 1ï¸âƒ£ å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
torch.manual_seed(0)

# 2ï¸âƒ£ è¶…å‚æ•°å®šä¹‰
batch_size = 2
seq_len = 4
d_model = 8
d_ff = None   # æµ‹è¯•è‡ªåŠ¨æ¨å¯¼åˆ†æ”¯

# 3ï¸âƒ£ æ„é€ å±‚ä¸è¾“å…¥ï¼ˆå¯é€‰æµ‹è¯• CUDA + float16ï¼‰
layer = SwiGLU(d_model=d_model, d_ff=d_ff)

x = torch.randn(batch_size, seq_len, d_model, requires_grad=True)
print("Input x shape:", x.shape)

# ğŸ§ª GPU æµ‹è¯•ï¼ˆè‹¥å¯ç”¨ï¼‰
if torch.cuda.is_available():
    device = torch.device("cuda")
    layer = layer.to(device)
    x = x.to(device).half().detach().requires_grad_(True)
    print("Running on CUDA with float16")

# 4ï¸âƒ£ å‰å‘è®¡ç®—
y = layer(x)
print("Output y shape:", y.shape)

# 5ï¸âƒ£ æ‰‹å·¥éªŒè¯é€»è¾‘ï¼š
# gate = w1(x)
# hidden = silu(w2(x))
# y_ref = w3(gate * hidden)
with torch.no_grad():
    x32 = x.float()
    gate = layer.w1(x32)
    hidden = silu(layer.w2(x32))
    y_ref32 = layer.w3(gate * hidden)
    y_ref = y_ref32.to(y.dtype)

print("All close to manual reference?",
      torch.allclose(y, y_ref, atol=1e-5 if y.dtype.is_floating_point else 1e-3))

# 6ï¸âƒ£ å½¢çŠ¶æ–­è¨€
assert y.shape == (batch_size, seq_len, d_model), "âŒ è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…ï¼"
print("âœ… å½¢çŠ¶æ£€æŸ¥é€šè¿‡ã€‚")

# 7ï¸âƒ£ åå‘ä¼ æ’­æ£€æŸ¥
loss = y.sum()
loss.backward()

assert x.grad is not None, "âŒ è¾“å…¥æ²¡æœ‰æ¢¯åº¦"
assert layer.w1.weight.grad is not None, "âŒ w1 æƒé‡æ²¡æœ‰æ¢¯åº¦"
assert layer.w2.weight.grad is not None, "âŒ w2 æƒé‡æ²¡æœ‰æ¢¯åº¦"
assert layer.w3.weight.grad is not None, "âŒ w3 æƒé‡æ²¡æœ‰æ¢¯åº¦"
print("âœ… åå‘ä¼ æ’­æ£€æŸ¥é€šè¿‡ã€‚")

# 8ï¸âƒ£ æ•°å€¼ç¨³å®šæ€§ï¼šé›¶è¾“å…¥åº”è¾“å‡ºé›¶ï¼ˆå› ä¸º gate*hidden=0ï¼‰
with torch.no_grad():
    x_zero = torch.zeros_like(x)
    y_zero = layer(x_zero)
    assert torch.count_nonzero(y_zero) == 0, "âŒ é›¶è¾“å…¥æ—¶è¾“å‡ºåº”å…¨ 0"
print("âœ… é›¶è¾“å…¥ç¨³å®šæ€§é€šè¿‡ã€‚")

# 9ï¸âƒ£ è‡ªåŠ¨æ¨å¯¼çš„ d_ff æ£€æŸ¥
expected_d_ff = int((8/3) * d_model)
expected_d_ff = (expected_d_ff + 63) // 64 * 64
print(f"Expected d_ff={expected_d_ff}, actual w1.out_features={layer.w1.out_features}")
assert layer.w1.out_features == expected_d_ff, "âŒ d_ff è‡ªåŠ¨æ¨å¯¼ä¸æ­£ç¡®"
print("âœ… è‡ªåŠ¨æ¨å¯¼æ£€æŸ¥é€šè¿‡ã€‚")