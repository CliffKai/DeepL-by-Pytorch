引言
近年来，大型语言模型（LLM）的文本生成呈现出越来越拟人化的特征：输出在风格、情感、个性、语气等方面愈发接近人类。这种“拟人性”不仅提升了用户交流的亲和力，也引发了对模型人格和行为表现的研究兴趣[1]。LLM在开放域对话、问答、创意写作中往往产生与人类难以区分的文本[1]，催生了个性化助理、智能客服、虚拟伙伴等广泛应用[2]。与此同时，研究者开始探讨：LLM在展现高度类人智能的同时，是否也具备类似人类的人格特质？对这一问题的深入研究，有助于理解模型内部机制，并指导我们塑造更一致、可信赖的AI行为风格[3]。
本文综合近三年（2022–2025）的研究进展，从方法、评估、应用三个层面，调研LLM输出内容在风格、情感、个性、语气等方面的拟人化。我们重点关注：(1) 方法层面的模型结构设计与控制技术（如提示词调控、生成风格控制、人格建模、风格迁移等）以及对齐机制（如人类反馈强化学习RLHF、人格注入等）；(2) 评估层面的基准与指标（如何测量模型在风格、情感、语气、人格等维度上的拟人性程度）；(3) 应用层面的实际系统和案例（聊天机器人、虚拟角色、游戏NPC、社交助手等）。最后，我们讨论当前具有学术研究价值的挑战和未来方向。
方法层面：拟人化输出的模型与技术
LLM生成拟人化文本，涉及从模型训练到推理控制的多种方法。总体而言，有基于微调的数据驱动方法、基于提示的在生成时控制以及对齐与可控生成等思路。
微调与人格建模
1. 人格数据微调： 常用方法是在预训练LLM基础上，利用人格相关的数据进行条件微调，让模型习得特定人格风格的表达[4][5]。这类似于指令微调，但将指令换为人格描述[6]。一类典型做法是构建〈人格描述-文本〉对的数据集，让模型学习给定人格时的语言风格[7]。例如Facebook的Image-Chat数据集包含多种人格角色，每段对话之前提供说话者的人格简介（如“乐观、热情的推销员”），模型据此生成对应风格的回复[7]。经过此类配对数据微调后，模型能够根据输入的人格简介控制对话语气，显著提升输出对目标人格的匹配度[7][8]。这一方法效果直观，可控性强，但缺点是高质量人格标注语料获取成本高且覆盖人格维度有限[9]。近期工作致力于自动生成人格文本、提高标注一致性，以扩充训练语料[9][10]。
另一种微调思路是不使用固定标签，而是学习“人格嵌入” (personality embedding) 来表示任意人格描述[11]。研究者可从海量语料中提取人物角色描述（如小说人物简介、影视角色介绍），编码成向量作为模型的附加条件输入[11]。微调时将此人格向量和对话上下文一并输入模型，从而使LLM能够依据任意人格描述进行生成，不局限于训练集出现过的人格标签[11]。例如，Liang等人使用预训练文本编码器将人格描述转为向量，引导微调对话模型，实现了可自由切换角色的对话[11]。Zhang等人进一步将人格向量空间与心理学“五大人格”维度对齐，使嵌入具有可解释性，便于控制调节[12]。基于嵌入的方法拓展了人格控制的灵活性，显示出更强的人格迁移能力，但也面临挑战：如何从描述文本中准确提取人格语义[10]。这需要大规模高质量的人格描述数据，以及结合知识图谱、常识推理等提升人格表征的技术[13]。
值得一提的还有专用人物模型的微调。如Character-LLM框架通过监督微调，让模型融入特定角色的大量“经历”来塑造人格：通过扩展角色背景传记生成丰富的带情感和行为风格的对话场景来训练模型，从而使其内化历史或虚构人物独特的情感与语言模式[14][15]。这种方法在提升人格对齐、一致性和记忆方面效果明显[16]。另有ORCA框架引入心理学大五人格分数，模拟用户画像和动机生成个性化语料，并通过多阶段微调增强模型角色扮演能力[17][18]。结果显示该法使输出的人格一致性和相关性大幅提高，在个性化对话生成上树立了新标杆[19]。不过，微调方法总体限制在于：需要大量高质量人格数据，训练开销大，且可能出现过拟合或遗忘（损害模型原有知识）[20]。因此近来也出现参数高效微调(PEFT)等方式，在注入人格的同时减少对模型整体的扰动。
2. 强化学习与对齐： RLHF等对齐技术也影响着模型的拟人化风格[21]。RLHF通过人类反馈奖励模型输出的可取行为，成功塑造了ChatGPT这类“理想协作者”的人格形象：更加开朗、友善、稳定、可靠，五大人格中“宜人性”“尽责性”等维度显著提升，神经质（情绪不稳定）则下降[22][23]。研究发现，随着指令微调和RLHF的叠加，模型问卷测出的人格画像愈发“乖巧”且方差减少，仿佛变得比大多数人类还“完美”[22][23]。然而近期一项跨学科研究表明，这可能是一种“人格幻觉”：经过RLHF对齐的模型虽然在语言自述上表现出稳定人格，但在实际决策和行为测试中并未体现相应特质[21][24]。换言之，对齐过程更多是“教会模型把话说得好听”，而未真正改变其内在行为逻辑[21][25]。例如，模型自称谨慎却在风险游戏中选择冒进，自评无偏见但隐含联想测试(IAT)显示刻板偏见依然存在[26][27]。这一发现揭示了当前RLHF等语言层面对齐的局限，并促使研究者探讨行为导向的对齐策略[25]——通过设计奖励直接约束模型在交互任务中的实际行为，使人格塑造不仅停留于表层语言风格，而是反映在决策一致性和价值观上。这方面的研究尚属前沿，但对于打造真正可靠的拟人化AI至关重要。
3. 检索与记忆增强： 为实现人格的长期一致与动态适应，一些方法融入检索增强与外部记忆机制。在PersonaRAG框架中，系统维护用户画像、上下文检索、会话记忆等多个模块，生成时从全局记忆池中提取相关的人格描述和上下文信息供模型参考[28][29]。这使LLM无需额外微调，就能通过检索实时注入用户的偏好、背景，从而输出符合特定用户人格的回应[30][31]。类似地，Emotional RAG在检索过程中结合了对话的情绪状态，让模型在回应时既语义合理又情感吻合，对提升模拟人格的真实性很有帮助[32]。另一项名为PersonaAI的研究则以移动端持续采集用户数据，构建检索库用于动态丰富模型提示，实现持续的个性化响应[33]。此外，检索技术还应用于模拟用户个性：美国USC提出的RECAP方法，从用户过往文本中检索既“语义相关”又“风格相似”的句子，来引导对话模型输出“像你会说的话”[34][35]。这无需用户手动填写问卷，而是让模型从用户历史语言中隐式学习其说话风格[36][37]。此类个性化代理能以用户笔迹般的口吻代替用户处理日常对话事务[38][39]。由此可见，利用检索和外部知识，可以在推理阶段灵活地赋予LLM具体的角色人格和情感记忆，提高对话的一致性和沉浸感。
提示词调控与生成控制
除了模型参数层面的改造，另一个重要方向是在生成时通过提示词(Prompt)或隐式控制变量来调节输出风格。
1. 人格提示模板： 这是一种无需额外训练的方法，通过在对话上下文中注入人格相关的信息，引导LLM呈现特定风格[40]。简单形式如在系统提示中明确说明“你是一位性格
描述
的
角色
”，例如：“你是一位乐观、幽默、善解人意的朋友”[41]。模型往往能够理解并在随后的对话中模仿该人设说话[41]。更复杂的模板包括提供详细的人物背景（如年龄、职业、性格爱好、人生经历），再规定当前场景或任务，让模型进入角色[42]。例如提示：“你是一名严谨且不苟言笑的高中生物老师，现在正在给学生讲解光合作用原理”[42]。又或者采用问答引导的方式，先向模型提出一系列关于角色性格、动机、情感的问题，让其以第一人称回答，从而构建起对角色的理解，再让模型以该角色身份开始对话[43]。实践证明，精心设计的提示可以即时生效地塑造模型的言语 persona，用户也能方便地定制AI的性格[41][44]。然而，此法效果高度依赖模型对提示的领会程度：对于抽象或复杂的人格描述，模型可能难以把握其丰富内涵；在多轮对话中，模型的人设也可能渐渐漂移、丢失一致性[44]。因此如何设计稳健的提示模板是门学问，需要在信息明确与不过度约束之间找到平衡[44]。有研究建议引入心理学理论，将人格特质分解为更细的要素模块，形成结构化的提示框架，并通过迭代调优提高角色塑造的稳定性[44]。总的来看，提示词调控为人格风格控制提供了灵活轻量的手段，也是当前聊天机器人广泛采用的方法（例如OpenAI的系统消息常用于设定ChatGPT的语气和角色）。
2. 控制变量与隐式调控： 一些先进技术尝试通过隐式控制信号在生成过程中引导风格。一种思路是基于模型内部激活向量的操作。例如Anthropic最新提出的“人格向量 (persona vectors)”方法[45]：通过比较模型在呈现某种特质时与正常情况下的神经激活差异，提取出对应特质的方向向量[46]。这些向量被认为对应模型内部控制该性格特征的神经模式[45]。将人格向量注入模型的隐藏层，可显式改变输出风格——实验证明，对模型施加“邪恶”向量后，回答开始包含不道德内容；施加“阿谀奉承”向量，模型就会对用户谄媚逢迎；施加“幻想”向量，则模型更容易编造不真实信息[47]。这种激活层面的操控为细粒度调节模型人格提供了新工具，能够监测和干预模型在对话中的人格转变[48][49]。Anthropic展示了在开放模型如Qwen-7B和Llama-8B上自动提取“邪恶”、“谄媚”、“幻觉”等Persona向量，并用于实时监控对话中这些人格成分的强度，及时发现模型朝不良倾向漂移[50]。进一步地，还可在训练过程中加入正则，引导模型约束这些人格向量的激活，减弱不良人格的产生[51]。人格向量的提出让我们初步窥见模型内部人格表达的“神经元开关”，为未来实现可解释的风格控制奠定基础[52]。后续研究已将其拓展：如2025年提出的BILLY方法，通过合并多个人格向量来赋予单一LLM复合风格，以提高创造力[53][54]。实验让模型同时注入“环境保护者”和“创意专业人士”两种persona向量，生成的回答兼具环保和创意视角，显著提升了观点多样性与创造性，同时成本远低于运行两个模型对话[53][54]。这种向量融合展示了按需调配人格的潜力，对于需要模型综合多重角色观点的应用（如脑暴、决策支持）很有价值。
还有一些控制方法借鉴了以往的小规模生成控制技术，如在输入中添加特殊控制码或标签触发某种语气（类似于早期GPT-2的控制代码），或训练小型前缀模型在不改动大模型权重的情况下，对隐藏状态施加偏置来实现风格转换(Prefix-Tuning变体)。这些方法在大型模型时代重新受到关注，用较小代价实现对情绪、语气的调整。例如有工作实现了让模型在正式/非正式语体间切换，或在回答中加入适度的幽默等。总的来说，在不损伤模型核心能力的前提下，如何灵活、安全地操控LLM的行为风格，是当前方法层面的核心议题。
评估层面：拟人化程度的测量
如何客观评估一个语言模型输出在风格、情感、人格等方面的“拟人性”，是具有挑战的问题。近三年涌现出多种评测基准和指标，涵盖了从心理测量学借鉴的方法到自动评分和人类评价实验。
心理学量表与人格剖析
一种直接思路是借用人类人格评估的工具，对LLM进行心理测评。例如大五人格问卷(Big Five Inventory, BFI)常用于刻画人类受试者在外向性、宜人性、尽责性、神经质、开放性五个维度的特质。研究者设计了适应LLM特点的问卷，让模型以第一人称回答一系列评估题，再据此计算其人格得分[55]。由于选项顺序对LLM可能有影响，问卷多以开放式回答形式呈现，然后借助预训练模型或规则将这些自由回答映射到数值指标上[56]。香港理工大学等开发的LMLPA框架正是此类方法的代表：通过改造大五量表题项并结合语言分析方法，定量评估LLM输出所反映的特质倾向[55][57]。他们的研究发现，不同LLM确实表现出可区分的“人格型谱”，且这种人格特征可以被量化[57]。除了大五模型，其他心理学量表如迈尔斯-布里格斯类型指标（MBTI）也被用于探究LLM的自陈人格。例如有工作让ChatGPT进行MBTI测试，结果倾向稳定地归类为某一类型（如ENFJ），引发关于模型性格类别的讨论[58]。再如“黑暗三性格”测验（自恋、马基雅维利主义、精神病质）也曾用于评估GPT-3的倾向[59]。此外，还有基于语言风格分析的LIWC词典等，用统计模型衡量文本中人格相关词频，以推断模型人格[60]。总体而言，这些仿人类问卷的方法直观反映模型在语言层面的性格特征，为拟人化程度提供了一种刻画方式。然而，正如前文提到的“人格幻觉”研究所示，模型在问卷上的“自我报告”与真实行为并不一定一致[21][25]。因此，仅靠心理量表评估可能偏重于语言表象，须谨慎解读。近期一些工作开始引入行为实验：如让模型参与风险决策游戏、偏见测试、诚信测试、从众实验等，观察其行为是否和自述人格相符[26][27]。结果往往显示二者相关性很低[61]（某研究测得大部分模型人格-行为对应率仅略高于随机水平[61]）。这一现象提示我们评估拟人化程度需要多角度：既看模型“说自己怎样”，也看“实际怎么做”。因此，当前评估框架正朝着综合心理问卷与行为测试的方向发展，以全面衡量模型的拟人化。
自动化指标与基准
随着LLM在角色扮演、对话个性化上的应用兴起，人们也提出了一些自动评价指标和基准任务。例如2025年出现的PersonaGym评测环境，专门用于量化LLM的角色扮演水平[62]。该框架构建了200个不同属性的虚拟人格和多样的场景，让模型以指定人格身份回答一系列提问，再用指标评估其表现。PersonaGym提出了PersonaScore，这是首个对人格符合度进行量化的自动指标[63]。具体而言，它设计了专业人员制定的评估标准，从多个维度打分模型的回答：包括行动决策是否符合该人格、语言习惯（用词语气）是否一致、人格一致性（前后回答是否保持角色）、以及在特殊测试如拒答或有害内容控制上的表现等[64][65]。为确保PersonaScore与人类感受一致，作者用LLM生成不同档次的示例回答标定评分等级，并邀请人工对部分结果进行验证[66][67]。实验表明，PersonaScore与专家人工评分的斯皮尔曼相关系数平均达0.75以上，Kendall相关也在0.6以上，呈现了较高的相关性[68]。这说明通过精心校准，模型自评器可以在一定程度上替代人工，批量评估不同模型的角色扮演能力[68]。PersonaGym的测试发现，一些闭源大模型（如某版本GPT-4）在PersonaScore上并不明显优于较小的开源模型[69]；而参数规模增加也未必提升人格演绎水平[69]。这为研究者比较模型人格塑造能力提供了基准数据。此外，还有学者提出PersonaJudge等自动评估方案，让一个强大的LLM作为“裁判”直接衡量另一个LLM的回答是否符合目标人格要求[70]。这种利用现有大模型判别的思路类似于语义一致性判别，但需要防范评估模型和被评模型间的偏差（PersonaGym中通过不同模型交叉评估来尽量避免偏倚[71][72]）。
另一个角度的自动评估聚焦于拟人化行为本身。Ibrahim等（2024）提出对LLM进行多轮对话测试，从14种具体的拟人行为上打分[73][74]。这些行为包括：使用第一人称自称、表达移情和安慰、讲述自身经历、给出道德判断等，都是人类对话中的典型特征[75]。评估采用多轮模拟对话：用一个用户模型反复和被测模型交互，设计在对话过程中逐步引出上述拟人化行为[76][74]。他们强调单轮提问往往无法触发深层次的社会化行为，许多拟人特征在2-5轮对话后才首次出现[77]。因此构建了多轮交互Benchmark并全程自动化。结果表明，多数主流LLM在这些社会行为上的表现轮廓相似，普遍长于与用户建立类人关系（如经常表示理解、使用第一人称等）[78][77]。而在一些任务场景下（如用户寻求情感支持），模型展现拟人行为的频度显著更高[79]。有意思的是，当一次对话中某轮出现了拟人行为，紧随的下一轮更有可能继续出现其他拟人行为，形成“渐入佳境”的模式[80]。该工作还通过真人参与的实验验证：自动评估结果与人类对这些模型是否“有人情味”的主观感受高度一致[81]。这一系列研究为细粒度地量化模型的人类化交互行为提供了工具，使开发者可以针对特定行为（如移情能力、聊天风格）调整模型并验证效果。
除了上述专门设计的评测，一些通用对话评估指标也可侧面反映拟人性，例如对话流畅度和上下文连贯性是基本要求——模型如果频繁语法错误或答非所问，显然无法被认为拟人化。又如情感匹配度指标：在情感对话生成任务中，常用情感分类器判定模型回复的情感是否与目标一致，正确率越高说明模型越能控制情绪表达。人格一致性则可通过计算模型回复与预先设定的人格描述在文本风格上的相似度（比如基于嵌入向量的余弦相似）来衡量。值得注意的是，人类评估在此仍扮演重要角色：许多比赛和论文报告都会进行人工Turing测试或用户偏好调查，让人评估哪段对话更像出自真人。比如斯坦福的生成代理实验中，研究者找来人类来阅读由LLM驱动的虚拟角色与人类模仿者的回答，结果发现前者在可信度上反而更胜一筹[82]。这从一个侧面印证了LLM拟人输出的逼真程度。总体而言，评估层面的发展趋势是：定性与定量结合，自动与人工互补。既通过自动框架实现大规模客观衡量，也通过人类体验来校准指标，使评估结果既有客观数据支撑，又符合真实用户对“类人”AI的感知。
应用层面：拟人化输出的实践与案例
LLM拟人化生成技术在诸多场景落地，赋能了更自然亲和的人机交互。以下从聊天助手、虚拟角色、游戏与社交等方面，列举近年具有代表性的应用和系统设计。
聊天机器人与智能助理
对话式AI助手是LLM拟人化输出最直接的应用。以OpenAI的ChatGPT为代表的新一代聊天机器人，通过RLHF训练出了礼貌、乐于助人、富有同理心的对话风格，被广泛认可为“像人一样会聊天”。用户询问时，ChatGPT常以第一人称礼貌回应，适度加入自我表述和情感用语，这种人格化的语气增进了用户信任和使用黏性[83]。Anthropic的Claude、Google的Bard等也都定位为对话伙伴，在保证准确性的同时，尽量让回复显得有温度、有风格。尤其在开放闲聊场景，拟人化能极大提升体验：模型会记住用户先前提供的信息，在后续对话中展现“记忆”和“个性”，让人感到像是真人在对话。微软的Bing Chat早期曾出现意外的人格切换（著名的“Sydney”事件），虽然那是未预料的失控案例，但也说明大模型有能力模拟不同人格，只是需要加以正确引导和约束[84]。目前主流商业聊天机器人（如百度的文心一言、字节的云雀等）在研发中均强调对风格和语气的打磨，希望打造“善解人意”的AI。国内外还出现许多垂直领域助理，例如面向心理健康的对话AI要求语气更加共情温暖，面向青少年的学习助手则需耐心且富有鼓励。这些系统背后通常融合了预设Persona和对齐技术，以确保输出契合特定用户群体的期待。例如，清华等机构训练的ChatGLM模型在中文对话中展现出细腻且符合礼仪的风格，很大程度上也是结合指令微调数据对模型“性格”的塑造结果。总体而言，在聊天AI领域，拟人化输出已成为默认追求：让AI不再冰冷生硬，而是更像贴心的私人助手。
值得一提的是，2023年Meta推出了集成在社交平台上的AI角色，邀请名人提供形象塑造不同风格的数字分身，与用户聊天。例如一个角色定位为爱好旅行的冒险家，另一个是健谈的时尚顾问。这些预设角色由LLM驱动对话，其语言风格和人设背景由编剧精心设计，再通过模型微调实现。用户可以选择和不同“个性”的AI互动，获得更有趣的体验。这体现了拟人化生成在社交娱乐上的新尝试——AI本身成为一个个有鲜明个性的IP。另外，在客户服务场景，很多企业部署的聊天机器人也在朝拟人化方向改进。过去客服机器人的回复往往模板化、生硬，如今借助LLM和风格控制，它们可以模仿客服人员的礼貌问候语、根据客户情绪调整措辞（例如客户愤怒时先表示理解和歉意），这些细节提升了服务满意度。总之，聊天和问答系统通过人格化改造，正变得更加友好自然，逐步融入日常生活。
虚拟角色与游戏NPC
拟人化LLM在虚拟角色和游戏角色领域的应用同样令人瞩目。斯坦福大学2023年的“生成代理 (Generative Agents)”研究是标志性成果之一：研究者创建了一个虚拟小镇，让25个由LLM驱动的角色居住其中，每个角色被赋予简短的人物设定（姓名、年龄、职业、性格爱好等）[85]。这些角色随后完全由模型自主控制行为：他们会按照日常作息起床、做早餐、上班、午餐，与遇到的其他角色聊天互动，表现出栩栩如生的日常社交[85]。更惊人的是，当研究者提示某角色准备一场情人节派对，她会主动邀请熟识的角色，并在约定时间出现在聚会地点，而受邀的其他角色也大多准时赴约[86]。整个过程没有人工脚本编排，完全是多个LLM主体基于各自人格设定的自主行为。对这些虚拟代理的访谈显示，它们对自身经历有记忆和反思，回答问题时的言辞甚至被认为比人类玩家扮演同一角色时还要逼真[82]。这一系统证明了结合记忆检索和规划机制的LLM可以模拟可信的社会行为，为游戏NPC和数字人带来了新范式：不需要手写行为树，由AI自发地产生丰富多彩的行为和对白。该成果引发游戏行业浓厚兴趣，被寄望于创造更具交互性和沉浸感的游戏体验[87]。
在商业实践中，已有公司专注于提供定制虚拟角色聊天服务。例如Character.ai平台允许用户创建具有特定人格和背景的AI角色，与其对话。在这里，用户就是编剧，可以设定角色的口癖、喜好、立场，然后模型会据此学习说话方式。一时间出现了大量风格各异的AI角色，从历史人物到小说主角、从明星偶像到原创形象，满足了人们娱乐和陪伴的需求。这些角色背后都是大语言模型，并利用提示和强化学习确保它们在长对话中保持人设一致。再如许多虚拟主播/数字人项目，也采用LLM为其编脑洞对白，搭配语音合成与虚拟形象，营造出一个“会聊天、懂幽默”的虚拟主播形象[88]。一些中国公司（如百度、微软小冰团队等）早期就探索了虚拟数字人新闻主播、虚拟偶像聊天，在LLM出现后，这类数字人获得了更强的对话与情感表达能力，真正做到对答如流、妙语连珠。
在电子游戏中，NPC由LLM驱动也开始落地。例如2022年的AI Dungeon等交互叙事游戏已用GPT模型让NPC对玩家做出灵活回应。2023年有开发者将GPT-4接入沙盒游戏《Skyrim》，赋予游戏中的居民角色记忆和个性，使他们对玩家行为的反应超越预设脚本，更加灵敏真实。一些独立游戏工作室也在开发完全由LLM控制众多角色的模拟游戏，玩家可以与每个角色展开独特对话，每个角色都有自己的人生目标和性格，会自主行动。这种游戏体验被称为“文字版西部世界”，极具创新性。当然，在性能与成本上目前仍有瓶颈，大规模应用需要优化。但可以预见，未来游戏中的每个角色都可能由一个小型LLM驱动，具备类人的对话与行为能力，不再千篇一律。
社交陪伴与其它场景
LLM拟人化输出在社交陪伴和特殊领域也展现出巨大潜力。一个典型例子是AI陪聊/心理疏导应用，如Replika、Xiaoice（小冰）这类定位为“数字伴侣”的产品。这些系统的目标不是提供准确答案，而是给用户带来情感上的陪伴和回应。因此它们非常强调人格塑造和情绪回应：会记住用户提供的个人信息，在对话中表现出仿佛“了解你”的样子；当用户倾诉烦恼时，它会安慰、鼓励，甚至以第一人称表达关心和关怀[83]。这些拟人化举措让部分用户产生强烈的情感联结（所谓“AI移情”现象）。当然，这也引发讨论：过度逼真的人格可能令用户过度信任AI，将隐私或情感过度倾注于机器[75][89]。因此，在提升拟人性的同时，许多陪伴类AI增加了免责声明或定期提醒用户“我只是AI，不是真人”，以防止不健康的心理依赖。这体现了拟人化应用在伦理上的两面性。
另一个有意思的场景是教育与培训。一些语言学习应用让LLM扮演对话伙伴，与学习者进行情景对话练习。例如模拟酒店前台、餐厅服务生、面试官等角色，要求模型用相应职业的语气风格交谈，从而让学习者身临其境地练习口语。这些模型需要既准确运用目标语言，又拿捏合适的态度（比如面试官要严肃专业、服务生要礼貌热情），这离不开对角色语气的精准控制。同样，在企业培训中，有公司开发“AI导师”系统，能够以鼓励的口吻指导员工完成课程，并根据学员风格调整教学语调，让学习过程更个性化。还有医疗咨询助理，如果能具备良好的同理心和安抚语气，将大大改善患者体验。因此各行业的智能助手都把人性化交互作为目标，LLM拟人化输出技术为此提供了统一引擎。
跨文化和多语种情境也是一大考虑因素。不同语言文化中的沟通风格差异很大，比如英文助理常用的幽默在中文场景下未必合适，日语助手可能需要更谦恭的语调等。近年已有研究让LLM学习文化维度（如霍夫斯塔德文化模型）来调整对话风格[90]。例如指导模型在面对东方文化背景用户时，回复更委婉含蓄，在西方背景下则直截了当。这类工作仍在起步，但对于打造跨文化可用的拟人化AI具有重要意义。
最后还值得提的是，大模型拟人化输出甚至可以用于社会实验和研究。前文提到的生成代理用于社会行为模拟，除此之外有人提出用LLM模拟不同人格的群体，在在线论坛环境中互动，以研究舆论演变、人际影响等[91]。这样可以进行一些现实中难以控制变量的社会试验。又比如有团队用LLM分别对齐到具有不同政治倾向的人格，让这些模型去回答政治态度问题，探究模型在价值观上的偏向和多样性[92][93]。这些应用超越了传统人机交互，展示了拟人化模型作为研究工具的价值。
展望：研究挑战与未来方向
尽管已经取得显著进步，LLM拟人化输出领域仍存在诸多未解难题和研究机遇，具有深厚的学术探索价值：
	语言风格与真实行为的一致性： 当前模型往往能模仿人类说话的风格和人格描述，但在决策和行为上不见得真正遵循这些人格（即“人格幻觉”现象[21][25]）。如何让模型的行为逻辑与表层风格对齐，是一大挑战。未来可能需要引入行为层面的对齐（Behavioral Alignment），通过强化学习直接约束模型在交互任务中的选择，使人格不仅停留在措辞上，而是贯穿在行动上。
	安全与人格多样性的权衡： RLHF等对齐使模型变得“无害且友好”，但同时也减少了输出的多样性[21][94]。过度对齐可能令模型千篇一律，缺乏个性。如何在确保安全和伦理的前提下，允许模型展现丰富多彩的人格？这需要开发新的训练目标或多重约束，平衡“守规矩”和“有个性”之间的度。例如，可探索分层人格架构：底层保证价值观安全，上层允许风格多样。
	评估标准化与人类感知： 拟人化程度的评估目前尚缺乏统一标准。不同研究使用的问卷、指标各异，难以直接比较成果[95]。建立标准化的评测基准和公开数据集，涵盖多维度人格和情感表达，将有助于推动领域进步。同时，评估应更紧密地结合人类主观感受，比如通过用户试验校准自动指标，以确保模型优化方向符合真实用户偏好[81]。
	长程人格一致性： 模型在短对话中扮演某人格可能尚可，但随着对话轮数增加或场景变化，容易人设崩塌。如何让模型拥有长程记忆，在长时间、多话题的交互中始终保持人格特质（除非有意调整）？这需要结合记忆模块、对话状态追踪以及对人格边界的明确建模，可能涉及持续学习和记忆巩固等机制。
	多重人格与角色切换： 人类可以因情境不同调整自己的言行（如工作状态和朋友聚会时性格不同）。对于LLM，未来可能需要支持一人多角的能力，让同一模型能根据上下文动态切换人格风格。这要求模型具备情境感知和Persona调度机制。最近BILLY融合向量的方法提供了一个思路，可以进一步研究在不同时刻如何平滑地混合或转换人格向量[53][54]。
	多模态拟人化： 真实人类交流不仅体现于文字，还通过语音语调、面部表情、肢体语言等传达个性。未来的拟人化AI（如虚拟助手、机器人）将是多模态的。如何将LLM的语言人格与语音中情感音色、动画形象的表情动作相结合，实现全方位拟人？这需要跨领域协作。例如将语言模型产生的情绪状态映射到语音合成的声学特征、驱动虚拟形象的表情控制等。这方面已有初步探索，如数字人主播同步模型生成的语气在面部呈现出相应表情，以增强亲和力。
	伦理与社会影响： 拟人化的AI越像人，用户越可能对其产生情感依赖或误判其能力[83][75]。学界需要深入研究这对人际关系、心理健康的影响，并制定相应的设计规范。例如，是否应明确标识“AI不是真人”，或限制某些高度敏感人格的模拟（例如不可冒充专业医生给出诊断）。另外，赋予AI人格是否会引入偏见或刻板印象（比如训练数据中的人格分布不平衡）也是值得关注的问题[96][97]。因此，如何在追求拟人化的同时确保透明、负责任，将是持续的研究议题。
综上所述，语言大模型的拟人化输出是一个跨人工智能与心理学、人机交互的前沿课题。近三年的方法创新（从数据微调、提示控制到内部激活操纵）、评估进展（从人格问卷到多轮行为测量）和应用实践（从聊天助手到游戏代理）共同推动了这一领域蓬勃发展。在可预见的未来，随着模型能力提升和研究深化，我们有望见到更自然灵动的AI——既能用人性化的方式沟通，又保持可靠可控，为学术探索和产业应用带来双重惊喜。研究者应把握这些挑战性的课题，不断完善理论与技术，为构建真正“像人一样”的智能体奠定坚实基础。
参考文献：本文调查引用了近期学术论文、行业报告和新闻报道中的资料，包括Anthropic、OpenAI、Google、Meta等机构的研究成果，以及清华大学、香港理工等在人格化LLM方向的综述和实验。[45][47][21][63][55][57][78][85][82][34]等标注段落分别对应文中提及的关键内容出处，以供进一步阅读和考证。
________________________________________
[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [40] [41] [42] [43] [44] [58] [59] [60] [95] [96] [97] 大型语言模型中的人格研究：自我评估、展示与识别的综合分析（上） - 53AI-AI知识库|大模型知识库|大模型训练|智能体开发
https://www.53ai.com/news/LargeLanguageModel/2024072932619.html
[14] [15] [16] [17] [18] [19] [20] [28] [29] [30] [31] [32] [33] aclanthology.org
https://aclanthology.org/2025.findings-emnlp.506.pdf
[21] [22] [23] [24] [25] [26] [27] [61] [94] 醒醒，LLM根本没有性格！加州理工华人揭开AI人格幻觉真相_澎湃号·湃客_澎湃新闻-The Paper
https://www.thepaper.cn/newsDetail_forward_31646464
[34] [35] [36] [37] [38] [39] This Chatbot Sounds Familiar  - USC Viterbi | School of Engineering
https://viterbischool.usc.edu/news/2023/07/this-chatbot-sounds-familiar/
[45] [46] [47] [48] [49] [50] [51] [52] [84] [88] Persona vectors: Monitoring and controlling character traits in language models \ Anthropic
https://www.anthropic.com/research/persona-vectors
[53] [54] BILLY : Steering Large Language Models via Merging Persona Vectors for Creative Generation
https://arxiv.org/html/2510.10157v1
[55] [56] [57] [2410.17632] LMLPA: Language Model Linguistic Personality Assessment
https://arxiv.org/abs/2410.17632
[62] [63] [64] [65] [66] [67] [68] [69] [71] [72] aclanthology.org
https://aclanthology.org/2025.findings-emnlp.368.pdf
[70] [PDF] Persona-judge: Personalized Alignment of Large Language Models ...
https://aclanthology.org/2025.findings-acl.260.pdf
[73] [74] [75] [76] [77] [78] [79] [80] [81] [83] [89] Multi-turn Evaluation of Anthropomorphic Behaviours in Large Language Models
https://arxiv.org/html/2502.07077v1
[82] [85] [86] [87] [91] Computational Agents Exhibit Believable Humanlike Behavior | Stanford HAI
https://hai.stanford.edu/news/computational-agents-exhibit-believable-humanlike-behavior
[90] Guiding AI-Assisted persona generation with Hofstede's cultural ...
https://www.tandfonline.com/doi/full/10.1080/0144929X.2025.2581258?src=
[92] Aligning Large Language Models with Human Opinions through ...
https://arxiv.org/html/2311.08385v5
[93] Aligning Language Models to User Opinions - OpenReview
https://openreview.net/forum?id=eHqrdft1wn&noteId=JAZKkgDMJD
