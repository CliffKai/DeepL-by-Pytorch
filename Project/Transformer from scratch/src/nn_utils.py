import torch

from torch import Tensor
from jaxtyping import Float
from typing import Iterable

def softmax(x: Tensor, dim: int) -> Tensor:
    shifted = x - x.max(dim=dim, keepdim=True).values
    exps = torch.exp(shifted)
    return exps / exps.sum(dim=dim, keepdim=True)

def silu(x: Float[Tensor, "..."]) -> Float[Tensor, "..."]:
    return x * torch.sigmoid(x)

def cross_entropy(inputs: Tensor, targets: Tensor) -> Tensor:
    logsumexp = torch.logsumexp(inputs, dim=1, keepdim=True)
    log_probs = inputs - logsumexp
    gathered = log_probs.gather(1, targets.view(-1, 1)).squeeze(1)
    return -gathered.mean()

def gradient_clipping(parameters: Iterable[torch.nn.Parameter], max_l2_norm: float) -> None:
    params = [p for p in parameters if p.grad is not None]
    if not params:
        return
    device = params[0].grad.device
    grads_norms = torch.stack([p.grad.detach().norm(2) for p in params]).to(device)
    total_norm = grads_norms.norm(2)
    clip_coef = max_l2_norm / (total_norm + 1e-6)
    if clip_coef < 1.0:
        for p in params:
            p.grad.detach().mul_(clip_coef.to(p.grad.device))

# test code
'''
import math
import torch
import torch.nn.functional as F

from typing import Iterable
from torch import Tensor
from src.nn_utils import silu, softmax, gradient_clipping, cross_entropy


# ====== å·¥å…·å‡½æ•° ======
def banner(s: str):
    print("\n" + "="*10 + " " + s + " " + "="*10)

# å›ºå®šéšæœºç§å­
torch.manual_seed(0)


# =========================
# 1) softmax æµ‹è¯•
# =========================
def test_softmax():
    def banner(s: str):
        print("\n" + "="*10 + " " + s + " " + "="*10)

    banner("softmax")

    # 1) å½¢çŠ¶ + ä¸Žå®˜æ–¹å®žçŽ°ä¸€è‡´æ€§ + å½’ä¸€åŒ–
    for shape, dim in [((2, 5), 1), ((3, 4, 7), -1), ((4, 6), 0)]:
        x = torch.randn(*shape, dtype=torch.float32)
        y = softmax(x, dim)
        y_ref = torch.softmax(x, dim=dim)
        assert torch.allclose(y, y_ref, atol=1e-6), "softmax ä¸Ž torch.softmax ä¸ä¸€è‡´"
        s = y.sum(dim=dim)
        assert torch.allclose(s, torch.ones_like(s), atol=1e-6), "softmax æ²¿ dim çš„å’Œåº”ä¸º 1"

    # 2) æ•°å€¼ç¨³å®šæ€§
    x0 = torch.tensor([[1.2, -0.7, 3.4]], dtype=torch.float32)

    # 2a) ä¸Žå®˜æ–¹åœ¨ç›¸åŒè¾“å…¥ä¸Šå¯¹é½ï¼ˆå¤§å¸¸æ•°åœºæ™¯ï¼‰
    for c in (1e6, 1e4, 1e2):
        y0 = softmax(x0 + c, dim=1)
        y0_ref = torch.softmax(x0 + c, dim=1)
        assert torch.isfinite(y0).all(), f"softmax(x+{c}) å‡ºçŽ°éžæœ‰é™å€¼"
        assert torch.allclose(y0, y0_ref, atol=1e-6, rtol=1e-6), f"ä¸Ž torch.softmax(x+{c}) ä¸ä¸€è‡´"

    # 2b) è‡ªèº«â€œå¹³ç§»ä¸å˜æ€§â€ä¸Žå®˜æ–¹ä¸€è‡´ï¼ˆæ›¿ä»£è¿‡ä¸¥çš„ç»å¯¹ä¸å˜æ€§æ–­è¨€ï¼‰
    # æ€è·¯ï¼šæ¯”è¾ƒæˆ‘ä»¬ä¸Žå®˜æ–¹åœ¨ c=1e4 æ—¶çš„å·®å¼‚æ˜¯å¦åŒé‡çº§ã€‚
    c = 1e4
    y_plain      = softmax(x0, dim=1)
    y_shift      = softmax(x0 + c, dim=1)
    y_plain_ref  = torch.softmax(x0, dim=1)
    y_shift_ref  = torch.softmax(x0 + c, dim=1)

    diff_mine = (y_plain - y_shift).abs().max().item()
    diff_ref  = (y_plain_ref - y_shift_ref).abs().max().item()

    # å…è®¸æžå°çš„æ•°å€¼æ³¢åŠ¨ï¼ˆæˆ‘ä»¬ä¸Žå®˜æ–¹æœ€å¤šç›¸å·® 10% æˆ– 1e-6 ä¸­è¾ƒå¤§è€…ï¼‰
    tol = max(1e-6, 0.1 * diff_ref)
    if abs(diff_mine - diff_ref) > tol:
        print(f"[diag] invariance mine={diff_mine:.8g}, ref={diff_ref:.8g}, tol={tol:.8g}")
    assert abs(diff_mine - diff_ref) <= tol, \
        "softmax çš„å¹³ç§»ä¸å˜æ€§æ•°å€¼è¯¯å·®ä¸Žå®˜æ–¹ä¸ä¸€è‡´ï¼ˆä½†è¿™ä¸å½±å“åŠŸèƒ½ï¼Œåªæ˜¯æµ‹è¯•è¿‡ä¸¥ï¼‰"

    # 3) å·¨å¤§é—´éš” â†’ è¿‘ä¼¼ one-hot
    huge_gap = torch.tensor([[1000.0, 0.0, -1000.0]], dtype=torch.float32)
    y_gap = softmax(huge_gap, dim=1)
    target = torch.tensor([[1.0, 0.0, 0.0]], dtype=y_gap.dtype)
    assert torch.allclose(y_gap, target, atol=1e-6), "softmax å·¨å¤§é—´éš”æ—¶åº”è¿‘ä¼¼ one-hot"

    # 4) åå‘ä¼ æ’­æ£€æŸ¥
    xg = torch.randn(4, 7, dtype=torch.float32, requires_grad=True)
    yg = softmax(xg, dim=1)
    yg.sum().backward()
    assert xg.grad is not None and xg.grad.shape == xg.shape, "softmax åå‘ä¼ æ’­åº”äº§ç”Ÿæ¢¯åº¦"

    print("âœ… softmax æµ‹è¯•é€šè¿‡")

# =========================
# 2) silu æµ‹è¯•
# =========================
def test_silu():
    banner("silu")
    x = torch.linspace(-5, 5, steps=100)
    y = silu(x)
    # æ­£ç¡®çš„ SiLU: x * sigmoid(x)
    y_true = x * torch.sigmoid(x)

    # è¿™ä¸ªæ–­è¨€åœ¨ä½ å½“å‰å®žçŽ°ï¼ˆä»… sigmoidï¼‰ä¼šå¤±è´¥ï¼Œç”¨äºŽæé†’ä¿®å¤ï¼š
    assert torch.allclose(y, y_true, atol=1e-6), (
        "âŒ silu å®žçŽ°ä¸æ­£ç¡®ï¼šåº”ä¸º x * sigmoid(x)ã€‚"
        " ä½ å½“å‰å®žçŽ°è¿”å›žçš„æ˜¯ sigmoid(x)ã€‚è¯·æ”¹ä¸ºï¼šreturn x * torch.sigmoid(x)"
    )

    # é›¶è¾“å…¥ï¼šsilu(0)=0
    assert silu(torch.tensor(0.0)).abs() < 1e-8, "silu(0) åº”ä¸º 0"

    # åå‘ä¼ æ’­å¯ç”¨æ€§
    x2 = torch.randn(8, requires_grad=True)
    (silu(x2).sum()).backward()
    assert x2.grad is not None, "silu åå‘ä¼ æ’­åº”äº§ç”Ÿæ¢¯åº¦"

    print("âœ… silu æµ‹è¯•é€šè¿‡ï¼ˆä¿®æ­£å®žçŽ°åŽå†è¿è¡Œåº”é€šè¿‡ï¼‰")


# =========================
# 3) cross_entropy æµ‹è¯•ï¼ˆäºŒç»´è¾“å…¥ï¼‰
# =========================
def test_cross_entropy():
    banner("cross_entropy")
    B, V = 16, 11
    x = torch.randn(B, V, requires_grad=True)  # logits
    t = torch.randint(low=0, high=V, size=(B,))

    # æˆ‘ä»¬å®žçŽ°
    loss = cross_entropy(x, t)

    # PyTorch å‚è€ƒ
    loss_ref = F.cross_entropy(x, t, reduction="mean")

    assert torch.allclose(loss, loss_ref, atol=1e-6), "cross_entropy ä¸Ž F.cross_entropy ä¸ä¸€è‡´"

    # åå‘ä¼ æ’­æ¢¯åº¦å­˜åœ¨
    loss.backward()
    assert x.grad is not None and x.grad.shape == x.shape, "cross_entropy åå‘ä¼ æ’­å¤±è´¥"

    # ç®€å• sanityï¼šç›¸åŒ target çš„ä¸¤è¡Œï¼Œloss ç›¸ç­‰ï¼ˆåœ¨ logits ç›¸åŒå‰æä¸‹ï¼‰
    with torch.no_grad():
        x2 = torch.randn(2, V)
        t2 = torch.tensor([3, 3])
        l2 = cross_entropy(x2, t2)
        l2_ref = F.cross_entropy(x2, t2)
        assert torch.allclose(l2, l2_ref, atol=1e-6)

    # æžç«¯è¾“å…¥ç¨³å®šæ€§ï¼ˆå¤§æ­£/å¤§è´Ÿï¼‰
    big = torch.tensor([[1000.0] + [-1000.0]*(V-1)], requires_grad=True)
    tgt = torch.tensor([0])
    l_big = cross_entropy(big, tgt)
    assert torch.isfinite(l_big), "å¤§æ•°æƒ…å†µä¸‹ loss åº”ä¸ºæœ‰é™å€¼"

    print("âœ… cross_entropy æµ‹è¯•é€šè¿‡")


# =========================
# 4) gradient_clipping æµ‹è¯•
# =========================
def test_gradient_clipping():
    banner("gradient_clipping")

    # æž„é€ ä¸¤ä¸ªå‚æ•°å¹¶è®¾å®šæ¢¯åº¦
    p1 = torch.nn.Parameter(torch.randn(5))
    p2 = torch.nn.Parameter(torch.randn(3))
    p1.grad = torch.tensor([3.0, 4.0, 0.0, 0.0, 0.0])  # ||g1||=5
    p2.grad = torch.tensor([0.0, 0.0, 12.0])          # ||g2||=12
    # å…¨å±€èŒƒæ•° = sqrt(5^2 + 12^2) = 13
    total = math.sqrt(5**2 + 12**2)  # 13

    max_norm = 6.5  # æ°å¥½æ˜¯ 13 çš„ä¸€åŠ
    expected_coef = max_norm / (total + 1e-6)  # â‰ˆ 0.5

    gradient_clipping([p1, p2], max_norm)

    assert torch.allclose(p1.grad, torch.tensor([3.0, 4.0, 0.0, 0.0, 0.0]) * expected_coef, atol=1e-6)
    assert torch.allclose(p2.grad, torch.tensor([0.0, 0.0, 12.0]) * expected_coef, atol=1e-6)

    # å¦‚æžœæ€»èŒƒæ•°å°äºŽé˜ˆå€¼ï¼Œä¸åº”æ”¹å˜
    p3 = torch.nn.Parameter(torch.randn(2))
    p3.grad = torch.tensor([0.3, 0.4])  # èŒƒæ•° 0.5
    before = p3.grad.clone()
    gradient_clipping([p3], max_l2_norm=1.0)
    assert torch.allclose(p3.grad, before), "ä½ŽäºŽé˜ˆå€¼æ—¶ä¸åº”ç¼©æ”¾"

    # ç©ºå‚æ•°/æ— æ¢¯åº¦çš„å¥å£®æ€§
    gradient_clipping([], 1.0)  # ä¸åº”æŠ¥é”™
    p4 = torch.nn.Parameter(torch.randn(2))
    p4.grad = None
    gradient_clipping([p4], 1.0)  # ä¸åº”æŠ¥é”™

    print("âœ… gradient_clipping åŸºç¡€æµ‹è¯•é€šè¿‡")

    # ä¸Ž torch.clip_grad_norm_ è¯­ä¹‰ä¸€è‡´æ€§ï¼ˆå…¨å±€ç³»æ•°ï¼‰
    # è¯´æ˜Žï¼šPyTorch è¿”å›žè¢«è£å‰ªå‰çš„æ€»èŒƒæ•°ï¼›æˆ‘ä»¬æ¯”å¯¹ç¼©æ”¾åŽçš„æ¢¯åº¦æ¯”å€¼æ˜¯å¦ä¸€è‡´
    ps = [torch.nn.Parameter(torch.randn(10)) for _ in range(3)]
    for p in ps:
        p.grad = torch.randn_like(p)
    ref_params = [torch.nn.Parameter(p.detach().clone()) for p in ps]
    for a, b in zip(ref_params, ps):
        a.grad = b.grad.detach().clone()

    maxn = 2.0
    # æˆ‘ä»¬å®žçŽ°
    gradient_clipping(ps, maxn)
    # å‚è€ƒå®žçŽ°ï¼ˆin-placeï¼‰
    ref_total = torch.nn.utils.clip_grad.clip_grad_norm_(ref_params, maxn)

    # æ¯”è¾ƒæ¯ä¸ªå¼ é‡ç¼©æ”¾ç³»æ•°ç›¸ç­‰ï¼ˆé™¤éžæœ¬æ¥æ€»èŒƒæ•° <= é˜ˆå€¼ï¼Œæ­¤æ—¶éƒ½ä¸ç¼©æ”¾ï¼‰
    # å³ï¼šgrad_after / grad_before åœ¨æ‰€æœ‰ param ä¸Šåº”è¿‘ä¼¼ä¸€è‡´
    ratios = []
    for p in ps:
        # é¿å… 0 é™¤
        m = p.grad.abs().max().item()
        ratios.append((p.grad / (p.grad.detach() / ((p.grad / p.grad).abs().nan_to_num(0)+1e-12))).abs().max().item())
    # æ›´ç›´æŽ¥ï¼šæ£€æŸ¥æ–¹å‘ä¸å˜ä¸”åˆå¹¶åŽæ€»èŒƒæ•°ä¸è¶…è¿‡é˜ˆå€¼
    tot = torch.sqrt(sum([(p.grad.detach()**2).sum() for p in ps]))
    assert tot <= maxn + 1e-4, "è£å‰ªåŽå…¨å±€èŒƒæ•°åº”ä¸è¶…è¿‡é˜ˆå€¼"

    print("âœ… ä¸Ž torch.clip_grad_norm_ è¯­ä¹‰ä¸€è‡´ï¼ˆå…¨å±€ L2 è£å‰ªï¼‰")


# =========================
# 5) CUDA + float16ï¼ˆå¯é€‰ï¼‰
# =========================
def test_cuda_half_optional():
    banner("CUDA float16ï¼ˆå¯é€‰ï¼‰")
    if not torch.cuda.is_available():
        print("(è·³è¿‡ï¼šCUDA ä¸å¯ç”¨)")
        return

    device = torch.device("cuda")
    # softmax å½¢çŠ¶/å½’ä¸€åŒ–æ£€æŸ¥
    x = torch.randn(4, 7, device=device).half().requires_grad_(True)
    y = softmax(x, dim=1)
    assert y.shape == x.shape
    assert torch.allclose(y.sum(dim=1), torch.ones(4, device=device, dtype=y.dtype), atol=1e-3)

    # cross_entropy ä¸Ž F.cross_entropy å¯¹é½ï¼ˆåŠç²¾åº¦è½¬ float32 æ¯”è¾ƒï¼‰
    B, V = 32, 50
    logits = torch.randn(B, V, device=device).half().requires_grad_(True)
    targets = torch.randint(0, V, (B,), device=device)
    loss = cross_entropy(logits, targets)
    loss_ref = F.cross_entropy(logits.float(), targets, reduction="mean").to(loss.dtype)
    assert torch.allclose(loss, loss_ref, atol=3e-3)
    loss.backward()
    assert logits.grad is not None

    print("âœ… CUDA float16 åŸºæœ¬æµ‹è¯•é€šè¿‡")


if __name__ == "__main__":
    test_softmax()
    # å…ˆè¿è¡Œ silu ä¼šâ€œæœ‰æ„å¤±è´¥â€æé†’ä½ ä¿®æ­£ï¼›è‹¥ä½ å·²ä¿®æ­£ä¸º x*sigmoid(x)ï¼Œæ­¤æµ‹è¯•ä¼šé€šè¿‡
    try:
        test_silu()
    except AssertionError as e:
        print(str(e))
        print("ðŸ‘‰ æç¤ºï¼šæŠŠ silu æ”¹ä¸º `return x * torch.sigmoid(x)` åŽï¼Œé‡æ–°è¿è¡Œæœ¬æµ‹è¯•ã€‚")

    test_cross_entropy()
    test_gradient_clipping()
    test_cuda_half_optional()

    print("\nðŸŽ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆã€‚")
'''

