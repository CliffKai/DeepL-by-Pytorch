import torch
import math
import torch.nn as nn

from torch import Tensor
from jaxtyping import Float, Bool
from einops import einsum

from ..nn_utils import softmax

def attention(
    q: Float[Tensor, "... quaries d_k"],
    k: Float[Tensor, "... key d_k"],
    v: Float[Tensor, "... value d_k"],
    mask: Bool[Tensor, "... quaries value"] | None=None,
) -> Float[Tensor, "... quaries d_k"]:
    d_k = q.size(-1)
    scores = einsum(q, k, "... q d, ... k d -> ... q k") / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == False, float("-inf"))
    aw = softmax(scores, dim=-1)
    output = einsum(aw, v, "... q k, ... k d -> ... q d")
    return output














# attention test code
'''
import torch
from torch import Tensor
from einops import einsum

from src.model.attention import attention

# 1ï¸âƒ£ å›ºå®šéšæœºç§å­
torch.manual_seed(0)

# 2ï¸âƒ£ åŸºæœ¬è¶…å‚æ•°
batch_size = 2
seq_len_q = 3
seq_len_kv = 4
d_k = 8

# 3ï¸âƒ£ æ„é€  Q, K, Vï¼ˆå¯ä»¥å¸¦ batch ç»´åº¦ï¼‰
q = torch.randn(batch_size, seq_len_q, d_k, requires_grad=True)
k = torch.randn(batch_size, seq_len_kv, d_k, requires_grad=True)
v = torch.randn(batch_size, seq_len_kv, d_k, requires_grad=True)

# 4ï¸âƒ£ æ„é€  maskï¼šå…è®¸å‰ä¸¤ä¸ª query çœ‹åˆ°æ‰€æœ‰ keyï¼Œæœ€åä¸€ä¸ª query åªçœ‹å‰ 2 ä¸ª key
mask = torch.ones(batch_size, seq_len_q, seq_len_kv, dtype=torch.bool)
mask[:, -1, 2:] = 0  # æ¨¡æ‹Ÿä¸‹ä¸‰è§’æˆ–å±€éƒ¨æ³¨æ„åŠ›çš„æƒ…å†µ

print("Q shape:", q.shape)
print("K shape:", k.shape)
print("V shape:", v.shape)
print("Mask shape:", mask.shape)

# 5ï¸âƒ£ å‰å‘ä¼ æ’­
output = attention(q, k, v, mask)
print("Output shape:", output.shape)
print("Output sample:\n", output[0, 0, :5])

# 6ï¸âƒ£ æ‰‹åŠ¨å®ç°å¯¹æ¯”ï¼ˆéªŒè¯ softmax & einsum é€»è¾‘ï¼‰
with torch.no_grad():
    d_k_sqrt = d_k ** 0.5
    scores_ref = einsum(q, k, "... q d_k, ... k d_k -> ... q k") / d_k_sqrt
    scores_ref = scores_ref.masked_fill(mask == False, float("-inf"))
    aw_ref = torch.softmax(scores_ref, dim=-1)
    output_ref = einsum(aw_ref, v, "... q k, ... k d_k -> ... q d_k")

print("\nAll close to manual reference?", torch.allclose(output, output_ref, atol=1e-6))

# 7ï¸âƒ£ æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
assert output.shape == (batch_size, seq_len_q, d_k), "âŒ è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…ï¼"
print("âœ… å½¢çŠ¶æ£€æŸ¥é€šè¿‡ã€‚")

# 8ï¸âƒ£ é›¶è¾“å…¥ç¨³å®šæ€§ï¼ˆåº”è¾“å‡ºå…¨ 0ï¼‰
with torch.no_grad():
    q_zero = torch.zeros_like(q)
    k_zero = torch.zeros_like(k)
    v_zero = torch.zeros_like(v)
    mask_all = torch.ones_like(mask, dtype=torch.bool)
    out_zero = attention(q_zero, k_zero, v_zero, mask_all)
    assert torch.count_nonzero(out_zero) == 0, "âŒ é›¶è¾“å…¥æ—¶è¾“å‡ºåº”ä¸ºå…¨ 0"
print("âœ… é›¶è¾“å…¥ç¨³å®šæ€§é€šè¿‡ã€‚")

# 9ï¸âƒ£ åå‘ä¼ æ’­æ£€æŸ¥
loss = output.sum()
loss.backward()
assert q.grad is not None, "âŒ Q æ²¡æœ‰æ¢¯åº¦"
assert k.grad is not None, "âŒ K æ²¡æœ‰æ¢¯åº¦"
assert v.grad is not None, "âŒ V æ²¡æœ‰æ¢¯åº¦"
print("âœ… åå‘ä¼ æ’­æ£€æŸ¥é€šè¿‡ã€‚")

# 1ï¸âƒ£0ï¸âƒ£ CUDA åŠç²¾åº¦æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
if torch.cuda.is_available():
    print("\nğŸš€ åœ¨ CUDA + float16 ä¸‹æµ‹è¯•ï¼š")
    device = torch.device("cuda")
    q_half = q.detach().to(device).half().requires_grad_(True)
    k_half = k.detach().to(device).half().requires_grad_(True)
    v_half = v.detach().to(device).half().requires_grad_(True)
    mask_half = mask.to(device)
    out_half = attention(q_half, k_half, v_half, mask_half)
    assert out_half.shape == (batch_size, seq_len_q, d_k)
    print("âœ… CUDA float16 å½¢çŠ¶æ£€æŸ¥é€šè¿‡ã€‚")
'''