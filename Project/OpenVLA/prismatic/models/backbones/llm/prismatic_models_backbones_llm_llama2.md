# ğŸ” ç±»ï¼š`LLaMa2LLMBackbone`

### ğŸ§  ç±»çš„æ€»ä½“ä½œç”¨

* `LLaMa2LLMBackbone` æ˜¯ HuggingFace é£æ ¼çš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal LLMï¼‰çš„å­ç±»ï¼›
* å…·ä½“é€‚é…äº† LLaMA-2 åŠå…¶è¡ç”Ÿæ¨¡å‹ï¼ˆå¦‚ Vicunaï¼‰ï¼›
* ä¸º OpenVLA æ¡†æ¶æä¾›ç»Ÿä¸€çš„ LLM æ¥å£ï¼Œå¹¶å¤„ç†äº† tokenizer/pad token/prompt æ„å»ºç­‰ç‰¹æ®Šå¤„ç†ï¼›
* åŸºç±»ä¸º `HFCausalLLMBackbone`ï¼Œå†å¾€ä¸Šæ˜¯ `LLMBackbone`ã€‚

---

## ğŸ“š ç±»ä¸­å‡½æ•°å’Œå±æ€§ä¸€è§ˆï¼š

| æˆå‘˜ç±»å‹ | åç§°                            | è¯´æ˜                 |
| ---- | ----------------------------- | ------------------ |
| æ„é€ å‡½æ•° | `__init__`                    | åˆå§‹åŒ– LLaMA2 ç³»åˆ—æ¨¡å‹    |
| å±æ€§   | `prompt_builder_fn`           | è¿”å› prompt æ„é€ å™¨ç±»å‹    |
| å±æ€§   | `transformer_layer_cls`       | è¿”å› transformer å•å±‚ç±» |
| å±æ€§   | `half_precision_dtype`        | æŒ‡å®šåŠç²¾åº¦ç±»å‹            |
| å±æ€§   | `last_layer_finetune_modules` | æŒ‡å®šå¾®è°ƒçš„æœ€åå‡ å±‚æ¨¡å—        |

---

## 1ï¸âƒ£ æ„é€ å‡½æ•° `__init__`

### âœ… å‡½æ•°ç­¾åï¼š

```python
def __init__(...)
```

### âœ… è¾“å…¥å‚æ•°ï¼š

| å‚æ•°                      | è¯´æ˜                       |
| ----------------------- | ------------------------ |
| `llm_backbone_id`       | æ¨¡å‹ IDï¼Œå¦‚ "llama2-7b-chat" |
| `llm_max_length`        | æœ€å¤§ token é•¿åº¦ï¼Œé»˜è®¤ 2048      |
| `hf_token`              | HuggingFace API token    |
| `inference_mode`        | æ˜¯å¦æ¨ç†æ¨¡å¼ï¼ˆä¸å¼€å¯è®­ç»ƒï¼‰            |
| `use_flash_attention_2` | æ˜¯å¦ä½¿ç”¨ Flash Attention 2   |

---

### âœ… æ ¸å¿ƒåŠŸèƒ½é€è¡Œè§£é‡Šï¼š

```python
super().__init__(..., **LLAMA2_MODELS[llm_backbone_id])
```

* è°ƒç”¨çˆ¶ç±» `HFCausalLLMBackbone` çš„æ„é€ å‡½æ•°ï¼›
* å…¶ä¸­ `LLAMA2_MODELS[llm_backbone_id]` ä¼šè¿”å›è¯¥æ¨¡å‹çš„ä¸‰å…ƒç»„ï¼š

  ```python
  {
    "llm_family": "llama2",
    "llm_cls": LlamaForCausalLM,
    "hf_hub_path": "meta-llama/..."
  }
  ```

---

### ğŸ”¸ ç‰¹æ®Šå¤„ç†ï¼šæ·»åŠ  PAD token

```python
self.tokenizer.add_special_tokens({"pad_token": "<PAD>"})
```

* ç»™ tokenizer æ·»åŠ  `<PAD>` tokenï¼Œè¿™å¯¹ LLaMA ç³»åˆ—æ˜¯å¿…éœ€çš„ï¼›
* é»˜è®¤çš„ tokenizer å¯èƒ½æ²¡æœ‰ PAD tokenã€‚

```python
self.llm.config.pad_token_id = self.tokenizer.pad_token_id
```

* è®¾ç½®æ¨¡å‹çš„ `pad_token_id`ï¼Œå¦åˆ™å¯èƒ½å‡ºç°è®­ç»ƒå¯¹é½é”™è¯¯ã€‚

```python
self.llm.resize_token_embeddings(len(self.tokenizer), pad_to_multiple_of=64)
```

* é‡æ–°è°ƒæ•´ token embedding å±‚å¤§å°ï¼›
* `pad_to_multiple_of=64` æ˜¯ä¸ºäº†ä¸ GPU åŠ é€Ÿå¯¹é½æ›´é«˜æ•ˆã€‚

---

## 2ï¸âƒ£ å±æ€§ `prompt_builder_fn`

```python
@property
def prompt_builder_fn(self) -> Type[PromptBuilder]:
```

### âœ… åŠŸèƒ½ï¼š

æ ¹æ®æ¨¡å‹ ID è¿”å›å¯¹åº”çš„ Prompt æ„é€ å™¨ç±»ï¼Œå½±å“ Prompt æ ¼å¼ï¼ˆèŠå¤©é£æ ¼ vs. çº¯æ–‡æœ¬ï¼‰ï¼š

| æ¡ä»¶                | è¿”å›å€¼                          |
| ----------------- | ---------------------------- |
| `llama2-xxx-pure` | `PurePromptBuilder`          |
| `llama2-xxx-chat` | `LLaMa2ChatPromptBuilder`    |
| `vicuna`          | `VicunaV15ChatPromptBuilder` |

#### ğŸ“Œ å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸ï¼š

```python
raise ValueError(f"No PromptBuilder defined for LLM Backbone `{self.identifier}`")
```

---

## 3ï¸âƒ£ å±æ€§ `transformer_layer_cls`

```python
@property
def transformer_layer_cls(self) -> Type[nn.Module]:
    return LlamaDecoderLayer
```

### âœ… åŠŸèƒ½ï¼š

* è¿”å› transformer ä¸­çš„å•å±‚æ¨¡å—ç±»ï¼ˆç”¨äº FSDP è‡ªåŠ¨åŒ…è£¹ç­–ç•¥ï¼‰ï¼›
* `LlamaDecoderLayer` æ˜¯ HuggingFace çš„ LLaMA å•å±‚ç»“æ„ã€‚

---

## 4ï¸âƒ£ å±æ€§ `half_precision_dtype`

```python
@property
def half_precision_dtype(self) -> torch.dtype:
    return torch.bfloat16
```

### âœ… åŠŸèƒ½ï¼š

* æŒ‡å®šä½¿ç”¨ BF16 ä½œä¸ºåŠç²¾åº¦æ•°æ®ç±»å‹ï¼›
* LLaMA-2 åŸå§‹è®­ç»ƒä½¿ç”¨çš„æ˜¯ BF16ï¼Œå…¼å®¹æ€§æ›´å¥½ï¼›
* å¯¹åº”å‚è€ƒï¼š[https://huggingface.co/docs/transformers/main/model\_doc/llama2](https://huggingface.co/docs/transformers/main/model_doc/llama2)

---

## 5ï¸âƒ£ å±æ€§ `last_layer_finetune_modules`

```python
@property
def last_layer_finetune_modules(self) -> Sequence[nn.Module]:
    return (
        self.llm.model.embed_tokens,
        self.llm.model.layers[-1],
        self.llm.lm_head,
    )
```

### âœ… åŠŸèƒ½ï¼š

è¿”å›æ¨èå¾®è°ƒçš„æœ€åå‡ å±‚æ¨¡å—ï¼š

* `embed_tokens`: åµŒå…¥å±‚
* `æœ€åä¸€å±‚ transformer block`: æå–æœ€åçš„é«˜çº§è¯­ä¹‰
* `lm_head`: è¾“å‡º logits çš„çº¿æ€§å±‚

é€‚åˆç”¨åœ¨ **éƒ¨åˆ†å¾®è°ƒï¼ˆparameter-efficient fine-tuningï¼‰** ä¸­ï¼Œä¾‹å¦‚ LoRAã€BitFitã€‚

---

## âœ… æ€»ç»“

| ç»„ä»¶        | å†…å®¹                                |
| --------- | --------------------------------- |
| æ¨¡å‹æ¥æº      | HuggingFace (`LlamaForCausalLM`)  |
| æ¨¡å‹ç±»å‹      | LLaMA-2 çº¯ç‰ˆã€Chatç‰ˆã€Vicuna           |
| ç‰¹æ®Šå¤„ç†      | æ·»åŠ  `<PAD>` tokenã€resize embedding |
| Prompt é€‚é… | è¿”å›ä¸åŒ PromptBuilder                |
| å¾®è°ƒå±‚æ¨è     | åªå¯¹åµŒå…¥ã€æœ€åä¸€å±‚å’Œè¾“å‡ºå¤´åš fine-tune          |

---

è¿™ä¸ªç±»æ˜¯ LLM æ¥å…¥ OpenVLA çš„å®Œæ•´ä¸€ç¯ã€‚ä¸Šæ¸¸æ³¨å†Œå™¨åŠ è½½æ¨¡å‹ IDï¼Œè°ƒç”¨æ„é€ å‡½æ•°ï¼Œè‡ªåŠ¨æ¥å¥½ tokenizerã€Prompt æ„é€ å™¨ã€forward æ¥å£ç­‰ã€‚

---
