### 1. Transformer的自注意力机制及相比RNN的优势？

| 对比点        | RNN/LSTM            | Transformer（自注意力）      |
| :--------- | :------------------ | :--------------------- |
| **并行性**    | 序列依赖，必须一步步处理（无法并行）  | 所有位置可同时计算（完全并行）        |
| **长程依赖**   | 随时间步增加，梯度容易消失或爆炸    | 任意两个位置的依赖都通过注意力直接建模    |
| **信息路径长度** | 从第一个到最后一个词路径长度 O(n) | 任意两点间路径长度 O(1)         |
| **捕捉全局语义** | 主要依赖隐藏状态的递归传播       | 每层直接全局建模所有词间关系         |
| **效率与扩展性** | 难以利用 GPU 并行；长序列性能下降 | 计算可完全矩阵化，高效训练；易于扩展到大模型 |
| **记忆能力**   | 隐状态是压缩信息瓶颈          | 多头注意力能显式建模多种依赖关系       |


### 2. 自注意力计算中为何除以 $\sqrt{d_k} $？

随着向量维度 (d_k) 增大：

* $ Q_i $ 和 $ K_j $ 的每个分量大多为随机变量；
* 点积是所有分量乘积的和；
* 因此：
  $$
  \text{Var}(Q_i \cdot K_j^T) \propto d_k
  $$
  → 方差会随维度线性增长。

这意味着：

* 维度大 → 点积的值变得很大；
* 进入 softmax 前的分数过大 → softmax 输出非常尖锐（几乎只有一个值接近 1，其他接近 0）；
* 导致梯度变得非常小（梯度消失），训练不稳定。

### 3. 现在LLM微调的方式有哪些？

1. Lora；
2. Adapter。

### 4. LLM的架构有哪些？

dense、moe

### 5. Prefix LM与Causal LM区别？

### 6. RLHF 是什么？流程如何？

Reinforcement Learning from Human Feedback

1. 监督微调（Supervised Fine-Tuning, SFT）
2. 奖励模型训练（Reward Model, RM）
3. 强化学习优化（RL Fine-Tuning）

### 7. LLM训练中遇到Loss突增如何解决？

1. 训练数据问题
2. 数值 或 梯度问题
3. 学习率 与 优化器超参
4. MoE架构本身问题