这个是为了记录一些常见的激活函数与损失函数的用处，从目的上去了解这些函数。

# 激活函数

## 1.softmax

Softmax ：通常用于**多分类问题的最后一层**，将网络的输出变成一个概率分布。

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
$$

> **常与交叉熵损失函数（Cross Entropy Loss）一起使用**，形成一种组合：**softmax + cross entropy**。

Softmax 本身**不是**损失函数，它是一个**激活函数**（activation function），通常用于**多分类问题的最后一层**，将网络的输出变成一个概率分布。

## 2. ReLU（Rectified Linear Unit）

最常用的激活函数，尤其在卷积神经网络（CNN）中广泛使用。它具有非线性特性，同时计算开销小，有助于缓解梯度消失问题。

$$
f(x) = \max(0, x)
$$

一般用于隐藏层，尤其适合深层网络。

PyTorch:`torch.nn.ReLU()`或`torch.nn.functional.relu()`

## 3. Leaky ReLU

为了解决 ReLU 在 $x < 0$ 时梯度为 0 的问题（“死亡 ReLU”问题），它允许负方向有一个小的斜率。

$$
f(x) = 
\begin{cases}
x & \text{if } x \geq 0 \\
\alpha x & \text{if } x < 0
\end{cases}
\quad \text{（通常 } \alpha = 0.01\text{）}
$$

一般用于深层网络或当 ReLU 导致神经元死亡时。

PyTorch:`torch.nn.LeakyReLU()`或`torch.nn.functional.leaky_relu()`
out = leaky_relu(x)

## 4. Sigmoid

将值压缩到 (0, 1) 区间，常用于二分类的输出层。

$$
f(x) = \frac{1}{1 + e^{-x}}
$$

* 输出概率（二分类）
* 早期深度学习中也用于隐藏层，但容易梯度消失


## 5. Tanh（双曲正切）

压缩到 (-1, 1)，比 sigmoid 更中心对称，对零对称，在某些任务中表现更好。

$$
f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

一般用于隐藏层，但也可能出现梯度消失。

## 6. Softmax

将一个向量转换成概率分布，常用于多分类任务的输出层。

$$
f_i(x) = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$

一般只用于输出层，配合 `CrossEntropyLoss` 不需要显式调用。


## 7. GELU（Gaussian Error Linear Unit）

用于 Transformer 架构（如 BERT、GPT），相比 ReLU 更平滑、表现更好。

$$
f(x) = 0.5x \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right]\right)
$$

一般用于高端模型（Transformer、NLP 模型）默认激活函数。

# 损失函数

## . 交叉熵损失：Cross-Entropy Loss

Cross-Entropy 本质上是衡量两个概率分布之间差异的指标。在分类任务中：

* 真实分布（标签） = ( y )
* 模型预测分布 = ( p )

交叉熵的意义是：“如果真实分布是 (y)，而模型预测为 (p)，那么预测越接近真实分布，损失越小。”

对于每个样本，

* 真实标签$ ( y_i \in {0, 1} ) $
* 模型预测概率$ ( p_i = P(y_i=1|x_i) ) $

损失定义为：

$
L_i = - \Big[ y_i \log(p_i) + (1 - y_i)\log(1 - p_i) \Big]
$

整个 batch 的平均为：

$
L = \frac{1}{N} \sum_{i=1}^{N} L_i
$

举个最直观的例子，假设有一个二分类问题：

| 样本 | 真实标签 (y) | 模型预测匹配概率 (p(y=1)) | 损失                            |
| -- | -------- | ----------------- | ----------------------------- |
| 1  | 1 (正样本)  | 0.95              | 损失 =$ (-\log(0.95)) ≈ 0.05$     |
| 2  | 1 (正样本)  | 0.10              | 损失 =$ (-\log(0.10)) ≈ 2.30$     |
| 3  | 0 (负样本)  | 0.05              | 损失 =$ (-\log(1 - 0.05)) ≈ 0.05$ |
| 4  | 0 (负样本)  | 0.90              | 损失 =$ (-\log(1 - 0.90)) ≈ 2.30$ |

可以看到：
模型预测越接近真实标签（概率越高），$ (-\log(\text{prob})) $越小，损失越低。

**多分类交叉熵（multi-class cross entropy）**

假设有 K 个类别：

* 真实分布是 one-hot 向量$ ( y_i = [0, 0, ..., 1, ..., 0] ) $
* 模型输出 softmax 概率分布$ ( p_i = [p_{i1}, p_{i2}, ..., p_{iK}] ) $

交叉熵定义为：
$
L_i = - \sum_{k=1}^{K} y_{ik} \log(p_{ik})
$

因为只有真实类的$ (y_{ik}=1) $，这等价于：
$
L_i = - \log(p_{i, y_i})
$

也就是「模型对真实类别的预测概率越高，损失越小」。

假设 batch = 2，对应输出：

| 样本 | 标签 (y_i) | 模型输出 logits | softmax 后概率 (p_i=[p_0,p_1]) | 损失 (L_i)              |
| -- | ------------------- | ----------- | --------------------------- | --------------------- |
| 1  | 1 == [0,1] | [0.2, 2.3]  | [0.1, 0.9]                  |$ (-\log(0.9) = 0.105) $|
| 2  | 0 == [1,0] | [1.5, 0.3]  | [0.80, 0.20]                |$ (-\log(0.80) = 0.223) $|

总损失：
$
L = \frac{1}{2}(0.105 + 0.223) = 0.164
$

## . InfoNCE

对比学习中最常用的损失函数之一，用来最大化正样本对之间的相似度、最小化负样本对之间的相似度，目标就是:
$
\text{maximize } \log \frac{\text{similarity between positive pairs}}{\text{similarity between positive + negatives}}
$

假设我们有一个anchor样本$ x_i $，它的正样本是$ x_i^+ $，有 K 个负样本$ x_i^1, x_i^2, …, x_i^K $。

我们用表示函数$ f(\cdot) $将样本映射到特征空间：
$
z_i = f(x_i)
$
并用相似度函数（通常是点积或余弦相似度）来衡量两个向量的接近程度：
$
s(z_i, z_j) = \frac{z_i \cdot z_j}{\tau}
$
其中$ \tau $是温度系数（temperature）。

对单个 anchor 样本 i，InfoNCE 的损失为：
$
L_i = -\log \frac{\exp(s(z_i, z_i^+)/\tau)}{\exp(s(z_i, z_i^+)/\tau) + \sum_{k=1}^{K} \exp(s(z_i, z_k^-)/\tau)}
$
整个 batch 的损失是平均：
$
L = \frac{1}{N} \sum_{i=1}^{N} L_i
$


## . ITC

ITC：Image-Text Contrastive，其实就是有多个图文对，然后 1 图和多文算 InfoNCE，1 文和多图算 InfoNCE。

在一个 batch 中我们有：
$
{(I_1, T_1), (I_2, T_2), \dots, (I_N, T_N)}
$

**图像到文本中**：对每个图像$ ( I_i ) $，我们希望它的正确文本$ ( T_i ) $比其他所有$ ( T_j (j \ne i) ) $相似度更高。

损失：
$
L_{I \to T} = -\frac{1}{N}\sum_{i=1}^N
\log \frac{\exp(s_{ii})}{\sum_{j=1}^N \exp(s_{ij})}
$

也就是：

> 每张图像作为 query，batch 内的所有文本是候选集；
> 正样本：匹配文本 (T_i)，负样本：其他 (T_j)。

**文本到图像中**：对每个文本$ ( T_i ) $，希望它的匹配图像$ ( I_i ) $比其他$ ( I_j(j \ne i) ) $相似度更高。

损失：
$
L_{T \to I} = -\frac{1}{N}\sum_{i=1}^N
\log \frac{\exp(s_{ii})}{\sum_{j=1}^N \exp(s_{ji})}
$

也就是：

> 每段文本作为 query，batch 内的所有图像是候选集；
> 正样本：对应图 (I_i)，负样本：其他 (I_j)。

**双向合并（平均）**

$
L_{ITC} = \frac{1}{2} \big( L_{I \to T} + L_{T \to I} \big)
$

这就是所谓的 ITC

## . ITM

多模态任务（尤其是图文预训练）中常见的的一个**判别式任务**，用于判断：“一张图片和一段文本是否匹配。”

也就是说，给定一对（图像 ( I )，文本 ( T )），模型要输出一个概率：
$
P(\text{match} | I, T)
$
这个概率越高，说明图片与文字语义越一致。

图像与文本首先被编码成向量表示：
$
v_I = \text{ImageEncoder}(I), \quad v_T = \text{TextEncoder}(T)
$

接下来的计算有两种主要方式：

1. **融合式结构（如 ALBEF、BLIP）**
把图像特征和文本特征拼接后送入一个 **cross-modal Transformer**：
$
z = \text{CrossEncoder}([v_I, v_T])
$
然后取 `[CLS]` token 的输出作为整对的语义表示。

2. **非融合式结构（如 CLIP）**
图像与文本分别编码，然后计算相似度（比如点积）来衡量匹配程度：
$
s = v_I^\top v_T
$
不过 CLIP 的这种是 **ITC (Image-Text Contrastive)**，而不是 ITM，但两者通常一起使用。

对于 ITM，最后通常会加一个二分类头：
$
p = \text{Sigmoid}(W z_{\text{[CLS]}} + b)
$
或等价地：
$
\hat{y} = \text{Softmax}(W z_{\text{[CLS]}})
$
其中：

* $ \hat{y} \in {0,1} $：是否匹配；
* $ p $：匹配概率。

使用二分类交叉熵损失：
$
\mathcal{L}_{\text{ITM}} = - [y \log p + (1-y) \log (1-p)]
$
其中：

* $ y=1 $：正样本（图片和文字是配对的）；
* $ y=0 $：负样本（图片与文字不匹配）。

ITM 的关键在于如何选负样本：

* **随机负样本**：随机选择其他图片或文本配对；
* **困难负样本（Hard Negatives）**：通过 ITC 相似度选取最“像真的”的错误配对；
  例如：
  $
  \text{hard negative} = \arg\max_{T' \ne T} \text{sim}(v_I, v_{T'})
  $
  这样可以让模型学会更细腻的语义区分。

ITM 是 **判别任务（discriminative）**，而 ITC（InfoNCE）是 **对比学习（contrastive）任务**。

* **ITC (InfoNCE)**：最大化匹配对的相似度，同时最小化不匹配对；
* **ITM**：显式判断“是否匹配”，类似二分类器。

很多模型（如 BLIP、ALBEF）会**同时使用两者**：
$
\mathcal{L} = \mathcal{L}*{\text{ITC}} + \mathcal{L}*{\text{ITM}}
$
这样模型既能学到全局对齐（ITC），也能学到局部判别（ITM）。


