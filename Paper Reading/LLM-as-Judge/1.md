# *Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena*

## 背景

现有的 Benchmark 已经不足以区分一些模型了，特别是能力较为相近的模型在这些 Benchmark 上完全没有区分度。

## 工作

1. **MT-Bench**：一个多轮对话的数据集，专门用于评估模型在各种能力上的多轮对话表现。
2. **Chatbot Arena**：一个开放式多轮对话的竞技场，可以进行多轮对话，但是论文中使用该平台主要用于单轮对话的打分。
3. **LLM-as-a-Judge**：设计了一个基于规则的程序来评估没有参考答案的开放式问题。

## 不足

1. 多轮对话最多就到两轮；
2. MT-Bench 也只有 80 个任务。