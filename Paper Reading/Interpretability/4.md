# **Improving Dictionary Learning with Gated Sparse Autoencoders**

下文称 Gated SAE

## 0 Abstract

Gated SAE 在训练效果上比目前主流方法实现了 Pareto 改进，其核心洞察是将功能分离为：
1. 确定使用哪些方向； 
2. 估计这些方向的幅值（大小）。

> 这里标记一下，后文会有详细的原理与数学解释。

## 1 Introduction

传统的 SAE（后称 baseline SAE）根据过完备基（overcomplete basis）或字典（dictionary）来寻找模型激活值的稀疏分解。尽管 SAE 在这方面展现了潜力，但在主流训练方法中用于鼓励稀疏性的 $L_{1}$ 惩罚项也引入了偏差，降低了 SAE 重构的准确性，因为模型可以通过牺牲一定的重构准确性来换取更低的 $L_{1}$ 值以降低总损失，这就是收缩偏差（Shrinkage Bias）。

收缩偏差正是由于 $L_{1}$ 惩罚项的“副作用”导致的：
- $L_{1}$ 的初衷： 它是为了让不重要的激活值归零，从而实现稀疏性。
- 产生的问题： 因为 $L_{1}$ 惩罚项是与激活值的绝对值大小成正比的，它在优化过程中不仅会剔除掉不重要的特征，还会 **无差别地“向下拉扯”** 那些真正代表特征的、SAE 需要保留的激活值。
- 结果： 这导致 SAE 输出的特征激活强度系统性地低于其在原始模型中应有的强度。论文中将其形象地描述为：损失函数在“重构误差”和“$L_{1}$ 惩罚”之间做交易——为了让 $L_{1}$ 这一项变小，模型宁愿牺牲一点重构精度，把激活值缩得更小一些。

本论文引入了对基准 SAE 架构的一种改进——门控 SAE（Gated SAE）——以及配套的损失函数，部分克服了上述局限性。Gated SAE 的核心思想是使用独立的仿射变换来分别执行以下任务：
- 确定在重构中使用哪些字典元素； 
- 估计活跃元素的系数（幅值）；并仅对前者应用稀疏性惩罚。在这些变换之间共享一部分权重，以避免与同等宽度的基准 SAE 相比显著增加参数量和推理计算需求。

本研究的主要贡献：
- 引入 Gated SAE：一种修改后的标准 SAE 架构，将“检测哪些特征存在”与“估计其幅值”解耦 ；
- 实现 Pareto 改进：证明了与基准 SAE 相比，Gated SAE 在稀疏性与重构保真度的权衡上表现更优 ；
- 克服收缩问题：确认 Gated SAE 解决了收缩问题，且优于其他旨在解决该问题的现有方法 ；
- 可解释性评估：通过一项小型双盲研究提供证据，证明 Gated SAE 的特征与基准 SAE 的特征在可解释性上不相上下。

![Figure_1](../../images/Interpretability/4_Figure_1.png)

图 1 的详细讲解：
### 1.1 核心结论：Pareto 改进 (Pareto Improvement)

图中的三张小图分别对应 Gemma-7B 模型第 20 层的三个不同位置：**残差流 (Residual stream)**、**MLP 输出 (MLP Output)** 和**注意力层输出 (Attention output)** 。
* **红色曲线 (Gated)** 代表门控 SAE，**蓝色曲线 (Baseline)** 代表基准 SAE 。
* 在所有图表中，红色曲线始终处于蓝色曲线的**左上方** 。这意味着在相同的稀疏度（L0）下，Gated SAE 能恢复更多的模型损失（保真度更高）；或者说，为了达到同样的保真度，Gated SAE 需要激活的特征数量更少 。这种全方位的领先被称为 **Pareto 改进** 。

### 1.2 坐标轴的含义
* **横轴：L0 (Lower is sparser)**
  * 表示平均每个输入样本激活的特征数量 。
  * 数值越低，代表分解越稀疏，可解释性理论上越好 。
  * 注意横轴采用的是**对数刻度 (log-scale)**，范围从 2 到 200 。
* **纵轴：Loss Recovered (Fidelity)**
  * 表示重构保真度，即 SAE 重构的激活值在多大程度上能替代原始激活值而不损失语言模型的性能 。
  * 1 代表完美重构（100% 恢复），0 代表表现等同于零消减（Zero Ablation） 。

### 1.3 实验的公平性：等计算量对比 (Equal Compute)

图中特别注明，这些 SAE 是在**计算量相等**的前提下进行对比的 。
* 由于 Gated SAE 的训练损失计算多出了一个辅助项，其单步训练成本比基准 SAE 高出约 50% 。
* 为了公平竞争，研究人员让基准 SAE 拥有了 **1.5 倍的特征数量（宽度）** 。
* 即便在基准 SAE 拥有更多参数和特征的情况下，Gated SAE 的表现依然显著优于它 。

### 1.4 具体表现亮点
* **效率提升**：论文指出，在典型的超参数范围内，Gated SAE 只需要大约**一半数量**的激活特征就能达到与基准 SAE 相当的重构精度 。
* **一致性**：这种性能提升不仅存在于图中展示的 Gemma-7B 第 20 层，在 GELU-1L、Pythia-2.8B 以及 Gemma-7B 的其他各层中都得到了验证 。

## 2 Sparse Autoencoder Background

现有 SAE 架构和训练方法（即 baseline SAE）所需的各种概念和符号。将在第 3.2 节中定义 **Gated SAEs** 。作者遵循的符号体系与 Bricken 等人 (2023) 大致相似，并建议将该作品作为在语言模型（LMs）上训练 SAE 的更完整介绍 。

SAE 希望将模型的激活值  分解为特征方向的稀疏线性组合 ：
$$x \approx x_0 + \sum_{i=1}^{M} f_i(x) d_i, \quad (1)$$

其中 $d_i$ 是 $M \gg n$ 个潜单元范数（unit-norm）特征方向，$f_i(x) \ge 0$ 是 $x$ 对应的稀疏特征激活值。公式 (1) 的右侧自然具有自编码器的结构：输入激活值 $x$ 被编码为一个（稀疏的）特征激活向量 $f(x) \in \mathbb{R}^M$，接着该向量被线性解码以重构 $x$。

### 公式解读

1. $x$：待分解的激活值
   - 含义：这是语言模型（LM）在处理某个文本时，某一层神经元的输出（比如残差流、MLP层的输出）。
   - 痛点：$x$ 是一个高维向量，里面的每个数字（神经元）通常是多义性的（一个神经元可能同时代表“医学”和“德语”两个无关概念），人类直接看不懂。
2. $d_i$：字典方向 (Dictionary Directions / Features)
   - 含义：这些是 SAE 学习到的 $M$ 个单位向量。每个 $d_i$ 代表一个单一的、可解释的特征（比如“代词的概念”、“关于法律的讨论”等）。
   - 关键点：$M \gg n$（过完备）。这意味着我们相信模型在有限的维度 $n$ 里，通过“叠加”存储了远多于 $n$ 个的特征。这些 $d_i$ 就是试图把这些被挤压在一起的特征重新找出来。
3. $f_i(x)$：特征激活值 (Feature Activations)
   - 含义：这是一个标量数字，表示在当前的输入 $x$ 中，特征 $d_i$ 出现了多少。
   - 稀疏性（Sparsity）：这是 SAE 的灵魂。虽然我们有几万个特征（$d_i$），但在处理某一个特定单词时，绝大多数 $f_i(x)$ 必须为 0。只有极少数（比如几十个）相关的特征会被激活。
   - 线性组合：$\sum f_i(x) d_i$ 的意思就是：我们把这些被激活的特征，按照各自的强度（$f_i$）加权累加起来。
4. $x_0$：中心化项/偏置 (Bias term)
   - 含义：在代码实现中，这通常对应解码器的偏置 $b_{dec}$。
   - 作用：它代表了激活值的“底噪”或平均水平。我们先减去这个平均值，再用特征去解释剩下的差异部分。

所以这个过程其实就是学习 $f_i()$ 这个规则，即：高维向量 $x$ 到 每一个单一字典方向 $d_i$ 的映射关系。

### 2.1 基准架构 (Baseline Architecture) 

利用这种对应关系，Bricken 等人 (2023) 及后续工作尝试通过参数化定义如下的单层自编码器 $(f, \hat{x})$ 来学习合适的稀疏分解：

$$f(x) := ReLU(W_{enc}(x - b_{dec}) + b_{enc}) \quad (2)$$

$$\hat{x}(f) := W_{dec}f + b_{dec} \quad (3)$$

并在训练过程（第 2.2 节）中使其能够重构来自大规模数据集的模型激活值 $x \sim \mathcal{D}$，同时约束隐藏层表示 $f$ 为稀疏的 13。一旦稀疏自编码器训练完成，我们可以通过以下方式获得公式 (1) 形式的分解：将解码器权重矩阵 $W_{dec} \in \mathbb{R}^{n \times M}$ 的（经过适当归一化的）列识别为特征方向字典 $d_i$，将解码器偏置 $b_{dec} \in \mathbb{R}^n$ 识别为中心化项 $x_0$，并将潜表示 $f(x) \in \mathbb{R}^M$ 的（经过适当归一化的）条目识别为特征激活值 $f_i(x)$。

### 2.2 基准训练方法 (Baseline Training Methodology) 

为了训练稀疏自编码器，Bricken 等人 (2023) 使用了一个共同鼓励 (i) 重构准确性和 (ii) 稀疏性的损失函数。重构准确性由 SAE 输入与其重构值之间的平方距离 $\|x - \hat{x}(f(x))\|_2^2$ 鼓励，称之为重构损失 (reconstruction loss)；而稀疏性则由活跃特征的 $L_1$ 范数 $\|f(x)\|_1$ 鼓励，称之为稀疏惩罚 (sparsity penalty)。使用 $L_1$ 系数 $\lambda$ 平衡这两项，优化 SAE 的总损失由下式给出：

$$\mathcal{L}(x) := \|x - \hat{x}(f(x))\|_2^2 + \lambda \|f(x)\|_1 \quad (4)$$

由于可以通过简单地缩小编码器输出并放大解码器权重范数，在不影响重构或稀疏度的情况下任意降低稀疏损失项，因此在训练期间约束 $W_{dec}$ 的列范数至关重要。遵循 Bricken 等人 (2023) 的做法，作者将列约束为精确的单位范数（unit norm）。

### 2.3 评估 (Evaluation)

为了直观了解训练后的 SAE 质量，作者使用了 Bricken 等人 (2023) 的两个指标：**L0（衡量 SAE 稀疏性）** 和 **恢复损失 (loss recovered)**（衡量 SAE 重构忠实度）：

- L0：定义为给定输入下活跃特征的平均数量，即 $\mathbb{E}_{\mathbf{x}\sim\mathcal{D}} \Vert \mathbf{f}(\mathbf{x}) \Vert_0$。
- 恢复损失：通过将 SAE 的重构结果剪接入语言模型（LM），计算模型在评估数据集上的平均交叉熵损失得出。如果我们用 $CE(\phi)$ 表示在模型前向传播中将 SAE 所在位置替换为函数 $\phi : \mathbb{R}^n \rightarrow \mathbb{R}^n$ 后的平均损失，则恢复损失为：
$$1 - \frac{CE(\mathbf{\hat{x}} \circ \mathbf{f}) - CE(Id)}{CE(\zeta) - CE(Id)} \tag{5}$$
其中 $\mathbf{\hat{x}} \circ \mathbf{f}$ 是自编码器函数，$\zeta : \mathbf{x} \mapsto \mathbf{0}$ 是零消融函数（zero-ablation function），而 $Id : \mathbf{x} \mapsto \mathbf{x}$ 是恒等函数。根据此定义，始终输出零向量进行重构的 SAE 的恢复损失为 0%，而能够完美重构输入的 SAE 的恢复损失为 100%。

> 三条注释：
> 注释 3: 在这项工作中，作者仅在 SAE 学习到的特征背景下使用“特征”一词，即通过线性组合产生重构结果的过完备基方向。特别地，学习到的特征始终是线性的，且不一定具有可解释性，这避开了定义“什么是特征”的困难。
> 注释 4: 模型激活通常取自特定的层和位置，例如第 17 层 MLP 部分的输出。
> 注释 5: 请注意，作者无法直接优化 $L0$ 范数（即活跃特征的数量），因为它不是可微函数。但作者确实使用 $L0$ 范数来评估 SAE 的稀疏性。

![Figure_2](../../images/Interpretability/4_Figure_2.png)
稀疏自编码器中的 $L1$ 惩罚会导致 **收缩 (shrinkage)** 现象——即便能够实现完美重构，重构结果也会偏向于更小的范数。
图中展示了两条曲线。红色虚线是纯重构损失 $(\hat{x}-1)^2$，其最小值在 $1$ 处；黑色虚线是加入了 $L1$ 惩罚后的总损失 $(\hat{x}-1)^2 + |\hat{x}|$。可以看到，加入惩罚后，总损失的最低点（最优重构值 $\hat{x}^*$）被“拉”向了 $0$ 的方向（即 $1/2$ 处）

## 3 门控稀疏自编码器 (Gated SAEs)

### 3.1 动机 (Motivation)

训练 SAE 的直观想法是在给定的稀疏度水平（由 $L0$ 衡量）下最大化重构忠实度，但在实践中，我们优化的是重构忠实度与 $L1$ 正则化的混合体。这种差异是稀疏自编码器训练中不必要偏差的来源：对于任何固定的稀疏度水平，训练后的 SAE 可以通过牺牲一点重构忠实度来换取更好的 $L1$ 稀疏惩罚表现，从而获得更低的（公式 (4) 中定义的）总损失。

这种偏差最明显的后果就是收缩 (shrinkage)。在保持解码器 $\hat{x}(\bullet)$ 固定不变的情况下，$L1$ 惩罚项会将特征激活值 $f(x)$ 推向零，而重构损失则会将 $f(x)$ 推向足够高的数值以产生准确的重构。因此，最优值落在两者之间，这意味着它系统性地低估了特征激活的幅度，且这种做法对于提升稀疏性并没有任何必然的补偿性收益。

我们该如何减少 $L1$ 惩罚项引入的偏差呢？基准 SAE 编码器 $f(x)$ 的输出承担了两个角色：
1. **检测哪些特征是活跃的**（根据输出是零还是严格正值来判断）。对于这个角色，$L1$ 惩罚是必要的，用以确保分解是稀疏的。
2. **估计活跃特征的幅度**。对于这个角色，$L1$ 惩罚则是一个不必要的偏差来源。

### 3.2 门控稀疏自编码器 (Gated SAEs)

#### 3.2.1 架构 (Architecture)

如何修改基准 SAE 编码器以实现这种权责分离？Gated SAE 的方案是将基准 SAE 的单层 ReLU 编码器替换为门控 ReLU 编码器 (Gated ReLU encoder)。受门控线性单元（Gated Linear Units）的启发，Gated SAE 将门控编码器定义如下：

$$\tilde{f}(x) := \underbrace{\mathbb{I}[\overbrace{(W_{gate}(x - b_{dec}) + b_{gate})}^{\pi_{gate}(x)} > 0]}_{f_{gate}(x)} \odot \underbrace{\text{ReLU}(W_{mag}(x - b_{dec}) + b_{mag})}_{f_{mag}(x)} \tag{6}$$

其中 $\mathbb{I}[\bullet > 0]$ 是（逐点）单位阶跃函数（Heaviside step function），$\odot$ 表示逐元素相乘。在这里，$f_{gate}$ 决定哪些特征被视为活跃，而 $f_{mag}$ 估计特征激活的幅度（仅对被视为活跃的特征有意义）；$\pi_{gate}(x)$ 是 $f_{gate}$ 子层的预激活值，用于下文定义的门控 SAE 损失函数。

从表面上看，我们似乎使编码器的参数量翻倍了，导致总参数量增加了 50%。我们通过权重共享 (weight sharing) 来缓解这一问题：我们对这些层进行参数化，使这两层共享相同的投影方向，但允许这些方向的范数以及层偏置有所不同。具体而言，我们根据 $W_{gate}$ 和一个额外的向量值重缩放参数 $r_{mag} \in \mathbb{R}^M$ 来定义 $W_{mag}$：

$$(W_{mag})_{ij} := (\exp(r_{mag}))_i \cdot (W_{gate})_{ij} \tag{7}$$

通过这种权重绑定方案，门控 SAE 仅比基准 SAE 多出 $2 \times M$ 个参数。

在权重绑定的情况下，门控编码器可以重新解释为带有非标准且不连续的 “跳变 ReLU” (Jump ReLU) 激活函数的单层线性编码器 $\sigma_\theta(z)$。准确地说，利用公式 (7) 的权重绑定方案，$\tilde{f}(x)$ 可以重新表达为 $\tilde{f}(x) = \sigma_\theta(W_{mag} \cdot x + b_{mag})$，其跳变间隙由 $\theta = b_{mag} - e^{r_{mag}} \odot b_{gate}$ 给出。

> 注释 7:仅仅对收缩后的特征激活进行简单的重缩放（如某些研究提出的后续修正方法）并不足以克服 $L1$ 惩罚带来的偏差。原因在于，使用 $L1$ 惩罚训练的 SAE 可能已经学习到了次优 (sub-optimal) 的编码器和解码器方向。即便你在事后调整了数值的大小，这些“歪了”的方向本身并没有得到修正。论文在 5.2 节提供了实验证据，证明 Gated SAE 学习到了比“基础模型+事后修正”更好的方向。 

![Figure_3](../../images/Interpretability/4_Figure_3.png)

**Gated SAE 架构**

这张图直观展示了 Gated SAE 的运行机制：
- 输入 $x$ 经过编码器权重 $W_{enc}$ 后，分成了两条路。
- 上方（蓝色）是幅度路径 (Magnitude Path)：负责计算如果特征存在，它的强度应该是多少。它包含一个重缩放/位移层和 ReLU 激活。
- 下方（红色）是门控路径 (Gating Path)：负责做“是/否”的判定。它将预激活值与 $0$ 比较，产生一个二值化的（0 或 1）掩码。
- 结合：两者进行逐元素相乘（即图中圆圈中间有一个点的符号 $\odot$）。如果门控路径判定为 $0$，那么无论幅度路径算出多少，最终结果都是 $0$。
- 权重共享：图中用同样的 $W_{enc}$ 框表示它们共享投影方向，体现了公式 (7) 的低参数量设计。

![Figure_4](../../images/Interpretability/4_Figure_4.png)

**Jump ReLU 激活函数**

这张图展示了 Gated SAE 在数学本质上的行为：
- 不连续性：传统的 ReLU 在 $0$ 点是连续的，而 Jump ReLU 在阈值 $\theta$ 处有一个明显的断层（跳变）。
- 物理意义：这意味着特征不会从极小的数值慢慢开启，而是一旦“越过门槛”（门控路径判定为活跃），它就会立即以一个显著的起始幅度激活。
- 解决收缩：这种机制允许模型设置一个较高的“进入门槛”来过滤噪声（保持稀疏），同时一旦进入，就能保持原本的幅度，而不会像 $L1$ 惩罚那样把数值往 $0$ 拽。

#### 3.2.2 训练门控 SAE (Training Gated SAEs)

训练门控 SAE 的一个自然想法是应用等式 (4)，同时将稀疏性惩罚仅限制在 $f_{gate}$ 上：

$$\mathcal{L}_{incorrect}(\mathbf{x}) := \underbrace{\Vert \mathbf{x} - \hat{\mathbf{x}}(\tilde{f}(\mathbf{x})) \Vert_2^2}_{\mathcal{L}_{reconstruct}} + \underbrace{\lambda \Vert f_{gate}(\mathbf{x}) \Vert_1}_{\mathcal{L}_{sparsity}}$$

不幸的是，由于 $f_{gate}$ 中使用了单位阶跃激活函数（Heaviside step function），梯度将无法传播到 $W_{gate}$ 和 $b_{gate}$。为了减轻稀疏惩罚项面临的这一问题，作者改为对预激活值的正部分应用 $L1$ 范数，即 $\text{ReLU}(\pi_{gate}(\mathbf{x}))$。为了确保 $f_{gate}$ 能通过检测活跃特征来辅助重构，Gated SAE 添加了一个辅助任务 (auxiliary task)，要求这些经过修正的预激活值也能被解码器用于产生良好的重构结果：

$$\mathcal{L}_{gated}(\mathbf{x}) := \underbrace{\Vert \mathbf{x} - \hat{\mathbf{x}}(\tilde{f}(\mathbf{x})) \Vert_2^2}_{\mathcal{L}_{reconstruct}} + \underbrace{\lambda \Vert \text{ReLU}(\pi_{gate}(\mathbf{x})) \Vert_1}_{\mathcal{L}_{sparsity}} + \underbrace{\Vert \mathbf{x} - \hat{\mathbf{x}}_{frozen}(\text{ReLU}(\pi_{gate}(\mathbf{x}))) \Vert_2^2}_{\mathcal{L}_{aux}} \tag{8}$$

其中 $\hat{\mathbf{x}}_{frozen}$ 是解码器的冻结副本，$\hat{\mathbf{x}}_{frozen}(f) := W_{dec}^{copy}f + b_{dec}^{copy}$，以确保来自 $\mathcal{L}_{aux}$ 的梯度不会传回给 $W_{dec}$ 或 $b_{dec}$。这通常可以通过停止梯度 (stop gradient) 操作来实现，而不需要真的创建副本——参见附录 G 中关于前向传播和损失函数的伪代码。

计算该损失（或其梯度）时，Gated SAE 需要运行两次解码器：一次用于执行 $\mathcal{L}_{reconstruct}$ 的主重构，另一次用于执行 $\mathcal{L}_{aux}$ 的辅助重构。这导致执行单个训练更新步骤所需的计算量增加了 50%。然而，总体训练时间的增加通常要少得多，因为根据作者的经验，大部分训练耗时都花在了在线生成语言模型激活值或磁盘 I/O（如果是在保存的激活值上训练）上。

## 4 评估 (Evaluation)

在本节中，作者将在多种模型和不同的位置上对门控 SAE (Gated SAEs) 进行基准测试（第 4.1 节）；展示它们解决了收缩问题（第 4.2 节）；并根据专家人类评分者的评估，展示它们产生的特征在可解释性上与基准 SAE (Baseline SAE) 的特征相当，尽管作者目前无法确凿地断定其中一种是否优于另一种（第 4.3 节）。

### 4.1 全面基准测试 (Comprehensive Benchmarking)

原文翻译在本小节中，作者展示了门控 SAE（Gated SAEs）在“恢复损失”（loss recovered）和 $L0$ 指标上相比基准 SAE（baseline SAEs）实现了帕累托改进 (Pareto improvement)。作者通过评估针对以下内容重构训练的 SAE 来证明这一点：
1. GELU-1L 模型中的 MLP 神经元激活：这是与 Bricken 等人 (2023) 研究最直接的对比；
2. Pythia-2.8B 模型（5 个不同层）和 Gemma-7B 基础模型（4 个不同层）中的 MLP 输出、注意力层输出（采样位置在 $W_O$ 之前，参考 Kissane 等人 (2024a)）以及残差流激活。

在这两组实验中，作者通过改变训练 SAE 时使用的 $L1$ 系数 $\lambda$，从而能够比较门控 SAE 与基准 SAE 在 $L0$ 和恢复损失之间的帕累托前沿 (Pareto frontiers)。

门控 SAE 的训练计算量最多是普通 SAE 的 1.5 倍。因此，为了确保评估中的公平比较，作者将门控 SAE 与多出 50% 学习特征（即更宽）的基准 SAE 进行对比。作者在图 5 中展示了 GELU-1L 的结果，在附录 B 中展示了 Pythia-2.8B 和 Gemma-7B 的结果。在附录 B（图 12）中，在所有测试的位置上，门控 SAE 均表现出对普通 SAE 的帕累托改进。在图 12 和图 13 的某些情况下，帕累托前沿出现了非单调的情况，作者将此归因于训练 SAE 时的固有困难。

> 1. 什么是“帕累托改进” (Pareto Improvement)？
> 在 SAE 的语境下，我们有两个互相矛盾的目标：
>  - 稀疏性 ($L0$)：活跃的特征越少越好。
>  - 保真度 (Loss Recovered)：重构的效果越接近原始模型越好。
> 帕累托改进意味着：相比于基准模型，Gated SAE 在不增加特征数量的情况下提高了准确率，或者在保持准确率不变的情况下减少了特征数量。简单来说，它全面超越了旧方法。
>
> 2. 为什么要对比“多出 50% 特征”的基准 SAE？
> 这是一个非常严谨的科研对比逻辑：
> - 成本计算：因为 Gated SAE 在训练时由于要跑两次解码器，多花了 50% 的计算量。
> - 公平竞赛：为了抵消这个“计算量优势”，作者让基准 SAE 的规模（特征数量 $M$）也增加 50%。
> - 结论的含金量：即便在基准 SAE 变宽、变强之后，Gated SAE 依然能在性能上胜过它。这证明了性能的提升来自于架构的优越性，而不是单纯靠增加计算资源。

![Figure_5](../../images/Interpretability/4_Figure_5.png)
Figure 5：整体性能的“帕累托改进”

这张图展示了在 GELU-1L 模型（一个单层语言模型）的神经元激活上，Gated SAE 与基准（Baseline）SAE 的性能对比。
- 坐标轴含义：
  - 横轴 (L0)： 代表特征的稀疏度（越往左越稀疏，激活的特征越少）。在可解释性研究中，我们希望 L0 越低越好。
  - 纵轴 (Loss Recovered / Fidelity)： 代表重构保真度（越往上重构越准）。1.0 代表完美重构了原模型的损失。
- 曲线对比：
  - 红色曲线 (Gated)： 代表门控 SAE。
  - 蓝色曲线 (Baseline)： 代表传统的基准 SAE（为了公平对比，这里的基准模型特征数多出 50%，即宽度是 1.5 倍）。
- 核心结论：
  - 红色曲线始终在蓝色曲线的左上方。这意味着在任何给定的稀疏度（L0）下，Gated SAE 的重构效果都更好。
  - 这种现象被称为帕累托改进 (Pareto improvement)，证明了 Gated SAE 的架构设计在处理“稀疏性”与“准确性”的矛盾时更具效率。

### 4.2 收缩现象 (Shrinkage)

正如第 3.1 节所述，用于训练基准 SAE 的 $L1$ 稀疏惩罚会导致特征激活被系统性地低估，这种现象被称为收缩 (shrinkage)。由于这反过来又会缩小 SAE 解码器生成的重构结果，我们可以通过测量其重构结果的平均范数，来观察受训 SAE 受收缩影响的程度。

具体而言，我们使用的指标是相对重构偏差 (relative reconstruction bias) $\gamma$：

$$\gamma := \arg \min_{\gamma'} \mathbb{E}_{x \sim \mathcal{D}} \left[ \left\| \hat{x}_{SAE}(x)/\gamma' - x \right\|_2^2 \right] \tag{9}$$

即 $\gamma^{-1}$ 是为了使 $L2$ 重构损失最小化，SAE 重构结果应该缩放的最优乘法因子。对于无偏的 SAE，$\gamma = 1$；当存在收缩时，$\gamma < 1$。通过显式求解等式 (9) 中的优化问题，相对重构偏差可以用 SAE 平均重构损失、输入激活值的均方范数以及 SAE 重构结果的均方范数来解析地表达，这使得 $\gamma$ 在训练期间易于计算和跟踪：

$$\gamma = \frac{\mathbb{E}_{x \sim \mathcal{D}} [ \| \hat{x}_{SAE}(x) \|_2^2 ]}{\mathbb{E}_{x \sim \mathcal{D}} [ \hat{x}_{SAE}(x) \cdot x ]} = \frac{2\mathbb{E}_{x \sim \mathcal{D}} [ \| \hat{x}_{SAE}(x) \|_2^2 ]}{\mathbb{E}_{x \sim \mathcal{D}} [ \| \hat{x}_{SAE}(x) \|_2^2 ] + \mathbb{E}_{x \sim \mathcal{D}} [ \| x \|_2^2 ] - \mathbb{E}_{x \sim \mathcal{D}} [ \| \hat{x}_{SAE}(x) - x \|_2^2 ]} \tag{10}$$

如图 6 所示，门控 SAE 的重构是无偏的，其 $\gamma \approx 1$，而基准 SAE 则表现出收缩现象 ($\gamma < 1$)，且收缩的影响随着 $L1$ 系数 $\lambda$ 的增加（以及随之而来的 $L0$ 减少）而变得更加严重。在附录 C 中，我们展示了这一结果可以推广到 Pythia-2.8B 模型。

> 数学原理讲解：什么是 $\gamma$？
> 作者定义这个 $\gamma$ 是为了量化模型“缩水”了多少。
> - 直观理解：如果模型重构出来的向量方向是对的，但是长度都变短了，那么 $\gamma$ 就会小于 1。
> - 补偿因子：$\gamma^{-1}$ 就是“补救系数”。比如模型重构出的值是 0.8，但真实值是 1.0，那么 $\gamma = 0.8$，你需要乘上 $1/0.8 = 1.25$ 才能还原 1。
> - 无偏状态：Gated SAE 的设计目标就是让 $\gamma$ 尽可能接近 1，意味着它重构出的特征强度就是它本该有的强度，没有被 $L1$ “拽走”。

![Figure_6](../../images/Interpretability/4_Figure_6.png)
Figure 6：精准消除“收缩现象” (Shrinkage)

这张图解释了为什么 Gated SAE 性能更好——因为它解决了传统 SAE 的一个系统性偏差：收缩问题。
- 坐标轴含义：
  - 横轴 (L0)： 同样代表稀疏度。
  - 纵轴 (Relative reconstruction bias $\gamma$)： 这是衡量重构偏差的指标。
    - $\gamma = 1$：代表无偏（重构结果的数值大小是准确的）。
    - $\gamma < 1$：代表收缩（重构出来的特征数值被系统性地拉低/变小了）。
- 曲线对比：
  - 红色曲线 (Gated)： 几乎是一条水平线，始终紧贴 1.0。这证明门控架构在提取特征时，给出的激活强度是准确的。
  - 蓝色曲线 (Baseline)： 随着 L0 减小（稀疏惩罚增大），曲线剧烈下滑。这说明传统 SAE 为了让模型变稀疏，不得不牺牲数值准确性，强行把特征激活值往零的方向拽。
- 核心结论：
  - Gated SAE 彻底解决了收缩问题。
  - 这是因为 Gated SAE 把“判定特征是否存在（开关）”和“计算特征强度（幅度）”分开了，只有开关部分受 L1 惩罚，而幅度部分不受影响，从而保留了最真实的特征强度。

> 注释 8：作者这样定义 $\gamma$ 是为了让 $\gamma < 1$ 在直觉上能对应“收缩”现象。
> 注释 9：第二个等号利用了恒等式 $2a \cdot b \equiv \|a\|_2^2 + \|b\|_2^2 - \|a - b\|_2^2$。请注意，一个无偏的重构（$\gamma = 1$）因此必须满足：
> $$\mathbb{E}_{x \sim \mathcal{D}} [ \| \hat{x}_{SAE}(x) \|_2^2 ] = \mathbb{E}_{x \sim \mathcal{D}} [ \| x \|_2^2 ] - \mathbb{E}_{x \sim \mathcal{D}} [ \| \hat{x}_{SAE}(x) - x \|_2^2 ]$$
> 换句话说，一个无偏但重构不完美的 SAE（即重构损失不为零的 SAE），即使在没有收缩的情况下，其重构结果的均方范数也必然严格小于输入激活值的均方范数。而收缩现象会使重构结果的均方范数变得更小。

### 4.3 手工可解释性评分 (Manual Interpretability Scores)

#### 4.3.1 实验方法 (Experimental Methodology)

虽然作者相信上文调查的各项指标传达了关于 SAE 质量的有意义信息，但它们仅仅是不完美的代理指标。截至目前，对于如何衡量学习到的特征在多大程度上具有“可解释性”，尚未达成共识。为了对学习到的字典特征之间的差异获得更定性的理解，作者进行了一项双盲人类评分者实验，对一组随机抽样的特征的可解释性进行了评分。

作者研究了来自不同层和位置的多种 SAE。
- 对于 Pythia-2.8B：有 5 名评分者，每人对图 12 中每个（位置，层）配对生成的基准 SAE 和门控 SAE 特征各评定一个，总计 150 个特征。
- 对于 Gemma-7B：有 7 名评分者；其中一人对每个（位置，层）配对评定 2 个特征，其余人各评定 1 个特征，特征来自图 13 中相应的基准或门控 SAE，总计 192 个特征。

在这两种情况下，特征以随机顺序展示给评分者，且不透露其来自哪个 SAE、位置或层。为了评估一个特征，评分者需要判断该特征的行为是否存在一种解释，特别是针对其最高激活样本的解释。随后，评分者输入该解释（如果适用），并选择该特征是：
- 可解释的（'Yes'）
- 不可解释的（'No'）
- 可能可解释的（'Maybe'）

作者使用开源的 SAE 可视化库作为实验界面（McDougall, 2024）。

#### 4.3.2 统计分析 (Statistical Analysis) 

为了测试门控 SAE（Gated SAEs）是否更具可解释性并估计其差异，我们将数据点根据所有协变量（模型、层、位置、评分者）进行配对（pairing）；这使我们能够在不进行任何参数假设的情况下控制所有变量，从而减少比较中的方差。我们使用单侧配对 Wilcoxon-Pratt 符号秩检验，并为基准（Baseline）和门控（Gated）标签之间的平均差异提供 90% BCa 自助法（bootstrap）置信区间，其中我们将“No”计为 0，“Maybe”计为 1，“Yes”计为 2。总体而言，针对“门控 SAE 的可解释性至多与基准 SAE 相当”这一原假设的检验得出的 $p$ 值为 0.060（估计值为 0.13，平均差异置信区间为 [0, 0.26]）。

具体细分如下：
- 在 Pythia-2.8B 数据上，$p = 0.15$（平均差异置信区间为 [−.07, .33]）。
- 在 Gemma-7B 数据上，$p = 0.13$（平均差异置信区间为 [−.04, .29]）。

对标签差异进行的 Mann-Whitney U 秩检验（比较两个模型的结果）未能拒绝（$p = 0.95$）“它们来自相同分布”的原假设；直接对标签进行的相同检验同样未能拒绝（$p = 0.84$）“它们整体上具有相似可解释性”的原假设。
用于这些结果的列联表（contingency tables）如图 7 所示。总体结论是，虽然我们不能断定门控 SAE 特征比基准 SAE 特征更具可解释性，但它们至少是相当的。我们在附录 H 中提供了更多按位置和层细分的分析。

![Figure_7](../../images/Interpretability/4_Figure_7.png)
Figure 7：展示了门控稀疏自编码器（Gated SAE）与基准 SAE（Baseline SAE）在特征可解释性方面的双盲人工评分对比结果。

这张图是由两张 **列联表（Contingency Tables）** 组成的，分别针对两个不同的语言模型：Pythia-2.8B 和 Gemma-7B。

1. 坐标轴含义
   - 横轴 (Baseline)：人类评分者对基准 SAE 特征给出的评分。
   - 纵轴 (Gated)：人类评分者对门控 SAE 特征给出的评分。
   - 评分等级：
     - YES：特征是可解释的。
     - MAYBE：特征可能具有可解释性。
     - NO：特征不可解释。
2. 如何阅读表中的数字？
表中的每个数字代表了配对测试中的样本数量。由于实验是“配对”进行的（即评分者同时评价来自同一模型、同一层、同一位置但不同 SAE 架构的特征），我们可以通过观察格子的分布来判断优劣：
   - 对角线（左下到右上）：代表两种 SAE 得到的评价一致。例如，在 Gemma-7B 表中，有 40 个特征在两种架构下都被评为“YES”。
   - 左上方区域（对角线以上）：代表 Gated SAE 表现更好。例如，在 Pythia-2.8B 表中，有 9 个样本在 Baseline 下被评为“MAYBE”，但在 Gated 下被评为“YES”。
   - 右下方区域（对角线以下）：代表 Baseline SAE 表现更好。例如，在 Gemma-7B 表中，有 12 个样本在 Baseline 下被评为“YES”，但在 Gated 下被评为“MAYBE”。
3. 核心统计结论
通过这些数据，作者进行了统计学分析（Wilcoxon-Pratt 符号秩检验），得出了以下结论：
   - 性能相当：总体的 $p$ 值为 .060。在统计学上，通常 $p < .05$ 被认为具有显著差异。因为该值略高于 .05，所以不能断定 Gated SAE 绝对比 Baseline 更好懂。
   - 没有退步：虽然 Gated SAE 的重构保真度（数学表现）大幅提升，但人工评分证明其可解释性至少与基准方法处于同一水平。
   - 优势倾向：从热力图颜色深度和数字分布来看，Gated SAE 在“YES”这一行（纵轴最上方）的数字普遍较大，说明它倾向于产生更多高质量、可理解的特征。

## 5 为什么 Gated SAE 能改善 SAE 的训练？ (Why do Gated SAEs improve SAE training?)

在本节中，作者通过消融研究 (Ablation Study) 深入分析了 Gated SAE 训练中的关键组成部分，并将其与另一种解决收缩问题的相关方法进行了基准测试。

### 5.1. 消融研究

在本节中，作者改变了门控 SAE 训练方法（第 3.2 节）的几个部分，以深入了解哪些训练环节是观察到的性能提升所必需的。由于门控 SAE 在多个方面与基准 SAE 不同，如果没有严谨的消融研究，很容易将性能增益错误地归因于一些无关紧要的细节。图 8 展示了这些变体的帕累托前沿，下面我们将依次描述每个变体并讨论我们对结果的解释。

1. 取消冻结解码器 (Unfreeze decoder)：作者在辅助损失 $\mathcal{L}_{aux}$ 中取消对解码器权重的冻结——即允许这一辅助任务在训练 $f_{gate}$ 参数的同时更新解码器权重。虽然这（略微）简化了损失函数，但性能出现了下降。这支持了以下假设：将 $L1$ 稀疏惩罚的影响限制在 SAE 中真正需要的参数上（即用于检测特征是否活跃的参数）是有益的。
2. 移除 $r_{mag}$ (No $r_{mag}$)：作者移除公式 (7) 中的 $r_{mag}$ 缩放参数，将其有效设置为零（即乘数为 $e^0 = 1$）；这进一步绑定了 $f_{gate}$ 和 $f_{mag}$ 的参数。在这种变化下，两个编码器子层的预激活值最多只能有一个逐元素的位移差异。实验观察到性能有轻微下降，这表明 $r_{mag}$ 对门控 SAE 的性能提升有一定贡献，但并非决定性的。
3. 非绑定编码器 (Untied encoders)：作者通过训练门控参数和 ReLU 编码器参数完全独立的门控 SAE，来检查“在两个编码器之间共享大部分参数”的选择是否显著损害了性能。尽管非绑定编码器具有更强的表达能力，但我们没有观察到性能提升——事实上反而略有恶化。这表明我们的绑定方案（公式 7）——即共享编码器方向但保持幅度和偏置独立——能有效捕捉门控 SAE 的优势，同时避免了非绑定 SAE 带来的 50% 参数量增长和推理计算开销。

> 作者通过这三个实验，有力地反驳了“Gated SAE 只是靠增加参数量或复杂度取胜”的质疑。
> - 为什么要冻结解码器？
>     - 辅助任务 $\mathcal{L}_{aux}$ 的初衷是“教 $W_{gate}$ 认路”。如果你不冻结解码器，模型可能会为了偷懒，通过修改解码器来迎合那些还没练好的门控信号。
>     - 结果证明：保持解码器的“纯洁性”（只由主任务训练）对于学习到高质量特征至关重要。
> - 为什么要保留 $r_{mag}$？
>     - 虽然 $W_{gate}$ 和 $W_{mag}$ 共享方向，但它们在不同路径下的信号强度（模长）可能不同。$r_{mag}$ 给了模型一点点灵活性去调整这两个路径的比例。
>     - 结果证明：这点灵活性是值得的，它带来了帕累托前沿的进一步提升。
> - 为什么“权重共享”反而比“完全独立”更好？
>     - 这是一个很有趣的发现。按理说参数越多（完全独立）表达能力越强，但实验结果却是共享权重的效果更好。
>     - 逻辑解释：这可能是因为权重共享起到了一种正则化 (Regularization) 的作用，强制“开关”和“幅度”必须在同一个方向上达成共识，防止模型学习到互不相关的乱七八糟的方向。

![Figure 8](../../images/Interpretability/4_Figure_8.png)

Figure 8:展示了在 GELU-1L 语言模型的 MLP 神经元激活上进行的**消融研究（Ablation Study）结果**。该图通过帕累托前沿（Pareto frontiers）对比了 Gated SAE 及其几种变体在重构保真度（Loss Recovered）与稀疏度（L0）**之间的权衡表现。

以下是图中各曲线代表的含义及结论：
- 冻结解码器的重要性（Ablation: unfreeze decoder）：
  - 实验内容：在辅助损失函数 $\mathcal{L}_{aux}$ 中取消对解码器权重的冻结，允许该任务同时更新解码器和门控编码器参数。
  - 结果：性能（图中绿色曲线）出现下降。
  - 结论：这证明了将 $L1$ 稀疏惩罚的影响范围限制在仅负责特征检测的参数上是有益的。
- 权重绑定的有效性（Ablation: untie encoder layers）：
  - 实验内容：让门控编码器和幅度编码器的参数完全独立，不再共享投影方向。
  - 结果：尽管独立编码器拥有更大的表达空间，但性能（图中紫色曲线）并未提升，反而略有恶化。
  - 结论：作者提出的权重绑定方案（公式 7）在减少参数量和计算开销的同时，有效捕捉了门控架构的优势。
- 重缩放参数的作用（Ablation: no r_mag）：
  - 实验内容：移除公式 (7) 中的重缩放参数 $r_{mag}$，进一步简化权重绑定方案。
  - 结果：性能（图中黄色曲线）轻微下降。
  - 结论：$r_{mag}$ 参数对 Gated SAE 的最佳表现有一定贡献，移除它会产生轻微负面影响。核心总结：Figure 8 证明了 Gated SAE 现有的每一个核心设计环节（解码器冻结、权重共享、以及引入 $r_{mag}$）对于实现最佳性能都是必要且有效的。

### 5.2. 仅仅解决“收缩问题”就足够了吗？ (Is it sufficient to just address shrinkage?)

在这一节中，作者探讨了 Gated SAE 性能提升的深层原因：究竟是因为消除了数值上的偏差（收缩），还是因为它学到了本质上更好的特征。

正如第 3.1 节所述，使用基准架构和 $L1$ 损失训练的 SAE 会系统性地低估潜在特征激活的幅度（即收缩现象）。门控 SAE 通过修改其架构和损失函数克服了这些限制，从而解决了收缩问题。

很自然会问，Gated SAE 的性能提升在多大程度上仅仅归因于解决了收缩问题。虽然在其他条件不变的情况下，解决收缩问题会提高重构保真度，但这并不是提高 SAE 性能的唯一途径：例如，门控 SAE 还可以通过学习更好的编码器方向（用于估计特征何时活跃及其幅度）或学习更好的解码器方向（即用于重构激活的更好字典）来改进基准 SAE。

在本节中，我们尝试通过将第 3.2.2 节所述训练的门控 SAE 与一种同样解决收缩问题但使用相同字典大小的基准 SAE 冻结编码器和解码器方向的替代方法（架构上等效）进行比较来回答这个问题。这种替代方法（我们称之为 “基准 + 重缩放与位移”，baseline + rescale & shift）相比基准 SAE 获得的任何性能提升只能归功于对活跃特征幅度的更好估计，因为从构造上看，采用该方法的 SAE 与基准 SAE 共享相同的编码器和解码器方向。

如图 9 所示，尽管仅解决收缩问题（“基准 + 重缩放与位移”）确实稍微改善了基准 SAE 的性能，但与门控 SAE 的性能相比仍存在显著差距。这表明门控架构和损失带来的益处源于学习到了更好的编码器和解码器方向，而不仅仅是克服了收缩问题。在附录 A 中，我们通过在推理时用优化算法替换各自的编码器，进一步探索了门控 SAE 和基准 SAE 解码器的不同之处。

![Figure_9](../../images/Interpretability/4_Figure_9.png)
Figure 9:解决“收缩问题”是否就是全部？

Figure 9 回答了一个核心科学问题：Gated SAE 的成功，仅仅是因为它消除了数值缩水的偏差（收缩问题）吗？


Figure 9 详细解析：解决“收缩问题”是否就是全部？
Figure 9 是论文中至关重要的一张图，它回答了一个核心科学问题：Gated SAE 的成功，仅仅是因为它消除了数值缩水的偏差（收缩问题）吗？

1. 实验背景与动机
在之前的章节中，作者证明了传统 SAE 因为 L1 惩罚会产生“收缩”（即系统性低估特征激活强度）。而 Gated SAE 通过架构改进彻底消除了这种偏差。 那么，如果我仅仅通过某种简单手段修复旧模型的收缩偏差，它的性能能追上 Gated SAE 吗？

2. 图中三条曲线的含义
为了回答上述问题，作者对比了三种配置：
- 蓝色曲线 (Baseline - equal width)：
  - 标准的基准 SAE。
  - 存在严重的收缩偏差，性能作为基准参考线。
- 绿色曲线 (Baseline + rescale & shift)：
  - 这是一个对照组。作者冻结了一个已训练好的基准 SAE 的编码器和解码器方向，仅在上面额外学习了重缩放参数 $r_{mag}$ 和位移参数 $b_{mag}$。
  - 它的作用是：在不改变特征搜索方向的前提下，强行修正数值偏差。
- 红色曲线 (Gated)：
  - 原生的 Gated SAE。
  - 同时拥有门控架构、优化的训练损失函数和自动消除收缩的能力。

3. 核心发现与结论
- 修复收缩确实有效，但有限：观察发现绿色曲线（修正偏差后）确实比蓝色曲线（基准）表现更好。这证明消除收缩偏差本身就能提升重构质量。
- 巨大的性能鸿沟：最关键的观察是，红色曲线（Gated SAE）依然大幅度领先于绿色曲线。即便绿色曲线已经解决了收缩问题，它与 Gated SAE 之间仍存在显著的性能差距。

4. 深度科学意义
Figure 9 证明了 Gated SAE 的优越性并不完全来自于“把数算准了”。它的真正优势在于：
- 更好的特征方向：由于采用了门控机制和特殊的损失函数（包括辅助任务 $\mathcal{L}_{aux}$），Gated SAE 在训练过程中学习到了更高质量的编码器和解码器方向（即更好的字典）。
- 协同效应：这种架构允许模型在学习“什么是特征”时，不再受到 $L1$ 惩罚对数值拟合的干扰，从而找到了比传统 SAE 更本质、更优的特征基底。

> 注释 12：关于 Figure 9 中“对照组”的具体实现，具体而言，作者通过训练基准 SAE（baseline SAEs），冻结其权重，然后学习额外的重缩放（rescale）和位移（shift）参数（类似于 Wright 和 Sharkey (2024) 的做法），这些参数被应用于（冻结的）编码器预激活值，随后再用于估计特征强度。（Figure 9 中绿色曲线（Baseline + rescale & shift）的补充方案说明）

## 6 相关工作 (Related Work)
这一节将 Gated SAE 的研究置于 **机械可解释性（Mechanistic Interpretability）** 这一更广泛的学术背景中，探讨了该技术对理解大型语言模型（LM）的意义。

### **机械可解释性**：
作者希望对稀疏自编码器（SAE）的改进能对机械可解释性研究有所帮助。近期的机械可解释性研究已在小型和大型语言模型中发现了重复出现的组件（Olsson et al., 2022），在小型模型中识别出了执行特定任务的计算子图（即电路 (circuits)；Wang et al. (2023)），并逆向工程了小型 Transformer 如何执行简单任务（Nanda et al., 2023）。

现有工作的局限性包括：
- 它们仅研究了自然语言训练分布中极窄的子集（尽管可参考 McDougall et al. (2023) 的研究）；
- 目前的工作尚未从机械原理上解释尖端（frontier）语言模型是如何运作的（Anthropic AI, 2024; Gemini Team, 2024; OpenAI, 2023）。

SAE 可能是解释模型在整个训练分布中行为的关键（Bricken et al., 2023），并且它们是在无监督下训练的，这可能使未来的工作能够解释更大规模的模型在更广泛任务中是如何运作的。

### **经典字典学习与语言模型中的应用**：
作者表示他们的工作建立在大量早于 Transformer 甚至早于深度学习的研究基础之上。例如：
- 稀疏编码 (Sparse Coding)：研究了离散和连续表示如何包含比基向量更多的表示形式，这与他们在第 1 节中的设置类似。
- 神经科学：稀疏表示在神经科学领域也得到了广泛研究。
- 收缩问题 (Shrinkage)：正如第 4.2 节所述，收缩现象是 Lasso 回归中固有的特性，并在统计学习中得到了充分研究。
- k-SVD 算法：一种字典学习算法，它也像 Gated SAE 一样采用两个阶段来学习字典。

### **语言模型中的字典学习 (Dictionary Learning in Language Models) **
将字典学习应用于语言模型的早期工作包括：

- Sharkey et al. (2022)：在类似 GPT-2 的模型上进行。
- Yun et al. (2023)：在 BERT 模型上进行。
- Tamkin et al. (2023)：在语言模型预训练期间使用离散特征。
- Cunningham et al. (2023)：在小型 Pythia 模型上进行。
- Bricken et al. (2023)：随后对训练于单层（1L）模型的 SAE 进行了广泛分析，通过将 SAE 接入前向传播来评估损失，评估学习到的特征对模型输出的影响，并利用自动可解释性对所有特征进行可视化和解释。

在此之后，其他研究者已将 SAE 的训练扩展到了：
- 注意力层输出：参考 Kissane et al. (2024a,b)。
- 残差流状态：参考 Bloom (2024)。

### **字典学习的局限与改进 (Dictionary Learning’s Limitations and Improvements)**

Wright 和 Sharkey (2024) 引起了人们对收缩问题 (shrinkage) 的关注（第 4.2 节），并提出通过解码器微调 (decoder finetuning) 来解决此问题。这种方法的一个困难在于，如果不丧失特征方向的稀疏性和/或可解释性，就不可能以这种方式微调 SAE 的所有参数。这限制了微调在消除基于 $L1$ 预训练期间植入 SAE 参数中的偏差方面的程度。门控 SAE 解决了这一问题（第 3.2 节）。

Marks 等人 (2024) 对 SAE 的实用性进行了压力测试，虽然取得了成功，但也依赖于在计算子图中留下许多“误差节点”的方法，这些节点代表了 SAE 重构与真实值之间的差异。针对 Bricken 等人 (2023) 工作的一系列更新也提出了 SAE 训练方法的改进建议（Batson et al., 2024; Olah et al., 2024; Templeton et al., 2024）。与我们的工作并行，Taggart (2024) 发现通过类似的 Jump ReLU 架构改变对 SAE 进行了初步改进，但使用了不同的损失函数，且未解决 $L1$ 带来的问题。

### **解离表示 (Disentanglement)**

解离表示旨在学习能够分离出底层数据生成过程中不同且独立的“变化因子”的表示形式。这与我们字典学习的目标有些相似，因为我们希望将激活向量分解为截然不同的、稀疏的变化因子（特征方向上的权重），尽管字典元素并非完全独立，因为由于非正交字典特征之间的干扰，可能无法同时准确地表示两个特征。

明确受解离表示学习启发的方法通常对学习到的表示施加先验结构，通常是让特征与潜在空间的基对齐。相比之下，在我们的工作中，我们专注于预训练语言模型的表示空间，而不是试图直接从数据中学习表示，并施加了不同的先验结构，即分解为超完备基的稀疏线性组合。从某种意义上说，我们的工作源于这样一种理论：语言模型已经成功学习到了具有特定结构的数据解离表示，而我们正试图找回它。

## 7 结论

在这项工作中，作者引入了门控 SAE (Gated SAEs)（第 3.2 节）。与基准 SAE 相比，它在重构质量和稀疏度方面实现了帕累托改进（第 4.1 节），且具有相当的可解释性（第 4.3 节）。作者通过消融研究证明，门控 SAE 方法的每一个关键部分对于实现强劲性能都是必不可少的（第 5.1 节）。这代表了在大语言模型（LLM）字典学习改进方面的重大进展——在许多测试位置，门控 SAE 仅需一半的 $L0$ 即可达到相同的恢复损失（图 12）。这可能会改进利用 SAE 来引导语言模型、解释电路或理解语言模型全分布组件的相关工作。

### 局限性

作者表示他们的工作与所有稀疏自编码器（SAE）研究一样，都建立在关于大语言模型（LLM）计算的稀疏性和线性的若干假设之上（第 1 节）。如果这些假设是错误的，他们的工作可能仍然有用（见脚注 1），但由于 SAE 固化了稀疏性和线性的假设，基于 SAE 的研究所得出的结论可能是不正确的。

此外，他们的工作通过更复杂的编码器增加了 SAE 训练的难度。一个令人担忧的问题是，增加稀疏自编码器的表达能力会导致重构激活值时出现过拟合（参考 Olah et al. (2023) 关于字典学习的担忧），因为底层语言模型仅使用简单的 MLP 和注意力头，尤其缺乏像阶跃函数（step functions）这类不连续性结构。

总体而言，他们没有发现过拟合的证据。他们的评估使用了预留的测试数据，并手动检查了可解释性。但这些评估并非完全全面：例如，它们没有测试所学习到的字典是否包含了模型计算中具有因果意义的中间变量。特别是不连续性（指 Jump ReLU 的跳变）给诸如积分梯度 (Integrated Gradients)（Sundararajan et al., 2017）等方法带来了挑战，因为这些方法通过离散方式逼近路径积分，正如 Marks et al. (2024) 将其应用于 SAE 那样。

最后，有人可能会认为，门控 SAE 与基准 SAE 之间的部分性能差距可以通过廉价的推理时干预来缩小，例如修剪基准 SAE 中经常出现的许多低激活特征——因为基准 SAE 不具备像门控 SAE 那样的阈值机制（见附录 E）。如果不进行此类干预，这些低激活特征会增加基准 SAE 在特定恢复损失下的 $L0$ 指标，但由于其幅度极低，对重构的贡献很小，且对可解释性的影响尚不明确。

### 未来工作 

未来的工作可以验证门控 SAE（Gated SAEs）是否在超过 70 亿（7B）参数的基础大语言模型（LLM）上继续改善字典学习，例如扩展到更大的对话模型（Chat models），甚至扩展到多模态（Multimodal）或混合专家模型（Mixture-of-Experts, MoE）。

此外，可以深入研究门控 SAE 和基准 SAE 所学到的特征，并确定除了本研究中指出的差异外，两种架构在**归纳偏置（Inductive biases）**方面是否存在其他差异。我们预见，通过对架构和训练程序的进一步调整，有可能进一步提升门控 SAE 的性能。

最后，作者表示最期待的是利用字典学习技术在整体上推进可解释性研究，例如改进语言模型中的电路发现（Circuit finding）或方向引导（Steering），并希望门控 SAE 能够起到加速这类研究的作用。

