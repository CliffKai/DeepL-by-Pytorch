这个是为了记录一些常见的激活函数与损失函数的用处，从目的上去了解这些函数。

# 激活函数

## 1.softmax

Softmax ：通常用于**多分类问题的最后一层**，将网络的输出变成一个概率分布。

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
$$

> **常与交叉熵损失函数（Cross Entropy Loss）一起使用**，形成一种组合：**softmax + cross entropy**。

Softmax 本身**不是**损失函数，它是一个**激活函数**（activation function），通常用于**多分类问题的最后一层**，将网络的输出变成一个概率分布。

## 2. ReLU（Rectified Linear Unit）

最常用的激活函数，尤其在卷积神经网络（CNN）中广泛使用。它具有非线性特性，同时计算开销小，有助于缓解梯度消失问题。

$$
f(x) = \max(0, x)
$$

一般用于隐藏层，尤其适合深层网络。

PyTorch:`torch.nn.ReLU()`或`torch.nn.functional.relu()`

## 3. Leaky ReLU

为了解决 ReLU 在 $x < 0$ 时梯度为 0 的问题（“死亡 ReLU”问题），它允许负方向有一个小的斜率。

$$
f(x) = 
\begin{cases}
x & \text{if } x \geq 0 \\
\alpha x & \text{if } x < 0
\end{cases}
\quad \text{（通常 } \alpha = 0.01\text{）}
$$

一般用于深层网络或当 ReLU 导致神经元死亡时。

PyTorch:`torch.nn.LeakyReLU()`或`torch.nn.functional.leaky_relu()`
out = leaky_relu(x)

## 4. Sigmoid

将值压缩到 (0, 1) 区间，常用于二分类的输出层。

$$
f(x) = \frac{1}{1 + e^{-x}}
$$

* 输出概率（二分类）
* 早期深度学习中也用于隐藏层，但容易梯度消失


## 5. Tanh（双曲正切）

压缩到 (-1, 1)，比 sigmoid 更中心对称，对零对称，在某些任务中表现更好。

$$
f(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

一般用于隐藏层，但也可能出现梯度消失。

## 6. Softmax

将一个向量转换成概率分布，常用于多分类任务的输出层。

$$
f_i(x) = \frac{e^{x_i}}{\sum_j e^{x_j}}
$$

一般只用于输出层，配合 `CrossEntropyLoss` 不需要显式调用。


## 7. GELU（Gaussian Error Linear Unit）

用于 Transformer 架构（如 BERT、GPT），相比 ReLU 更平滑、表现更好。

$$
f(x) = 0.5x \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right]\right)
$$

一般用于高端模型（Transformer、NLP 模型）默认激活函数。

